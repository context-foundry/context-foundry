"""
Data Analysis Example

Demonstrates using BAML + Anthropic Agent Skills for dataset analysis.
"""

import asyncio
import logging
from pathlib import Path
from typing import Literal

# Import BAML client (generated by baml-cli)
# from baml_client import b

from shared.config import load_config
from shared.utils import setup_logging, async_retry, DataAnalysisError


logger = logging.getLogger("baml_anthropic.data_analysis")

AnalysisType = Literal["trends", "anomalies", "forecast", "summary"]


@async_retry(max_retries=3, initial_delay=1.0, backoff_factor=2.0)
async def analyze_dataset(
    data_source: str,
    analysis_type: AnalysisType = "summary",
) -> dict:
    """
    Analyze a dataset using BAML + Anthropic Agent Skills.

    This function demonstrates:
    - Type-safe data analysis prompts
    - Agent Skills for data processing
    - Multiple analysis types
    - Structured output with recommendations

    Args:
        data_source: Path to data file or description of data source
        analysis_type: Type of analysis to perform:
            - "trends": Identify patterns over time
            - "anomalies": Detect outliers and unusual patterns
            - "forecast": Predict future values
            - "summary": Comprehensive statistical summary

    Returns:
        DataInsights object with:
            - trends: Identified patterns
            - anomalies: Detected outliers
            - recommendations: Actionable insights
            - visualizations: Suggested charts/graphs
            - statistics: Statistical summary

    Raises:
        DataAnalysisError: If data analysis fails
        FileNotFoundError: If data file doesn't exist

    Example:
        >>> result = await analyze_dataset(
        ...     "sales_data.csv",
        ...     "trends"
        ... )
        >>> print(f"Trends: {result['trends']}")
        >>> print(f"Recommendations: {result['recommendations']}")
    """
    # Validate data source if it's a file path
    data_path = Path(data_source)
    if data_path.suffix and not data_path.exists():
        raise FileNotFoundError(f"Data file not found: {data_source}")

    logger.info(f"Analyzing dataset: {data_source}")
    logger.info(f"Analysis type: {analysis_type}")

    try:
        # NOTE: This requires BAML client generation
        # Uncomment after running: baml-cli generate

        # Call BAML function (type-safe)
        # result = await b.AnalyzeDataset(
        #     data_source=data_source,
        #     analysis_type=analysis_type
        # )

        # For demonstration purposes (before BAML generation)
        # Return mock response with correct structure
        result = {
            "trends": [
                f"Placeholder trend analysis for {analysis_type}",
                "Run 'baml-cli generate' to enable real API calls",
                "The BAML client will provide type-safe data analysis",
            ],
            "anomalies": [
                "No anomalies detected (placeholder)",
            ],
            "recommendations": [
                f"Based on {analysis_type} analysis, consider...",
                "Investigate seasonal patterns",
                "Monitor key performance indicators",
            ],
            "visualizations": {
                "time_series": f"{analysis_type}_over_time.png",
                "distribution": "distribution_histogram.png",
                "correlation": "correlation_matrix.png",
            },
            "statistics": {
                "mean": 42.0,
                "median": 40.0,
                "std_dev": 12.5,
                "count": 1000.0,
            },
        }

        logger.info(f"Analysis complete. Found {len(result['trends'])} trends")
        return result

    except Exception as e:
        logger.error(f"Data analysis failed: {e}")
        raise DataAnalysisError(f"Failed to analyze dataset: {e}") from e


async def progressive_data_analysis(data_source: str):
    """
    Progressive analysis that starts simple and adds complexity.

    This demonstrates progressive disclosure by first doing a quick summary,
    then diving deeper if needed.

    Args:
        data_source: Path to data file

    Returns:
        Complete analysis results with progressive insights

    Example:
        >>> result = await progressive_data_analysis("sales.csv")
        >>> print(f"Quick summary: {result['quick_summary']}")
        >>> print(f"Deep insights: {result['deep_insights']}")
    """
    logger.info(f"Starting progressive analysis of: {data_source}")

    # Step 1: Quick summary (lightweight)
    logger.info("Step 1: Quick summary analysis")
    summary = await analyze_dataset(data_source, "summary")

    # Step 2: Decide if deeper analysis is needed
    logger.info("Step 2: Evaluating if deeper analysis is needed")

    # If data looks interesting, do trend analysis
    has_time_series = True  # Simplified logic
    if has_time_series:
        logger.info("Step 3: Performing trend analysis")
        trends = await analyze_dataset(data_source, "trends")
    else:
        trends = None

    # Step 3: Check for anomalies if trends show variation
    if trends and len(trends.get("trends", [])) > 2:
        logger.info("Step 4: Checking for anomalies")
        anomalies = await analyze_dataset(data_source, "anomalies")
    else:
        anomalies = None

    return {
        "quick_summary": summary,
        "trend_analysis": trends,
        "anomaly_detection": anomalies,
        "skills_used": [
            "summary" if summary else None,
            "trends" if trends else None,
            "anomalies" if anomalies else None,
        ],
    }


async def main():
    """Example usage of data analysis."""
    # Load configuration
    config = load_config()
    setup_logging(config.log_level)

    # Example data analysis
    test_file = Path(__file__).parent.parent.parent / "test_data" / "sample.csv"

    if not test_file.exists():
        logger.warning(f"Test file not found: {test_file}")
        logger.info("Creating placeholder test file...")
        test_file.parent.mkdir(parents=True, exist_ok=True)
        test_file.write_text("date,value\n2024-01-01,100\n2024-01-02,105\n")

    try:
        # Example 1: Simple analysis
        print("\n" + "=" * 60)
        print("EXAMPLE 1: TREND ANALYSIS")
        print("=" * 60)

        result = await analyze_dataset(str(test_file), "trends")

        print(f"\nDataset: {test_file.name}\n")
        print("Trends Identified:")
        for trend in result['trends']:
            print(f"  • {trend}")
        print("\nRecommendations:")
        for rec in result['recommendations']:
            print(f"  • {rec}")
        print("\nSuggested Visualizations:")
        for viz_name, viz_file in result['visualizations'].items():
            print(f"  • {viz_name}: {viz_file}")

        # Example 2: Progressive analysis
        print("\n" + "=" * 60)
        print("EXAMPLE 2: PROGRESSIVE ANALYSIS")
        print("=" * 60)

        progressive_result = await progressive_data_analysis(str(test_file))

        print(f"\nSkills Used: {progressive_result['skills_used']}")
        print("\nProgressive disclosure demonstrates loading skills only when needed!")

        print("\n" + "=" * 60)

    except Exception as e:
        logger.error(f"Example failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
