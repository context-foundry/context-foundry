[{"title":"FAQ","slug":"faq","category":"getting-started","categoryName":"Getting Started","description":"Frequently Asked Questions - Making Context Foundry Transparent","content":"Context Foundry FAQ Frequently Asked Questions - Making Context Foundry Transparent Philosophy: \"No Magic, Just Transparency\" This FAQ explains how Context Foundry works internally, where everything is stored, and demystifies the agent delegation model. Looking for technical details? If you're a software developer, architect, or AI engineer looking for deep technical information about parallelization, token management, MCP server architecture, or prompt engineering, see the Technical FAQ with 52 in-depth questions and code examples. --- Table of Contents Build Artifacts & Plans Where can I find the architect's plan for my project? What other build artifacts are created? Where is the pattern library stored? System Prompts & Delegation Did Context Foundry change Claude Code's system prompt? If I use Claude Code for other tasks, do I need to change the system prompt back? How does the MCP server trigger builds? Where are all the prompts located? Context & Agents Why is my context usage so low during builds? What happens to the agents after the build completes? Can I see the agent conversations? Usage & Control Can I review the architect's plan before building starts? How do I know what patterns are being applied to my build? Can I disable autonomous mode and have more control? --- Build Artifacts & Plans Where can I find the architect's plan for my project? Location: Inside your project directory at Example for VimQuest: [code] Example for Satoshi's Chore Tracker: [code] What's in it: Complete system architecture File structure plan Module specifications Data models Component hierarchy State management design Testing plan Implementation roadmap File size: Typically 30-90KB of detailed technical specifications! How to view: [code] --- What other build artifacts are created? Every project gets a directory with: [code] Key Files Explained: | File | Purpose | When to Read | |------|---------|--------------| | | Research findings, technology recommendations | Understand why certain tech choices were made | | | Complete technical design | Learn the system architecture | | | Implementation chronology | Track what was built and when | | | Test results and coverage | See test pass rate and quality metrics | | | Metadata: files created, duration, GitHub URL | Quick build overview | --- Where is the pattern library stored? Two Locations - Different Purposes: Global Pattern Library (shared across ALL builds) [code] Per-Project Patterns (this build's specific patterns) [code] How it works: Global patterns are READ by Scout/Architect/Test phases New patterns discovered during build go to project directory Phase 7: Feedback analyzes and promotes valuable patterns to global library Next build benefits from accumulated learnings View global patterns: [code] --- System Prompts & Delegation Did Context Foundry change Claude Code's system prompt? NO! Context Foundry does NOT modify Claude Code's system prompt at all. How it actually works: [code] Key Points: ✅ Claude Code's system prompt: UNCHANGED ✅ Orchestrator prompt: Sent as USER message to delegated instance ✅ Your main Claude window: Clean and separate ✅ Build work: Happens in separate Claude instance --- If I use Claude Code for other tasks, do I need to change the system prompt back? NO! You never need to worry about this. Why: Context Foundry runs in delegated Claude instances, not your main session Your regular Claude Code usage is completely unaffected The MCP server is only activated when you explicitly request a build Regular code editing, file operations, git commands all work normally You can freely: [code] Think of it like: Regular Claude Code = Your IDE Context Foundry = Calling a contractor who works offsite --- How does the MCP server trigger builds? The Delegation Flow: [code] Why this is brilliant: Main context stays clean (< 10% usage) Delegated instance can use 100% of ITS context Multiple builds can run in parallel Each build is isolated Main window just monitors progress --- Where are all the prompts located? Prompt Locations: | Prompt | Location | Purpose | |--------|----------|---------| | Orchestrator Prompt | | Main build orchestration (7 phases) | | MCP Server | | Delegation logic | | Agent Prompts | Embedded in | Scout, Architect, Builder, Test, etc. | | Claude Code System Prompt | NOT MODIFIED | Standard Claude Code prompt | View the orchestrator prompt: [code] It's 1000+ lines and defines: Phase 1: Scout (Research) Phase 2: Architect (Design) Phase 3: Builder (Implementation) Phase 4: Test (Quality Assurance + Self-Healing) Phase 5: Documentation Phase 6: Deployment Phase 7: Feedback Analysis --- Context & Agents Why is my context usage so low during builds? You noticed something important! Your observation is correct: The main Claude window shows very low context usage during builds. Why: [code] Meanwhile, in the delegated instance: [code] The magic: All the heavy lifting happens in a SEPARATE Claude instance that you never see! Benefits: ✅ Your main session stays clean ✅ You can continue working on other tasks ✅ Multiple builds can run in parallel ✅ Each build gets a fresh 200K context budget ✅ No context pollution between projects This is why Context Foundry can build multiple apps simultaneously without hitting context limits! --- What happens to the agents after the build completes? Agents are ephemeral - they exist only during the build and disappear when complete. Lifecycle: [code] What persists: ✅ Files created (your actual project code) ✅ artifacts (plans, reports, logs) ✅ Pattern library updates ✅ Git commits ✅ GitHub repository What disappears: ❌ Agent conversation histories ❌ Delegated Claude instance ❌ Temporary build state ❌ Agent \"memory\" (only artifacts remain) Why this is good: Clean slate for each build No state pollution Predictable behavior Agents can't accumulate incorrect assumptions Next build: Fresh agents are created, read the pattern library, and benefit from past learnings without carrying forward any conversation baggage. --- Can I see the agent conversations? Short answer: Not currently, but you can see their OUTPUT. What you CAN see: Each agent documents their work: | Agent | Output File | What It Shows | |-------|-------------|---------------| | Scout | | Research findings, technology recommendations | | Architect | | Complete system design | | Builder | | Implementation progress | | Test | | Test runs, failures, fixes | | Feedback | | Learnings extracted | Example - Reading Scout's findings: [code] You'll see: What Scout researched Technologies recommended Risks identified Prior art analyzed Patterns applied What you CAN'T see: The actual back-and-forth between Scout and file tools Internal reasoning steps Trial-and-error during research Why: The delegated Claude instance only returns the final artifacts, not the full conversation log. Future enhancement: Could add mode to capture full conversation logs if desired for debugging. --- Usage & Control Can I review the architect's plan before building starts? Yes! You have options: Option 1: Autonomous Mode (Default - No Review) [code] Option 2: Non-Autonomous Mode (Manual Review) [code] With checkpoints enabled: Scout completes → Shows scout-report.md → Waits for approval You review: You approve: \"Continue\" Architect completes → Shows architecture.md → Waits for approval You review: You approve: \"Continue\" Builder starts... Option 3: Review After Completion (Most Common) [code] Best practice: First build: Use autonomous mode (trust the system) After build: Review architecture.md to learn If adjustments needed: Edit code or request changes --- How do I know what patterns are being applied to my build? Patterns document themselves in the build artifacts! Check 1: Scout Report [code] Look for section: [code] Check 2: Architecture Document [code] Look for section: [code] Check 3: Session Summary [code] [code] Check 4: Global Pattern Library [code] See all available patterns and their auto-apply conditions. --- Can I disable autonomous mode and have more control? Yes! Multiple control levels: Level 1: Full Autonomous (Walk Away) [code] No checkpoints Runs start to finish Use when: You trust the system Level 2: Checkpoint Mode (Review Key Decisions) [code] Pauses after Scout (review research) Pauses after Architect (review plan) Pauses after Test (review results) Use when: You want oversight Level 3: Manual Orchestrator (Full Control) [code] Level 4: Traditional Claude Code (No Context Foundry) [code] Recommendation: First time: Use autonomous mode to see what it can do After seeing results: Decide if you want more control Complex projects: Use checkpoint mode Learning/teaching: Use manual orchestrator to see each phase --- Advanced Questions What's the difference between the MCP server and manual orchestrator? MCP Server (Recommended): Integrates with Claude Code's MCP protocol One-command builds from any directory Supports parallel builds Automatic delegation Clean API Usage: [code] Manual Orchestrator: Direct use of orchestrator prompt More explicit control Educational (see how it works) Debugging builds Usage: [code] When to use each: MCP: Production use, ease of use, parallel builds Manual: Learning, debugging, custom workflows --- Where can I learn more? Documentation: Quick Start: QUICKSTART.md User Guide: USERGUIDE.md Architecture Deep Dive: docs/ARCHITECTURE.md Delegation Model: docs/DELEGATIONMODEL.md Feedback System: FEEDBACKSYSTEM.md Real Examples: VimQuest: Satoshi's Chore Tracker: 1942 Clone: Pattern Library: Global patterns: See what Context Foundry has learned! --- Still Have Questions? Open an issue: https://github.com/snedea/context-foundry/issues Philosophy:** If the documentation doesn't answer it, the documentation is incomplete. We'll add your question to this FAQ!","url":"/docs/getting-started/faq","weight":1.5,"tags":[],"headings":["Context Foundry FAQ","Table of Contents","Build Artifacts & Plans","Where can I find the architect's plan for my project?","What other build artifacts are created?","Where is the pattern library stored?","System Prompts & Delegation","Did Context Foundry change Claude Code's system prompt?","If I use Claude Code for other tasks, do I need to change the system prompt back?","How does the MCP server trigger builds?","Where are all the prompts located?","Context & Agents","Why is my context usage so low during builds?","What happens to the agents after the build completes?","Can I see the agent conversations?","Usage & Control","Can I review the architect's plan before building starts?","How do I know what patterns are being applied to my build?","Can I disable autonomous mode and have more control?","Advanced Questions","What's the difference between the MCP server and manual orchestrator?","Where can I learn more?","Still Have Questions?"]},{"title":"Overview","slug":"readme","category":"getting-started","categoryName":"Getting Started","description":"> The AI That Builds Itself: Recursive Claude Spawning via Meta-MCP > Context Foundry uses Claude Code to spawn fresh Claude instances that autonomously build...","content":"🏭 Context Foundry The AI That Builds Itself: Recursive Claude Spawning via Meta-MCP Context Foundry uses Claude Code to spawn fresh Claude instances that autonomously build complete projects. Walk away and come back to production-ready software. Version 2.1.0 - October 2025 📌 Version Note: This is Context Foundry v2.x (MCP server for Claude Code). For the legacy v1.0 Python CLI with multi-provider support, see the branch or download release. --- What is Context Foundry? Context Foundry is an MCP (Model Context Protocol) server that empowers Claude Code CLI to build complete software projects autonomously with self-healing test loops and automatic GitHub deployment. Unlike traditional AI coding tools that require constant supervision, Context Foundry lets you describe what you want and walk away while it: Researches requirements (Scout phase) Designs architecture (Architect phase) Implements code with tests (Builder phase) Auto-fixes test failures (Test phase with self-healing) Captures screenshots automatically (Screenshot phase - NEW!) Documents everything with visual guides (Documentation phase) Deploys to GitHub (Deployment phase) Real Example: [code] --- 🚀 The Breakthrough: Meta-MCP Innovation What makes Context Foundry unique? Most MCP servers call external tools. Context Foundry does something radical: it uses MCP to recursively spawn Claude Code itself. [code] The paradigm shift: Traditional MCP: Claude → MCP → External System (database, API, etc.) Context Foundry: Claude → MCP → Claude (recursive spawning) Why this matters: 🧠 Fresh 200K context windows - Each spawned Claude starts clean, no token accumulation 🔄 Parallel execution - Multiple Claude instances work simultaneously 🎯 Autonomous decision-making - Agents decide when to spawn new agents 🏗️ Self-orchestration - AI orchestrates AI through meta-prompts Learn more: Read the complete technical breakdown in docs/INNOVATIONS.md - all 15 innovations explained with code examples. --- 🎮 See It In Action: Evolution Quest Real build example - Watch Context Foundry autonomously build a complete game from a simple request: Starting the Build Evolution Quest - Build Starting User makes a request, then walks away. Context Foundry's autonomous workflow begins. Guided Progress Updates Build Status Update 1 Scout phase completes research. Architect phase designs the game architecture. Builder phase begins implementation. Build Status Update 2 Tests running and passing. Self-healing test loop ensures quality. Screenshot capture phase documents the UI. Build Complete Build Complete All phases complete automatically. Tests passing. Documentation generated. Ready for deployment. The Result Evolution Quest - Ready to Play Complete, working game deployed to GitHub with tests, documentation, and screenshots. Total time: 12 minutes. User interaction: Just the initial request. Key Takeaway: User described what they wanted, walked away, came back to a finished, tested, documented, and deployed application. --- 💬 Just Ask Naturally (No Commands to Memorize!) The easiest way to use Context Foundry: Just describe what you want in plain English. ✅ Say This [code] [code] [code] That's it! Claude Code automatically uses Context Foundry's autonomous build system. No need to remember tool names or syntax. 📚 Quick Start One-time setup (2 minutes) - See QUICKSTART.md Start Claude Code: [code] Just ask: [code] Walk away - System builds autonomously (7-15 min) Get results - Deployed to GitHub with tests and docs 🎯 Say This, Not That | ✅ Say This (Easy) | ❌ Not This (Hard) | |-------------------|-------------------| | \"Build a blog with React\" | \"Use mcpautonomousbuildanddeploy with task: 'Build a blog', workingdirectory: '/tmp/blog', ...\" | | \"Create a Snake game\" | \"I need to call the autonomous build tool with parameters...\" | | \"Make a weather API\" | \"How do I use the MCP server to build an API?\" | Claude Code handles the MCP calls automatically when you ask to build something! 💡 Tips for Best Results Be specific: [code] Include tech requirements: [code] Want to learn more? → QUICKSTART.md for 5-minute tutorial --- 🎨 Key Innovations Context Foundry introduces 15 groundbreaking innovations that transform AI software development. Here's a quick overview organized by category: 🏗️ Architecture Innovations Meta-MCP Innovation - Use MCP to recursively spawn Claude Code instances (the breakthrough that enabled v2.0) Subprocess Delegation - Spawn fresh Claude instances via with auth inheritance Context Window Isolation - Each agent gets a fresh 200K token window, no accumulation File-Based Context System - Shared memory via filesystem, artifacts Markdown-First Design - files over JSON for human+AI readability 🤖 Automation Innovations Self-Healing Test Loop - Auto-fix test failures through redesign→rebuild→retest cycles Parallel Execution Architecture - Phase 2.5 and 4.5 spawn concurrent agents (30-45% faster) Meta-Prompt Orchestration - AI orchestrates AI via (no Python) 8-Phase Workflow - Scout→Architect→Builder→Test→Screenshot→Docs→Deploy→Feedback Async Task Management - Non-blocking subprocess execution, work while builds run 🧠 Intelligence Innovations Global Pattern Learning - Cross-project knowledge accumulation in with automatic community sharing Output Truncation Strategy - 45-45-10 split keeps critical context visible 🎨 User Experience Innovations Screenshot Capture Phase - Playwright-based visual documentation (Phase 4.5) TUI Real-time Monitoring - Textual framework terminal dashboard Livestream Integration - WebSocket-based remote monitoring Want the complete technical breakdown? See docs/INNOVATIONS.md for in-depth explanations with code examples, real-world impact analysis, and paradigm shifts for each innovation. What Changed from 1.x? | Feature | 1.x (Python CLI) | 2.0 (MCP Server) | |---------|------------------|-------------------| | Orchestration | Python scripts + API calls | Native Claude + meta-prompts | | Cost Model | Pay-per-token API | Claude Max subscription ($20/month unlimited) | | Testing | Manual review at checkpoints | Self-healing auto-fix loops | | Deployment | Manual git operations | Automatic GitHub deployment | | Tool Access | Limited to Python functions | Full Claude Code tool suite (Read, Edit, Bash, etc.) | | Context | API conversation history | File-based artifacts (.context-foundry/) | | Providers | 7 AI providers supported | Claude only (optimized for quality) | | Complexity | ~3000 lines of Python | ~1400 lines (MCP + meta-prompt) | Want the technical details? See ARCHITECTUREDECISIONS.md for comprehensive explanations of what changed and why. Using Context Foundry 1.x? The Python CLI is preserved in and still functional. --- 🔄 How It Works: 8-Phase Architecture Context Foundry orchestrates autonomous builds through 8 distinct phases, with parallel execution at key stages for maximum performance: [code] Key Features: Phase 2.5: Spawns 2-8 concurrent Builder agents based on project complexity Phase 4: Self-healing loop with up to 3 auto-fix iterations Phase 4.5: Parallel screenshot capture using Playwright Phase 7: Extracts patterns and updates global knowledge base Total Duration: 7-15 minutes for most projects (autonomous, zero human intervention) --- 📚 Understanding Context Foundry How It Really Works (No Magic, Just Transparency) Key Insight: Context Foundry does NOT modify Claude Code. It uses delegation to spawn separate Claude instances that do the work. [code] Why your context usage stays low: The heavy work happens in a separate Claude instance! Where Are My Build Artifacts? Every project gets a directory: [code] Example - View VimQuest's architecture plan: [code] Pattern Library Location Global patterns (shared across ALL builds): [code] How it works: Each build reads global patterns Applies past learnings automatically Discovers new patterns Updates library for future builds See what Context Foundry has learned: [code] Common Questions ❓ Did Context Foundry change Claude Code's system prompt? ✅ No! It spawns separate Claude instances via delegation. Your regular Claude Code usage is unaffected. ❓ Where can I find the architect's plan? ✅ ❓ Why is my context usage so low? ✅ The build runs in a separate Claude instance. Your main window just monitors progress. ❓ What happens to the agents after the build? ✅ They're ephemeral - disappear after the build. Only artifacts and patterns persist. ❓ Can I review the plan before building? ✅ Yes! Use to enable checkpoints, or review after completion. 📖 Full FAQ Want comprehensive answers? See FAQ.md for: Complete delegation model explanation Where all prompts are located How agents work and disappear Build artifact locations Pattern library details Control options (autonomous vs manual) And much more! Additional Documentation: FAQ.md - Comprehensive Q&A (transparency focus) USERGUIDE.md - Step-by-step usage guide docs/DELEGATIONMODEL.md - Technical deep dive on delegation FEEDBACKSYSTEM.md - How self-learning works docs/BAMLINTEGRATION.md - Type-safe LLM outputs with BAML (v1.3.0+) --- Quick Start Prerequisites Python 3.10 or higher (for MCP server) Claude Code CLI installed and in PATH Claude Max subscription ($20/month) or Anthropic API key Git and GitHub CLI (for deployment features) Installation Stable Release (Recommended) [code] Detailed setup guide: See CLAUDECODEMCPSETUP.md for troubleshooting and advanced configuration. Nightly Builds (Bleeding Edge) For the latest features and fixes before they're in a stable release: [code] About Nightly Releases: 🌙 Built daily at midnight UTC if there are new commits 🏷️ Format: (e.g., ) ⚡ Latest features that haven't been released in stable yet 🔬 Pre-release quality - May contain bugs, use stable releases for production 📋 Auto-generated release notes - Grouped by commit type (feat, fix, docs, etc.) 🔄 Retained for 30 days then automatically cleaned up View nightly releases: GitHub Releases (Pre-releases) Basic Usage Start Claude Code CLI: [code] Inside your Claude Code session: Build a New Project (Autonomous) [code] What happens: Build starts immediately in the background You get a taskid to check progress You can continue working while it builds! System completes autonomously: Scout researches requirements (1-2 min) Architect designs system (1-2 min) Builder implements code + tests (2-5 min) Tester validates (tests fail? auto-fixes up to 3x) Screenshot Capturer takes visual documentation (30-60 sec) Documentation created with screenshots (1 min) Deployed to GitHub (30 sec) Total: ~7-15 minutes, zero human input required. Check status anytime: [code] Claude will automatically check and report progress. Delegate a Simple Task [code] Returns when complete with full output. Parallel Task Execution [code] Time saved: 3 sequential tasks (30 min) → parallel tasks (10 min) = 3x faster --- Background Builds (Non-Blocking Execution) The best part: Builds run in the background by default! You can continue working in your Claude Code session while projects build autonomously. How It Works When you trigger a build (by saying \"Build a weather app\" or using the MCP tool directly), the system: Starts immediately - Spawns a background Claude Code process Returns taskid - Gives you a tracking ID Runs autonomously - Completes all phases without blocking you Notifies when done - You can check status anytime Example Workflow [code] Checking Build Status Ask naturally: [code] Or use the MCP tool directly: [code] Listing All Active Builds [code] Or: [code] Benefits of Background Builds ✅ No blocking - Keep working while builds run ✅ Parallel work - Build multiple projects simultaneously ✅ Better UX - Natural workflow, no waiting ✅ Check anytime - Monitor progress when convenient Note: If you want synchronous execution (wait for completion), you can explicitly request the non-async version, but async is recommended for the best experience. --- MCP Tools Reference 🚀 (Recommended) Fully autonomous Scout → Architect → Builder → Test → Deploy workflow that runs in the background (non-blocking). Why async is better: ✅ Continue working while build runs ✅ Build multiple projects simultaneously ✅ No session blocking (7-15 min builds don't freeze Claude) ✅ Check status anytime Parameters: (required): What to build (e.g., \"Build a todo app\") (required): Where to build it (optional): Deploy to this GitHub repo (optional): Enhance existing repo instead (default: \"newproject\"): \"newproject\" | \"fix\" | \"enhance\" (default: true): Enable self-healing when tests fail (default: 3): Max auto-fix attempts (default: 90.0): Total timeout Returns: JSON with taskid and status message (build continues in background) Note: When you naturally say \"Build a weather app\", Claude Code automatically uses this async version. 🚀 (Synchronous) Same as async version above, but blocks your Claude Code session until complete. When to use: You specifically want to wait for the build You're debugging and need immediate output You prefer synchronous workflows Returns: JSON with complete results after build finishes (7-15 min wait) Example: [code] ⚡ Synchronous delegation - starts task and waits for completion. Parameters: (required): Task description (optional): Where to run (default: current) (default: 10.0): Max execution time (optional): CLI flags to pass Returns: JSON with status, stdout, stderr, duration 🔄 Asynchronous delegation - starts task in background, returns immediately. Parameters: Same as Returns: JSON with for tracking 📊 Check status and retrieve results of async task. Returns: If running: If complete: If timeout: 📋 List all active and completed async tasks. Returns: JSON array of tasks with status and elapsed time --- Real-World Examples Example 1: Build Express.js Weather API [code] Result: Duration: 8.3 minutes Files: 12 (server.js, routes/, controllers/, tests/, README.md, etc.) Tests: 15/15 passed (iteration 1 failed, auto-fixed) Deployed: https://github.com/snedea/weather-api Example 2: Build Mario Platformer Game [code] Result: Duration: 7.42 minutes Files: 8 (index.html, game.js, player.js, level.js, physics.js, tests/, docs/) Tests: 12/12 passed (iteration 2 after collision fix) Deployed: https://github.com/snedea/mario-game Example 3: Parallel Full-Stack Build [code] Result: Sequential time: ~30 minutes (10 min each) Parallel time: ~12 minutes (limited by slowest task) Time saved: 18 minutes (60% faster) --- Self-Healing Test Loop in Action Scenario: Building Express.js authentication API [code] Total auto-fix time: ~2 minutes Human intervention: Zero --- Configuration Custom Python Version If you need a specific Python version for the MCP server: Edit : [code] Environment Variables Pass environment variables to MCP server: [code] Disable MCP Server Temporarily disable without deleting configuration: [code] --- Troubleshooting MCP Server Won't Start Error: Solution: [code] Error: (Debian/Ubuntu systems) Solution: [code] Error: Python version too old Solution: [code] Claude Code Doesn't See MCP Tools Symptoms: not available Solutions: Verify MCP settings exist: [code] Check path is correct: [code] Restart Claude Code: [code] Verify connection: [code] Delegations Timeout Symptoms: Tasks consistently hit timeout Solutions: Increase timeout: [code] Break into smaller tasks: Instead of \"Build entire application\" Use: Multiple delegations for modules Check if task requires manual input (shouldn't happen with ) Tests Keep Failing Symptoms: Test loop reaches max iterations without passing Solutions: Check test failure reports: [code] Review fix attempts: [code] Increase max iterations if issue seems fixable: [code] Disable test loop to see raw test output: [code] More help: See CLAUDECODEMCPSETUP.md for comprehensive troubleshooting. --- Project Structure [code] --- 💰 Cost Model & BAML Integration CRITICAL: What Runs on Your Subscription vs API Keys Context Foundry has two layers - understanding this is essential: Layer 1: Core Build System (FREE - Uses Your Claude Code Subscription) ALL the main work runs on your Claude Code subscription: ✅ Scout Agent - Codebase research, requirement analysis ✅ Architect Agent - System design, architecture planning ✅ Builder Agents (2-8 parallel) - ALL code implementation ✅ Test Agent - Running tests, analyzing failures ✅ Self-Healing Loop - Auto-fixing test failures (redesign → rebuild → retest) ✅ Screenshot Agent - Visual documentation capture ✅ Documentation Agent - README and guide generation ✅ Deploy Agent - GitHub deployment These agents account for 99%+ of the token usage and run entirely under your $20/month Claude Max subscription (unlimited usage). Layer 2: BAML Type-Safety (OPTIONAL - Requires API Key) BAML is an optional add-on for type-safe validation: ⚙️ Phase tracking validation (~10-15 calls/build) ⚙️ Scout report structure validation (1 call/build) ⚙️ Architecture blueprint validation (1 call/build) ⚙️ Build result validation (5-10 calls/build) BAML token usage: ~17,000 tokens per build BAML cost: ~$0.20 per build (20 cents) What you get: Guaranteed valid JSON structures Compile-time schema validation Type-safe outputs Multi-provider support (Claude/GPT/Gemini) What you lose if disabled: Simple JSON validation instead (works fine!) No type checking (but JSON parsing still works) Cost Comparison Table | Component | Runs On | Cost | Token Usage | What It Does | |-----------|---------|------|-------------|--------------| | Scout Agent | Claude Code subscription | $0 (included) | ~15,000 tokens | Research & requirements | | Architect Agent | Claude Code subscription | $0 (included) | ~25,000 tokens | System design | | Builder Agents (2-8×) | Claude Code subscription | $0 (included) | ~100,000 tokens | All code implementation | | Test Agent | Claude Code subscription | $0 (included) | ~20,000 tokens | Test execution & analysis | | Self-Healing (1-3×) | Claude Code subscription | $0 (included) | ~30,000 tokens/iteration | Auto-fix test failures | | Screenshot Agent | Claude Code subscription | $0 (included) | ~5,000 tokens | Visual docs | | Docs Agent | Claude Code subscription | $0 (included) | ~10,000 tokens | README generation | | Deploy Agent | Claude Code subscription | $0 (included) | ~5,000 tokens | GitHub deployment | | BAML Validation | API key (optional) | ~$0.20/build | ~17,000 tokens | Type-safe validation | Total per build: With BAML: $0.20 (subscription covers 99%, BAML adds $0.20) Without BAML: $0 (100% covered by subscription) BAML: Keep or Disable? Disable BAML (Recommended for most users): [code] Enable BAML (If you want type safety): [code] Our recommendation: Try Context Foundry without BAML first. If you need stronger type guarantees, enable it later. --- Performance & Cost Performance Metrics Based on real-world usage: | Metric | Value | |--------|-------| | Avg build time | 7-15 minutes (simple to moderate projects) | | Test auto-fix success | 95% (within 3 iterations) | | Parallel speedup | 3-10x (vs sequential) | | Token efficiency | No limits (file-based context) | | Code quality | 90%+ test coverage | Cost Comparison Context Foundry 1.x (API mode): [code] Context Foundry (Claude Max): [code] Savings: 95%+ for heavy users --- Documentation 📘 Getting Started | Document | Description | Audience | |----------|-------------|----------| | README.md (this file) | Quick start and overview | Everyone | | QUICKSTART.md | 5-minute setup guide | New users | | USERGUIDE.md | Step-by-step usage guide with examples | New users | | FAQ.md | Comprehensive Q&A - transparency focused | Everyone | 🔧 Setup & Configuration | Document | Description | Audience | |----------|-------------|----------| | CLAUDECODEMCPSETUP.md | Complete MCP setup and troubleshooting | All users | | .mcp.json | Project-shareable MCP configuration | Team leads | 🏗️ Architecture & Technical Deep Dives | Document | Description | Audience | |----------|-------------|----------| | ⭐ docs/INNOVATIONS.md | All 15 innovations explained with code examples | Everyone - START HERE! | | docs/FAQ.md | Technical FAQ (52 questions): parallelization, token management, MCP architecture, prompt engineering | Developers, architects, AI engineers | | docs/ARCHITECTUREDIAGRAMS.md | 🎨 Visual flowcharts and sequence diagrams (Mermaid) | Visual learners, everyone! | | docs/MCPSERVERARCHITECTURE.md | Complete MCP server technical architecture | Developers, contributors | | docs/CONTEXTPRESERVATION.md | How context flows between agents (ephemeral agents + persistent files) | Developers, curious users | | docs/DELEGATIONMODEL.md | Why delegation keeps main context clean | Technical users | | ARCHITECTUREDECISIONS.md | What changed in v2.0 and why | Technical users | | docs/ARCHITECTURE.md | Stateless conversation architecture | Developers | 🧠 Self-Learning & Patterns | Document | Description | Audience | |----------|-------------|----------| | FEEDBACKSYSTEM.md | Self-learning pattern library documentation | All users | | ~/.context-foundry/patterns/common-issues.json | Global pattern library (on your machine) | Curious users | 📚 Reference & Legacy | Document | Description | Audience | |----------|-------------|----------| | CHANGELOG.md | Version history and release notes | Everyone | | ROADMAP.md | Future plans | Contributors | | LEGACYREADME.md | Original Context Foundry 1.x documentation | v1.x users | 💡 Recommended Reading Order New Users: README.md - Understand what Context Foundry does QUICKSTART.md - Get set up in 5 minutes USERGUIDE.md - Learn how to use it docs/INNOVATIONS.md - Deep dive into all 15 innovations FAQ.md - Common questions answered Developers/Contributors: docs/INNOVATIONS.md - 🎨 START HERE! All 15 innovations with code examples docs/FAQ.md - Technical FAQ (52 questions on architecture, parallelization, etc.) docs/ARCHITECTUREDIAGRAMS.md - Visual flowcharts and sequence diagrams ARCHITECTUREDECISIONS.md - Why v2.0 architecture docs/MCPSERVERARCHITECTURE.md - How MCP server works docs/CONTEXTPRESERVATION.md - How context flows docs/DELEGATIONMODEL.md - Delegation architecture Troubleshooting: CLAUDECODEMCPSETUP.md - Setup issues FAQ.md - Common questions docs/FAQ.md - Technical troubleshooting (parallel execution, test loops, etc.) docs/MCPSERVERARCHITECTURE.md - Advanced debugging --- Philosophy Context Foundry Philosophy: Autonomous over supervised: Walk away while it builds Self-healing over manual debugging: Auto-fix test failures File-based over conversation-based: No token limits Quality over speed: Tests must pass before deployment Simplicity over features: Do one thing excellently Design Principles: ✅ AI orchestrates itself (meta-prompts, not Python) ✅ Native tools over custom wrappers (Claude Code Read/Edit/Bash) ✅ File artifacts over conversation memory (.context-foundry/ directory) ✅ Self-healing over checkpoints (auto-fix instead of human review) ✅ GitHub deployment over local-only (share your work) --- Roadmap v2.2.0 (Next Release) [ ] Enhanced test failure analysis [ ] Configurable test frameworks (Jest, pytest, etc.) [ ] Better error recovery in deployment phase [ ] Pattern library (save successful builds as reusable patterns) [ ] Multi-project orchestration (build related projects together) [ ] Cost tracking for API mode users [ ] Enhanced logging and debugging tools v3.0 (Vision) [ ] Visual progress dashboard [ ] Support for additional version control systems [ ] Integration with CI/CD pipelines [ ] Team collaboration features --- Contributing We welcome contributions! To contribute: Read the technical docs: ARCHITECTUREDECISIONS.md Understand the workflow: Scout → Architect → Builder → Test → Deploy Follow the principles: Autonomous, self-healing, file-based Submit PRs: With clear descriptions and tests --- License MIT License - See LICENSE file for details --- Credits Context Foundry builds upon: Anthropic's Claude Code - Native agent capabilities and MCP protocol Context Foundry 1.x - Original Scout/Architect/Builder workflow Dexter Horthy's \"anti-vibe coding\" - Systematic approach over chaotic iteration Anthropic Agent SDK patterns - Agent orchestration techniques --- Support Issues: GitHub Issues Discussions: GitHub Discussions Documentation: Start with this README, then ARCHITECTUREDECISIONS.md --- Context Foundry - Build complete software autonomously with self-healing AI workflows Version: 2.1.0 | Release Date: October 2025 | License:** MIT","url":"/docs/getting-started/readme","weight":1.5,"tags":[],"headings":["🏭 Context Foundry","What is Context Foundry?","🚀 The Breakthrough: Meta-MCP Innovation","🎮 See It In Action: Evolution Quest","Starting the Build","Guided Progress Updates","Build Complete","The Result","💬 Just Ask Naturally (No Commands to Memorize!)","✅ Say This","📚 Quick Start","🎯 Say This, Not That","💡 Tips for Best Results","🎨 Key Innovations","🏗️ Architecture Innovations","🤖 Automation Innovations","🧠 Intelligence Innovations","🎨 User Experience Innovations","What Changed from 1.x?","🔄 How It Works: 8-Phase Architecture","📚 Understanding Context Foundry","How It Really Works (No Magic, Just Transparency)","Where Are My Build Artifacts?","Pattern Library Location","Common Questions","📖 Full FAQ","Quick Start","Prerequisites","Installation","Stable Release (Recommended)","Nightly Builds (Bleeding Edge)","Basic Usage","Build a New Project (Autonomous)","Delegate a Simple Task","Parallel Task Execution","Background Builds (Non-Blocking Execution)","How It Works","Example Workflow","Checking Build Status","Listing All Active Builds","Benefits of Background Builds","MCP Tools Reference","🚀 autonomous_build_and_deploy_async() (Recommended)","🚀 autonomous_build_and_deploy() (Synchronous)","⚡ delegate_to_claude_code()","🔄 delegate_to_claude_code_async()","📊 get_delegation_result(task_id)","📋 list_delegations()","Real-World Examples","Example 1: Build Express.js Weather API","Example 2: Build Mario Platformer Game","Example 3: Parallel Full-Stack Build","Self-Healing Test Loop in Action","Configuration","Custom Python Version","Environment Variables","Disable MCP Server","Troubleshooting","MCP Server Won't Start","Claude Code Doesn't See MCP Tools","Delegations Timeout","Tests Keep Failing","Project Structure","💰 Cost Model & BAML Integration","CRITICAL: What Runs on Your Subscription vs API Keys","Layer 1: Core Build System (FREE - Uses Your Claude Code Subscription)","Layer 2: BAML Type-Safety (OPTIONAL - Requires API Key)","Cost Comparison Table","BAML: Keep or Disable?","Performance & Cost","Performance Metrics","Cost Comparison","Documentation","📘 Getting Started","🔧 Setup & Configuration","🏗️ Architecture & Technical Deep Dives","🧠 Self-Learning & Patterns","📚 Reference & Legacy","💡 Recommended Reading Order","Philosophy","Roadmap","v2.2.0 (Next Release)","v3.0 (Vision)","Contributing","License","Credits","Support"]},{"title":"Quick Start","slug":"quickstart","category":"getting-started","categoryName":"Getting Started","description":"Get from zero to deployed app in 5 minutes","content":"Context Foundry - 5-Minute Quickstart Get from zero to deployed app in 5 minutes --- What You'll Do One-time setup (2 minutes) Build your first app (3 minutes of your time, 7-15 min build runs in background) See it deployed on GitHub Total active time: 5 minutes Build time: 7-15 minutes (runs in background while you work - no waiting!) --- Step 1: One-Time Setup (2 minutes) Install Dependencies [code] Connect to Claude Code [code] Authenticate with GitHub [code] Done! You only do this once. --- Step 2: Build Your First App (3 minutes) Start Claude Code [code] Just Ask Naturally Inside your Claude Code session, say: [code] That's it! No commands to memorize, no copy/paste needed. What Happens Next (Runs in Background!) Claude Code will automatically start the build in the background: [code] You can now: ✅ Continue working on other things ✅ Start another build in parallel ✅ Close Claude and come back later ✅ Check status anytime The system autonomously: Scout (1-2 min): Research best practices Architect (1-2 min): Design the app Builder (2-5 min): Write all code + tests Test (1-2 min): Validate everything (auto-fixes failures!) Document (1 min): Create README and docs Deploy (30 sec): Push to GitHub Total time: 7-15 minutes (runs in background while you work) Visual Example: Real Build in Progress Here's what a real autonomous build looks like: Build Process - Starting Context Foundry begins the autonomous build process after your request Build Process - In Progress Guided workflow progresses through Scout → Architect → Builder phases automatically Build Process - Testing Self-healing test loop validates and fixes issues without your intervention Build Process - Complete All phases complete with tests passing and documentation generated Final Application Deployed, working application ready to use - from simple request to finished product The entire process runs autonomously - you can work on other things while it builds! Check status: [code] --- Step 3: Check Your Results You'll Get [code] What Was Created ✅ Full source code (HTML, CSS, JavaScript) ✅ Comprehensive tests (Jest) ✅ Complete documentation (README, usage guides) ✅ Deployed to GitHub ✅ All tests passing --- More Examples Weather App [code] REST API [code] Game [code] Full-Stack App [code] --- Tips for Best Results ✅ Do This Be specific about features: [code] Include technical requirements: [code] Mention deployment needs: [code] ❌ Don't Do This Too vague: [code] Just questions (won't trigger build): [code] Contradictory requirements: [code] --- Common Scenarios Scenario 1: Quick Prototype [code] Result: Working prototype in ~7 minutes Scenario 2: Production-Ready API [code] Result: Production-ready API in ~15 minutes Scenario 3: Learning Project [code] Result: Educational project with comments in ~8 minutes --- What If Something Goes Wrong? Build Failed Check for details: [code] The system auto-fixes 95% of failures. If it doesn't: Review the error reports Re-run with more iterations: (in Claude Code) \"Increase maxtestiterations to 5 and rebuild\" MCP Tools Not Available [code] Timeout For very complex projects, increase timeout: [code] --- Next Steps You Just Built Your First App! Now try: Build something useful - Solve a real problem you have Experiment - Try different tech stacks Learn - Review the generated code to learn patterns Share - Your apps are on GitHub, share them! Want to Learn More? README.md - Full feature overview USERGUIDE.md - Detailed usage guide ARCHITECTUREDECISIONS.md - How it works under the hood, what's new in 2.0 Advanced Features Once comfortable with basics: Parallel builds - Build multiple components simultaneously Custom workflows - Edit Existing projects - Enhance or fix existing code Complex systems - Multi-service architectures --- Troubleshooting Quick Reference | Problem | Solution | |---------|----------| | requirements-mcp.txt not found | - you need to be in the cloned directory | | MCP not connected | then re-run setup | | Python version error | Install Python 3.10+: | | Build timeout | Add: \"Use 30 minute timeout\" to request | | Tests failing | Check | | GitHub auth error | Run: | | Wrong directory | Specify: \"Build in /Users/name/projects/myapp\" | --- FAQ Q: Do I need to know the MCP tool names? A: No! Just describe what you want in natural language. Q: Can I use this for real projects? A: Yes! The code is production-ready with tests and documentation. Q: How much does it cost? A: Requires Claude Max subscription ($20/month unlimited) or pay-per-use API. Q: Can I customize the workflow? A: Yes! Edit to change phases. Q: What if I don't want GitHub deployment? A: Say: \"Build locally only, skip GitHub deployment\" Q: Can it work on existing code? A: Yes! Say: \"Enhance my project at /path/to/project by adding [features]\" Q: Is the generated code good quality? A: Yes - 90%+ test coverage, follows best practices, includes documentation. Q: Can I stop a build in progress? A: Builds are autonomous but time out after the specified duration (default 90 min). --- Summary The magic of Context Foundry: You: \"Build [describe your app]\" System: [Builds autonomously for 7-15 minutes] You: Get deployed app with tests and docs No commands to memorize. No copy/paste. No supervision needed. --- Ready to build? → Start Claude Code: Questions? → See USERGUIDE.md for comprehensive help Technical details? → See ARCHITECTUREDECISIONS.md --- Context Foundry - Build complete software autonomously","url":"/docs/getting-started/quickstart","weight":1.5,"tags":[],"headings":["Context Foundry - 5-Minute Quickstart","What You'll Do","Step 1: One-Time Setup (2 minutes)","Install Dependencies","Connect to Claude Code","Authenticate with GitHub","Step 2: Build Your First App (3 minutes)","Start Claude Code","Just Ask Naturally","What Happens Next (Runs in Background!)","Visual Example: Real Build in Progress","Step 3: Check Your Results","You'll Get","What Was Created","More Examples","Weather App","REST API","Game","Full-Stack App","Tips for Best Results","✅ Do This","❌ Don't Do This","Common Scenarios","Scenario 1: Quick Prototype","Scenario 2: Production-Ready API","Scenario 3: Learning Project","What If Something Goes Wrong?","Build Failed","MCP Tools Not Available","Timeout","Next Steps","You Just Built Your First App!","Want to Learn More?","Advanced Features","Troubleshooting Quick Reference","FAQ","Summary"]},{"title":"User Guide","slug":"user-guide","category":"getting-started","categoryName":"Getting Started","description":"Your step-by-step guide to autonomous AI development","content":"Context Foundry 2.0 - User Guide Your step-by-step guide to autonomous AI development --- Table of Contents Getting Started Basic Usage Autonomous Builds Task Delegation Parallel Execution Understanding the Workflow Real-Time Monitoring Dashboard (NEW) Troubleshooting Best Practices Advanced Usage --- Getting Started Prerequisites Check Before starting, verify you have: [code] If any of these fail, see Prerequisites Setup below. Installation Step 1: Clone Context Foundry [code] Step 2: Install MCP Server Dependencies [code] If you get errors, see Troubleshooting Installation. Step 3: Configure MCP Connection [code] Step 4: Verify Connection [code] If you see \"✗ Disconnected\" or errors, see Troubleshooting MCP Connection. Step 5: Test the Setup [code] ✅ Success! You're ready to use Context Foundry 2.0. --- How Claude Code Recognizes Build Requests The easiest way to use Context Foundry: Just ask naturally in plain English! 🎯 Intent Detection Claude Code automatically uses Context Foundry's autonomous build system when you: ✅ Use action words: \"build\", \"create\", \"make\", \"develop\", \"implement\" ✅ Describe an application or project: \"weather app\", \"REST API\", \"todo list\" ✅ Include features or requirements: List what you want it to do ✅ Examples That Trigger Autonomous Build These natural requests will automatically use : [code] [code] [code] [code] [code] [code] ❌ Examples That Won't Trigger (Just Explains) These are questions that ask for information, not requests to build: [code] [code] [code] [code] 💡 The Difference | Intent | Example | What Happens | |--------|---------|--------------| | Build Request | \"Build a weather app\" | ✅ Automatic autonomous build | | Question | \"How do I build a weather app?\" | ℹ️ Explains the process | | Discussion | \"What's the best way to build apps?\" | ℹ️ Discusses approaches | | Learning | \"Teach me to build a weather app\" | ℹ️ Educational response | 🚀 No Need to Mention MCP Tools! You don't need to say: ❌ \"Use mcpautonomousbuildanddeploy...\" ❌ \"Call the autonomous build tool...\" ❌ \"Execute the MCP command for...\" Just say what you want: ✅ \"Build a weather app\" ✅ \"Create a todo list\" ✅ \"Make a calculator\" Claude Code handles the MCP calls automatically! 📝 Being Specific Gets Better Results Vague (works, but basic): [code] Specific (much better results): [code] Very specific (best results): [code] 🎓 Learning the Pattern Pattern: Examples: Build a todo app with React and localStorage Create a REST API with Express and PostgreSQL Make a Snake game with HTML5 Canvas Develop a blog platform with Markdown support and authentication 💬 What If You're Unsure? Just ask naturally! Worst case: If it's a build request → Autonomous build starts If it's a question → You get an explanation (then you can say \"Actually, build it!\") Example conversation: [code] 🔧 Advanced: Customize the Build You can still specify advanced options if needed: [code] [code] [code] Claude Code will extract these requirements and use the right parameters. --- Basic Usage Your First Build Let's build a simple \"Hello World\" project to verify everything works. [code] Inside your Claude Code session, say: [code] What happens: Scout phase: AI researches how to build a simple HTML page Architect phase: AI designs the structure (HTML, CSS, button) Builder phase: AI creates the files Test phase: Skipped (enabletestloop: false) Documentation phase: AI creates README Deploy phase: AI pushes to GitHub Expected output: [code] Verify the result: [code] ✅ If you see your project deployed to GitHub, everything works! --- Autonomous Builds Building a Todo CLI App Let's build a more realistic project with tests enabled. [code] What's different: - AI will auto-fix if tests fail - Up to 3 auto-fix attempts Duration: ~5-10 minutes What to expect: [code] Final structure: [code] Try it out: [code] --- Task Delegation Simple Synchronous Delegation For quick, standalone tasks that don't need the full workflow: [code] When to use: Quick scripts or single files No need for full Scout → Architect → Builder workflow Want synchronous execution (wait for result) Returns immediately when complete with: stdout (script output) stderr (error messages) duration exit code Checking Working Directory Before delegating, optionally create and verify the directory: [code] Then delegate: [code] --- Parallel Execution Building a Full-Stack App in Parallel Instead of building components sequentially (slow), build them in parallel (fast). Step 1: Start All Tasks in Parallel [code] Claude will spawn all three tasks and return task IDs: [code] Step 2: Monitor Progress [code] Returns: [code] Step 3: Collect Results [code] Returns: [code] Repeat for other tasks when they complete. Performance Comparison Sequential approach: [code] Parallel approach: [code] --- Background Builds Important: By default, all autonomous builds run in the background (non-blocking), so you can continue working while projects build. Why Background Builds? Before (blocking builds): [code] After (background builds): [code] How It Works When you request a build, the system: Spawns a background process - Fresh Claude Code instance starts Returns immediately - You get a taskid right away Runs autonomously - Complete workflow happens in background No blocking - Your Claude Code session stays responsive Starting a Background Build Natural language (easiest): [code] Claude automatically uses the async version! Explicit MCP call: [code] You'll get: [code] Checking Build Status Ask naturally: [code] Use MCP tool: [code] If still running: [code] If complete: [code] Listing All Active Builds Natural language: [code] MCP tool: [code] Response: [code] Real-World Example Scenario: Build a full-stack app while working on documentation [code] Time saved: Instead of staring at the screen for 15 minutes, you worked on other tasks! Multiple Simultaneous Builds You can start multiple builds at once: [code] All three run in parallel! Check status with: [code] When to Use Synchronous (Blocking) Builds Most of the time, use async (default). Only use synchronous if: You're debugging the workflow You want to see live output as it happens You have a very short build (50%), Red (critical >75%) Real-time updates as task progresses Task Progress List of completed tasks (✓) Currently executing task (⏳) Pending tasks (○) Progress percentage bar Token Usage Panel 🔢 Used / Budget: e.g., \"45,234 / 200,000\" Visual Gauge: Gradient progress bar (green→yellow→red) Warning Thresholds: Safe: 75% (red - pulses) Updates every 3-5 seconds Test Loop Analytics 🧪 Total Iterations: Number of test/fix cycles Success Rate: Percentage of tests passing Iteration History: Last 3 iterations with pass/fail counts Visual Indicators: Green ✓ for passing, Red ✗ for failing Agent Performance 🤖 Per-Agent Cards: Scout, Architect, Builder, Tester Metrics: Execution time Success/failure status Issues found vs fixed Files created/modified Hover Effects: Cards highlight on mouseover Decision Quality 🧠 Total Decisions: Count of autonomous decisions made Average Quality: Rating 1-5 (Low, Medium, High) Lessons Applied: Number of times past patterns were used Recent Decisions: Last 3 decisions with quality badges Color Coding: High quality: Green Medium quality: Yellow Low quality: Red Lessons Indicator: Purple 📚 icon when lessons were applied Configuration Environment Variables [code] Polling Interval The default 3-5 second polling interval balances: Freshness: Near real-time updates Overhead: Minimal API traffic Responsiveness: Smooth UI updates To adjust: [code] API Endpoints The dashboard provides a REST API for programmatic access: [code] Metrics Database All metrics are persisted to SQLite for historical analysis: [code] Querying Metrics [code] Self-Improvement Integration The dashboard tracks metrics that help Context Foundry improve itself: Decision Quality Tracking Rates every autonomous decision (1-5) Tracks difficulty level Flags regrettable decisions to learn from Measures effectiveness of lessons learned Pattern Effectiveness Tracks which patterns were applied Measures if they prevented issues Correlates pattern usage with success rates Builds data for pattern refinement Agent Performance Analysis Measures time spent per agent type Tracks success rates by phase Identifies bottlenecks and inefficiencies Optimizes workflow over time Test Loop Analytics Monitors test iteration trends Identifies common failure patterns Measures fix effectiveness Reduces iterations over time through learning Dark Mode Design The dashboard uses a carefully crafted dark theme: Background: Deep black (#0a0a0a) for OLED-friendly viewing Panels: Dark gray (#111827) with subtle borders Text: High contrast (#e0e0e0) for readability Accents: Vibrant gradients for phase indicators Hover Effects: Smooth transitions and subtle glows Color Coding: Green: Success, safe levels Yellow: Warnings, caution Red: Critical, failures Blue: Info, in-progress Purple: Lessons learned, special features Perfect for overnight monitoring sessions! Remote Access With ngrok (Recommended) [code] Local Network [code] Troubleshooting Dashboard Server Won't Start [code] Dashboard Not Updating Check WebSocket Connection: Look for green \"Connected\" indicator Verify Task is Running: Ensure Context Foundry task is active Check Browser Console: Press F12, look for errors Refresh Page: Force reload with Cmd+Shift+R (Mac) or Ctrl+Shift+R Check Enhanced Metrics: If disabled, some panels may not populate Enhanced Metrics Not Available If you see \"⚠️ Enhanced metrics not available\": [code] Database Errors [code] Performance Latency: < 100ms from task update to dashboard Resource Usage: ~50MB RAM for server Concurrent Sessions: Tested with 10+ simultaneous tasks WebSocket Limit: 100 concurrent connections Database Size: ~1-5MB per 100 tasks Best Practices Start Dashboard Before Tasks: Launch dashboard first, then start builds Monitor Token Usage: Watch for yellow/red warnings Review Decision Quality: Identify patterns in high vs low quality decisions Analyze Test Failures: Use test loop panel to spot recurring issues Export Data: Periodically export session data for analysis Clean Up Old Tasks: Archive completed tasks to keep dashboard responsive Future Enhancements Planned features: [ ] Historical session comparison [ ] Performance trend graphs [ ] Desktop notifications on completion [ ] Mobile app [ ] Video recording of sessions [ ] Multi-user authentication [ ] Session sharing via URL [ ] Pattern library visualization [ ] Cost estimation improvements [ ] Integration with GitHub Issues --- Troubleshooting Prerequisites Setup Installing Python 3.10+ macOS (Homebrew): [code] Linux (Ubuntu/Debian): [code] Verify: [code] Installing Claude Code CLI Follow Anthropic's official installation guide for your platform. Verify: [code] Installing GitHub CLI macOS: [code] Linux: [code] Authenticate: [code] Troubleshooting Installation Error: \"No module named 'fastmcp'\" [code] Error: \"Python version too old\" Your system Python is < 3.10. Install Python 3.10+ (see above). Error: \"Permission denied\" Try with flag: [code] Troubleshooting MCP Connection Error: \"MCP server not found\" Check the path in your command: [code] Error: \"MCP server disconnected\" Check MCP configuration: [code] Should show: [code] Restart Claude Code: [code] MCP Tools Not Showing Up [code] If not showing: Verify MCP configuration: (project-scoped) or (global) Check for errors in configuration Restart Claude Code Troubleshooting Builds Build Hangs or Takes Too Long Increase timeout: [code] Check if build is actually running: [code] Tests Keep Failing (Max Iterations Reached) Check test failure reports: [code] Review fix attempts: [code] Options: Increase max iterations: [code] Disable test loop temporarily: [code] Fix manually and re-run GitHub Deployment Fails Check GitHub authentication: [code] Check GitHub CLI works: [code] Files Created in Wrong Directory Verify workingdirectory is absolute path: [code] Create directory first if it doesn't exist: [code] --- Best Practices Naming Projects Good project names: Lowercase with hyphens Descriptive GitHub-friendly [code] Working Directories Recommended: [code] Create directory first: [code] Timeout Settings Guidelines: | Project Type | Recommended Timeout | |--------------|---------------------| | Single file script | 2-5 minutes | | Simple CLI app | 5-10 minutes | | REST API | 10-20 minutes | | Full-stack app | 30-60 minutes | | Complex system | 60-120 minutes | Example: [code] Test Loop Settings Enable test loop when: ✅ Building production code ✅ Code has complex logic ✅ You want high quality output Disable test loop when: ✅ Rapid prototyping ✅ Throwaway code ✅ Debugging workflow issues Max iterations: 3 (default) - Good balance 5 - For complex projects 1 - See raw failures without auto-fix Task Descriptions Good task descriptions: [code] Sweet spot: Specific requirements, but let AI decide implementation details. --- Advanced Usage Customizing the Workflow Edit to customize the workflow: [code] Example customizations: Add a security audit phase: [code] Change test framework: [code] Add deployment to cloud: [code] Environment Variables Pass env vars to MCP server: Edit : [code] Using with Different GitHub Accounts [code] Inspecting Build Artifacts After a build, explore the directory: [code] These files help you understand what the AI did and debug issues. Resuming Failed Builds If a build fails, you can: Review the artifacts: [code] Fix manually and re-run: [code] Increase iterations: [code] --- Next Steps After Your First Successful Build Try more complex projects: Full-stack applications Microservices Game development Experiment with parallel execution: Build multiple components simultaneously Measure time savings Customize the workflow: Edit Add custom phases Adjust test strategies Read the technical docs: ARCHITECTUREDECISIONS.md for deep dives and 1.x vs 2.0 comparison Learning More README.md - Quick reference and overview ARCHITECTUREDECISIONS.md - Technical deep dives and what's new in 2.0 CLAUDECODEMCPSETUP.md - MCP setup and troubleshooting examples/ - Test scenarios and examples Getting Help GitHub Issues: Report bugs and request features GitHub Discussions: Ask questions and share projects Documentation: Start here, then technical docs --- Summary Cheat Sheet Quick Reference Start Claude Code: [code] Build a project (autonomous): [code] Delegate a simple task: [code] Parallel tasks: [code] Check MCP configuration: [code] View artifacts: [code] --- Happy Building! 🎉 Context Foundry 2.0 - Autonomous AI Development For more help: README.md | ARCHITECTUREDECISIONS.md","url":"/docs/getting-started/user-guide","weight":1.5,"tags":[],"headings":["Context Foundry 2.0 - User Guide","Table of Contents","Getting Started","Prerequisites Check","Installation","Step 1: Clone Context Foundry","Step 2: Install MCP Server Dependencies","Step 3: Configure MCP Connection","Step 4: Verify Connection","Step 5: Test the Setup","How Claude Code Recognizes Build Requests","🎯 Intent Detection","✅ Examples That Trigger Autonomous Build","❌ Examples That Won't Trigger (Just Explains)","💡 The Difference","🚀 No Need to Mention MCP Tools!","📝 Being Specific Gets Better Results","🎓 Learning the Pattern","💬 What If You're Unsure?","🔧 Advanced: Customize the Build","Basic Usage","Your First Build","Autonomous Builds","Building a Todo CLI App","Task Delegation","Simple Synchronous Delegation","Checking Working Directory","Parallel Execution","Building a Full-Stack App in Parallel","Step 1: Start All Tasks in Parallel","Step 2: Monitor Progress","Step 3: Collect Results","Performance Comparison","Background Builds","Why Background Builds?","How It Works","Starting a Background Build","Checking Build Status","Listing All Active Builds","Real-World Example","Multiple Simultaneous Builds","When to Use Synchronous (Blocking) Builds","Best Practices","Troubleshooting Background Builds","Understanding the Workflow","The 8-Phase Autonomous Workflow","Visual Walkthrough: Real Build Example","Phase 1: Scout (Research & Context Gathering)","Phase 2: Architect (Design & Planning)","Phase 3: Builder (Implementation)","Phase 4: Test (Validation & Quality Assurance)","Phase 4.5: Screenshot Capture (Visual Documentation) 📸 NEW!","Phase 5: Documentation","Usage","Testing","architecture.md (30-90KB)","build-log.md","test-results-iteration-X.md","fixes-iteration-X.md","session-summary.json","Pattern Library Locations","Reviewing Plans Before/After Build","Understanding the Delegation Model","Agent Lifecycle","For More Details","Real-Time Monitoring Dashboard","Features","Quick Start","1. Start the Dashboard","2. Run a Task","3. Monitor in Real-Time","Dashboard Panels","Main Status Panel","Context Usage","Task Progress","Token Usage Panel 🔢","Test Loop Analytics 🧪","Agent Performance 🤖","Decision Quality 🧠","Configuration","Environment Variables","Polling Interval","API Endpoints","Metrics Database","Querying Metrics","Self-Improvement Integration","Dark Mode Design","Remote Access","With ngrok (Recommended)","Local Network","Troubleshooting Dashboard","Server Won't Start","Dashboard Not Updating","Enhanced Metrics Not Available","Database Errors","Performance","Best Practices","Future Enhancements","Troubleshooting","Prerequisites Setup","Installing Python 3.10+","Installing Claude Code CLI","Installing GitHub CLI","Troubleshooting Installation","Error: \"No module named 'fastmcp'\"","Error: \"Python version too old\"","Error: \"Permission denied\"","Troubleshooting MCP Connection","Error: \"MCP server not found\"","Error: \"MCP server disconnected\"","MCP Tools Not Showing Up","Troubleshooting Builds","Build Hangs or Takes Too Long","Tests Keep Failing (Max Iterations Reached)","GitHub Deployment Fails","Files Created in Wrong Directory","Best Practices","Naming Projects","Working Directories","Timeout Settings","Test Loop Settings","Task Descriptions","Advanced Usage","Customizing the Workflow","Environment Variables","Using with Different GitHub Accounts","Inspecting Build Artifacts","Resuming Failed Builds","Next Steps","After Your First Successful Build","Learning More","Getting Help","Summary Cheat Sheet","Quick Reference"]},{"title":"Changelog","slug":"changelog","category":"guides","categoryName":"Guides","description":"All notable changes to Context Foundry will be documented in this file.","content":"Changelog All notable changes to Context Foundry will be documented in this file. The format is based on Keep a Changelog, and this project adheres to Semantic Versioning. --- [1.4.0] - 2025-01-13 🔌 BAML Full Integration: Complete implementation of BAML type-safe LLM outputs in the autonomous build system. 🚀 Added BAML Runtime Integration Full BamlRuntime API Implementation () Updated to use baml-py v0.211.2 API Implemented for type-safe phase tracking Implemented for structured Scout reports Implemented for architecture blueprints Implemented for Builder validation All functions call actual BAML LLM functions (not placeholders) CLI Tool for Orchestrator New CLI Tool () Status checking: Phase tracking: Scout reports: Architecture: Build validation: Easy bash integration for orchestrator scripts Graceful fallback to JSON when API keys not configured Orchestrator Integration BAML Section Added to Instructions for using BAML CLI tool Graceful fallback guidance API key requirements documented Optional usage pattern (doesn't break existing builds) Documentation Enhanced BAML Guide () API key setup instructions CLI tool usage examples Orchestrator integration patterns Complete end-to-end examples 🔧 Changed BAML Schema Fixes Fixed Enum Naming for BAML v0.211.2 compatibility : lowercase → Capitalized with : lowercase → Capitalized with : lowercase → Capitalized with All enums now comply with BAML naming requirements Backward compatible via annotations Integration Layer Replaced Placeholder Code with actual implementations All commented-out function calls now active Real BamlRuntime calls using Proper context manager usage Result parsing with ✨ Features Type-Safe Phase Tracking: Real-time BAML-validated phase updates Structured Scout Reports: Guaranteed schema compliance for Scout findings Validated Architecture: Type-checked architecture blueprints Build Result Validation: Type-safe Builder task results CLI Interface: Easy bash integration without Python imports API Key Detection: Automatic detection and status reporting Graceful Degradation: Falls back to JSON if BAML/API keys unavailable 📊 Technical Details BAML Version: v0.211.2 API: BamlRuntime (replaces deprecated BamlSyncClient) LLM Providers: Anthropic Claude, OpenAI GPT (configurable) Required Env Vars: or Backward Compatible: 100% - works without BAML/API keys ⚠️ Breaking Changes None - This release is fully backward compatible. BAML is optional. --- [1.3.0] - 2025-01-13 🎯 BAML Integration Release: Type-safe LLM outputs with BAML (Basically a Made-up Language) for improved reliability and structured responses. 🚀 Added BAML Integration BAML Schema Definitions for type-safe structured outputs - PhaseInfo, PhaseType, PhaseStatus - ScoutReport, TechStack, Challenge - ArchitectureBlueprint, TestPlan - BuildTaskResult, BuildError - Multi-provider LLM client configurations BAML Integration Module () Schema compilation and caching Type-safe validation functions Graceful JSON fallback mode Status checking and error reporting Optional Dependency - baml-py >= 0.211.0 Install with: Context Foundry works without it (JSON fallback) Testing Comprehensive test suite for BAML integration 19 unit tests for BAML integration module 26 schema validation tests 100% test coverage for new code All tests pass on first iteration Documentation BAML Integration Guide () Complete usage examples Installation instructions Migration path from JSON to BAML Troubleshooting guide Example Project () Task management with BAML Type-safe LLM integration demo Real-world usage patterns 🔧 Changed Requirements - Added optional BAML dependencies - Notes BAML as optional - New file for BAML-specific deps ✨ Benefits Reliability: Reduce phase tracking parsing errors from 5% to = 3) Pattern matching by project type for relevant application Frequency tracking to identify common vs rare issues Enhanced Phases with Self-Learning Scout (Phase 1) Reads past learnings before starting research Flags known risks from pattern library Warns about common issues for detected project types Example: Automatically flags CORS risk for browser apps with ES6 modules Architect (Phase 2) Applies proven architectural patterns automatically Includes preventive measures for known issues Adds dependencies/configurations that prevent common failures Example: Auto-includes http-server for browser apps with ES6 modules Test (Phase 4) Checks for known issues before running tests Runs pattern-based integration tests Validates against project-type-specific requirements Example: Verifies dev server configuration for browser apps Initial Patterns (Seeded from 1942 Clone Build) cors-es6-modules - CORS error with ES6 modules from file:// Root cause: Browsers block module imports from file:// protocol Solution: Include http-server in package.json from the start Auto-apply: TRUE for browser apps with ES6 modules unit-tests-miss-browser-issues - Unit tests don't catch browser integration issues Gap: Jest+jsdom mocks browser but doesn't catch CORS/module loading Solution: Add browser integration tests with Playwright/Selenium Auto-apply: TRUE for browser/web apps entity-component-game-architecture - Proven game architecture pattern Structure: Entity base classes + manager systems Benefits: Clean separation, 100% testable, easy to extend Success rate: 100% browser-es6-modules-risk-detection - Scout risk detection pattern Trigger: Project mentions browser/web AND uses ES6 modules Action: Flag CORS risk in scout-report.md Result: Issue prevented before it occurs 📚 Documentation FEEDBACKSYSTEM.md - Comprehensive 300+ line guide How Phase 7 works Pattern library details and schemas How patterns are applied in each phase Real-world example (1942 clone CORS issue) Pattern lifecycle from discovery to proven Metrics and analytics Best practices and FAQ Pattern Library README () Pattern file schemas and usage Auto-apply logic explanation Pattern lifecycle stages Best practices for manual pattern addition README.md Updates Added Feature #6: Self-Learning Feedback Loop Updated version to 2.0.1 🎯 Impact Continuous Improvement: Each build makes the next build smarter Proactive Prevention: Common issues prevented before they occur Reduced Iterations: Test iterations decrease over time as patterns accumulate Zero Manual Intervention: Feedback analysis and pattern application fully automated Knowledge Accumulation: Pattern library grows with every build Cross-Project Learning: Patterns portable across repositories 📊 Metrics From initial pattern seeding: Patterns Added: 4 Auto-Apply Enabled: 3 (75%) High-Severity Patterns: 2 Project Types Covered: browser-app, web-app, web-game, game, simulation --- [2.0.0] - 2025-10-18 🎉 Major Release: Complete architectural reimagining. Context Foundry now operates as an MCP server for Claude Code CLI with fully autonomous, self-healing workflows. 🚀 Added Core Features Autonomous Build & Deploy Tool () Complete Scout → Architect → Builder → Test → Documentation → Deploy workflow Fully autonomous operation (zero human intervention) Automatic GitHub deployment with detailed commit messages File-based context preservation in directory Configurable timeout (default: 90 minutes) Supports new projects, fixes, and enhancements Self-Healing Test Loops Automatic test failure analysis Architect redesigns solution when tests fail Builder re-implements fixes Tester re-validates (up to configurable max iterations) 95% auto-fix success rate within 3 iterations Detailed failure reports: Fix strategy documentation: Iteration tracking: Task Delegation System Synchronous delegation () Spawn fresh Claude Code instance Wait for completion Return full output Asynchronous delegation () Start tasks in background Continue working while task runs Track via unique task ID Result retrieval () Check task status (running/completed/timeout) Get stdout/stderr/duration Task monitoring () List all active and completed tasks View elapsed time for each Meta-Prompt Orchestration - 469 lines of plain-language workflow instructions AI self-orchestrates through phases using native Customizable without coding (edit text file) Supports custom phases and workflows File-Based Context System No token limit issues Context preserved across sessions All artifacts saved to : - Scout phase findings - Architect phase design - Builder phase log - Current test iteration - Test failure analysis - Fix strategies - Final test results - Complete session metadata Documentation ARCHITECTUREDECISIONS.md - Comprehensive technical deep dive Native vs Python SDK explanation Self-healing test loops detailed walkthrough Autonomous build/deploy implementation details Parallel async delegation architecture Meta-prompt orchestration philosophy Why features were removed (multi-provider, Python CLI, etc.) LEGACYREADME.md - Archived Context Foundry 1.x documentation ARCHITECTUREDECISIONS.md - What changed in 2.0 and why Side-by-side comparison of 1.x vs 2.0 Technical architecture decisions Migration guide FAQ USERGUIDE.md - Step-by-step practical guide Installation and setup Basic usage tutorials Autonomous builds walkthrough Task delegation examples Parallel execution guide Troubleshooting section Best practices CLAUDECODEMCPSETUP.md - MCP server setup guide Prerequisites Installation steps Configuration Troubleshooting Advanced configuration Updated README.md for v2.0 Focus on MCP + Claude Code integration Quick start guide Tool reference Real-world examples Performance metrics examples/testclaudecodedelegation.md - Test scenarios and examples ⚡ Changed Architecture Orchestration Method: FROM: Python scripts () managing API calls TO: Meta-prompts () enabling AI self-orchestration Agent Creation: FROM: Anthropic Agent SDK with Python classes TO: Native Claude Code command Tool Access: FROM: Custom Python function wrappers TO: Native Claude Code tools (Read, Edit, Bash, Glob, Grep) Context Management: FROM: API conversation history (200K token limit) TO: File-based artifacts (no limits) Deployment: FROM: Manual git operations TO: Automatic GitHub deployment with CLI Cost Model: FROM: Pay-per-token API calls ($3-10 per project) TO: Claude Max subscription ($20/month unlimited) Testing Test Workflow: FROM: Manual checkpoint reviews when tests fail TO: Automatic self-healing (redesign → re-implement → re-test) Test Iterations: FROM: Single test run per build TO: Up to 3 auto-fix iterations (configurable) User Experience Autonomy Level: FROM: Checkpoints require human review/approval TO: Fully autonomous (walk away builds) Duration: FROM: 15-30 minutes avg (with human interaction) TO: 7-15 minutes avg (fully autonomous) Success Rate: FROM: 85% (checkpoint-based quality control) TO: 95% (self-healing quality assurance) 🗑️ Removed Features Deprecated Multi-Provider Support (7 providers → Claude only) Removed: Support for OpenAI, Google Gemini, Groq, Cloudflare, Fireworks, Mistral Reason: Focus on quality over variety; MCP integration specific to Claude Code Mitigation: Context Foundry 1.x still available for multi-provider needs Impact: 890 lines of provider adapter code removed Python CLI ( command) Removed: Standalone CLI command Reason: Redundant with Claude Code CLI Mitigation: 1.x Python CLI preserved in LEGACYREADME.md Impact: Users interact with Claude Code directly Context Compaction Removed: Automatic summarization at 50% context usage Reason: File-based context eliminates token limits Impact: No more context loss from summarization Cost Tracking Removed: Detailed per-phase cost tracking and reporting Reason: Claude Max flat-rate pricing makes it less relevant Impact: Simplified codebase, users can manually calculate if needed Pattern Library (postponed to 2.1) Removed: Automatic pattern extraction and reuse system Reason: Scout research replaces static patterns with current best practices Impact: Patterns always up-to-date via web search vs outdated static library Future: May be added in 2.1 as optional enhancement 🔧 Technical Changes Codebase Size Reduction: FROM: ~3000 lines (Python orchestration + provider adapters + utilities) TO: ~1400 lines (MCP server + meta-prompt) Impact: 53% reduction, easier to maintain Dependencies: Added: , Removed: , , , provider SDKs Impact: Simpler dependency tree Process Management Subprocess Handling: Fixed delegation hanging with Added flag Added to prevent MCP recursion Added environment variable Async Task Management: for non-blocking execution Global task registry with UUIDs Timeout enforcement Status polling 📊 Performance Improvements Build Speed: 2x faster avg (7-15 min vs 15-30 min) Cost Efficiency: 95% reduction for heavy users Parallel Speedup: 3-10x on multi-component projects Auto-Fix Success: 95% within 3 iterations Test Coverage: 90%+ on generated code 🔄 Breaking Changes For Users Installation: Now requires MCP server setup vs simple Commands: Use MCP tools in Claude Code vs CLI commands Provider: Claude only vs 7 provider options Cost Model: Subscription vs pay-per-use (may be more expensive for < 5 projects/month) For Developers API: All Python SDK imports removed Architecture: Meta-prompt based vs Python orchestration Extensibility: Edit vs Python code 🐛 Bug Fixes Fixed subprocess hanging during delegation (stdin issue) Fixed MCP recursion when spawning child processes Fixed permission prompts blocking automation Fixed context overflow with file-based approach 🔒 Security Added safeguard (renamed to ) Added to prevent recursive MCP loading Environment variable isolation for child processes 📦 Migration Guide From Context Foundry 1.x to 2.0: Install MCP server: [code] Change workflow: [code] Verify results: Check directory for artifacts Review GitHub deployment For multi-provider users: Context Foundry 1.x remains available (see LEGACYREADME.md) Both versions can coexist For cost-conscious users: 2.0 cheaper if building 5+ projects/month 1.x cheaper for occasional use (< 5 projects/month) 📈 Statistics Lines of Code: 3000 → 1400 (53% reduction) Dependencies: 15 → 2 (87% reduction) Avg Build Time: 15-30 min → 7-15 min (50% faster) Cost (heavy users): $300-1000/month → $20/month (95% savings) Auto-Fix Success: N/A → 95% Context Limit: 200K tokens → Unlimited (file-based) 🎯 Goals Achieved ✅ Fully autonomous builds (walk away) ✅ Self-healing test loops (auto-fix failures) ✅ Parallel task execution (3-10x speedup) ✅ Unlimited context (file-based artifacts) ✅ Automatic deployment (GitHub integration) ✅ Simpler codebase (53% reduction) ✅ Lower cost for heavy users (95% savings) ✅ Meta-prompt extensibility (no coding needed) 🙏 Credits Anthropic's Claude Code - Native agent capabilities and MCP protocol Context Foundry 1.x - Original Scout/Architect/Builder workflow Dexter Horthy - \"Anti-vibe coding\" methodology Anthropic Agent SDK - Agent orchestration patterns --- [1.0.0] - 2024-XX-XX Added Initial release of Context Foundry Scout → Architect → Builder three-phase workflow Multi-provider AI support (7 providers) Python CLI ( command) Context compaction at 50% usage Pattern library with semantic search Cost tracking and optimization Git integration and checkpointing Session analysis and reporting MCP mode (terminal-based) Features - Create new projects - Fix bugs in existing projects - Add features to existing projects - Monitor progress - Session analysis - List available models - Update pricing data Providers Supported Anthropic (Claude) OpenAI (GPT) Google (Gemini) Groq Cloudflare Fireworks Mistral --- Version Comparison | Version | Release Date | Key Features | Status | |---------|--------------|--------------|--------| | 2.0.0 | 2025-10-18 | MCP server, self-healing, autonomous | ✅ Active | | 1.0.0 | 2024-XX-XX | Python CLI, multi-provider | 📦 Legacy | --- Upgrade Path From 1.0.0 to 2.0.0 Recommended for: Heavy users (5+ projects/month) Those wanting autonomous builds Claude Code users Steps: Install MCP server dependencies Configure MCP connection Verify with test build Migrate workflows to MCP tools See: ARCHITECTUREDECISIONS.md for detailed migration guide Need v1.0 (legacy)? v1.0 codebase is preserved in branch Download: release Use for: Multi-provider support, non-Claude LLMs --- Links GitHub: https://github.com/snedea/context-foundry Documentation: README.md Architecture: ARCHITECTUREDECISIONS.md User Guide: USERGUIDE.md Legacy Docs: LEGACYREADME.md --- Maintained by: Context Foundry Team License:** MIT","url":"/docs/guides/changelog","weight":1.2,"tags":[],"headings":["Changelog","[1.4.0] - 2025-01-13","🚀 Added","BAML Runtime Integration","CLI Tool for Orchestrator","Orchestrator Integration","Documentation","🔧 Changed","BAML Schema Fixes","Integration Layer","✨ Features","📊 Technical Details","⚠️ Breaking Changes","[1.3.0] - 2025-01-13","🚀 Added","BAML Integration","Testing","Documentation","🔧 Changed","✨ Benefits","📊 Technical Details","[2.1.0] - 2025-10-24","🚀 Added","Multi-Mode Support","Intelligent Detection System","Phase 0: Codebase Analysis (NEW!)","Enhancement-Aware Orchestrator","🛠️ Fixed","📚 Changed","📊 Impact","🎓 Live Testing","[2.0.2] - 2025-10-19","🚀 Added","Phase 4.5: Screenshot Capture (Visual Documentation)","Enhanced Documentation Phase","Enhanced Deployment Phase","Self-Learning Integration","🔧 Changed","📚 Documentation","💡 Benefits","[2.0.1] - 2025-10-18","🚀 Added","Phase 7: Feedback Analysis","Enhanced Phases with Self-Learning","Initial Patterns (Seeded from 1942 Clone Build)","📚 Documentation","🎯 Impact","📊 Metrics","[2.0.0] - 2025-10-18","🚀 Added","Core Features","Documentation","⚡ Changed","Architecture","Testing","User Experience","🗑️ Removed","Features Deprecated","🔧 Technical Changes","Codebase","Process Management","📊 Performance Improvements","🔄 Breaking Changes","For Users","For Developers","🐛 Bug Fixes","🔒 Security","📦 Migration Guide","📈 Statistics","🎯 Goals Achieved","🙏 Credits","[1.0.0] - 2024-XX-XX","Added","Features","Providers Supported","Version Comparison","Upgrade Path","From 1.0.0 to 2.0.0","Links"]},{"title":"Feedback System","slug":"feedback-system","category":"guides","categoryName":"Guides","description":"Making Context Foundry Smarter with Every Build","content":"Context Foundry Self-Learning Feedback System Making Context Foundry Smarter with Every Build Version: 1.0.0 | Last Updated: October 18, 2025 --- Overview Context Foundry now includes a Phase 7: Feedback Analysis that runs after every build (success or failure). This phase analyzes what happened, extracts learnings, and stores them in a pattern library that future builds can leverage. Result: Context Foundry gets smarter over time, preventing issues before they occur. --- The Feedback Loop [code] Key Insight: Every build makes the next build better. --- How It Works Phase 7: Feedback Analysis Runs: After Phase 6 (Deploy) on success, or after Phase 4 (Test) on failure What It Does: Collects build data: Reads all artifacts (.context-foundry/) Reviews test iterations and failures Analyzes what worked and what didn't Identifies root causes of issues Categorizes feedback: Scout improvements (missing research/risk flags) Architect improvements (design gaps/preventive measures) Builder improvements (implementation patterns) Test improvements (coverage gaps/integration tests) Extracts patterns: Identifies if issue is recurring or one-time Determines applicable project types Documents proven solutions Assigns severity (HIGH/MEDIUM/LOW) Updates pattern library: Creates new pattern entries Increments frequency for existing patterns Updates lastseen dates Enables auto-apply for proven patterns Generates recommendations: Specific actions for each phase Priorities (what to fix first) Expected impact Implementation guidance Output Files: (this build's analysis) (updated) (updated) (updated) (updated) (improvement suggestions) --- Pattern Library Storage Locations Context Foundry has TWO pattern libraries with different purposes: Global Pattern Library (shared across ALL builds) Location: [code] Purpose: ✅ Shared knowledge across ALL builds ✅ Automatically READ by Scout/Architect/Test phases ✅ Auto-apply when conditions match ✅ Updated by Phase 7: Feedback (promotes valuable patterns) Per-Project Pattern Library (build-specific) Location: [code] Purpose: ✅ Documents patterns discovered during THIS build ✅ Analyzed by Phase 7: Feedback ✅ Valuable patterns promoted to global library ✅ Archive of what was learned from this project How they work together: [code] View global patterns: [code] View project-specific patterns: [code] Pattern Files common-issues.json General issues across all project types Example: CORS errors, module loading problems, dependency conflicts test-patterns.json Testing strategy improvements Example: Unit tests miss browser issues, integration test needs architecture-patterns.json Proven design patterns Example: Entity-component architecture for games scout-learnings.json Risk detection and research insights Example: Flag CORS risk for browser apps with ES6 modules Pattern Schema [code] --- How Patterns Are Applied Phase 1: Scout (Updated) Before starting research: Reads Reads Identifies project type from task description Checks for patterns matching this type Flags known risks in scout-report.md Example: [code] Phase 2: Architect (Updated) During design: Reads Reads Checks Scout's flagged risks Applies proven architectural patterns Includes preventive measures Example: [code] Phase 4: Test (Updated) Before running tests: Reads Reads Checks for project-type-specific test needs Runs additional integration tests if indicated Example: [code] --- Real-World Example: 1942 Clone The Problem Build completed: ✅ 86 unit tests passing ✅ Deployed to GitHub ❌ Game stuck at loading screen (CORS error) Root cause: Jest+jsdom doesn't test actual browser environment. Feedback Analysis Phase 7 analyzed the build: [code] Pattern Created Added to : [code] Next Build Improvement When building next browser game with ES6 modules: Scout: Automatically flags CORS risk Architect: Automatically includes http-server Test: Automatically verifies server config Result: Issue prevented, no manual intervention needed Time saved: ~8 minutes debugging + better UX --- Pattern Lifecycle Discovery (Frequency: 1) [code] Validation (Frequency: 2-3) [code] Proven (Frequency: 5+) [code] Maintenance [code] --- Metrics & Analytics Build Quality Metrics Track over time: Test iterations trend: Should decrease as patterns prevent issues Build success rate: Should increase to 95%+ Common issue prevention: Track how often patterns prevent problems Average build duration: Should stabilize/decrease Pattern Library Metrics Total patterns: Growth indicates learning accumulation Auto-apply ratio: Higher = more automation Pattern frequency: Identifies most common issues Coverage: Project types represented Example Metrics Dashboard [code] --- Integration with Autonomous Builds MCP Tool Integration The tool automatically: Runs Phases 1-6 (as before) Runs Phase 7 (Feedback Analysis) Updates pattern library Returns feedback data in JSON No changes needed to existing builds! Async Builds Works with too: [code] --- Best Practices For Users ✅ Do: Review after builds Check pattern library growth over time Note when patterns prevent issues Provide feedback on pattern accuracy ❌ Don't: Disable feedback analysis (it's lightweight, ~1-2 min) Ignore high-priority recommendations Delete pattern library files Modify pattern files without understanding schema For Developers ✅ Do: Keep pattern library in version control (.context-foundry/patterns/) Share patterns across projects/teams Review and prune low-frequency patterns periodically Document custom patterns clearly Test auto-apply logic before enabling --- Configuration Enable/Disable Feedback Enabled by default. To disable: Edit : Comment out Phase 7 section Or set environment variable: Customize Pattern Application Edit pattern JSON files: Set to disable automatic application Adjust to refine when patterns apply Update based on real-world impact --- Troubleshooting Patterns Not Being Applied Check: Pattern file exists: Pattern Pattern conditions match project type Phase is reading pattern files (check orchestratorprompt.txt) Debug: Review scout-report.md for flagged risks Check architecture.md for applied patterns Look for pattern mentions in build artifacts Feedback Analysis Fails Common causes: Malformed JSON in pattern files Missing .context-foundry/test-results-.md Feedback agent crashed Solution: Validate JSON: Check .context-foundry/errors.md for details Review orchestrator execution logs --- Future Enhancements Planned Features ML-Based Pattern Detection Automatically identify new patterns without manual analysis Predict likely issues before they occur Cross-Project Learning Share patterns across multiple repositories Team-wide pattern libraries Pattern Confidence Scores Track success rate when applied Adjust auto-apply based on confidence Visualization Dashboard View pattern library growth See metrics trends over time Identify most impactful patterns Pattern Testing Automated validation of patterns A/B testing: with vs without pattern Measure effectiveness quantitatively --- Contributing Patterns Want to contribute patterns to the community? Build projects with Context Foundry Review generated feedback files Identify high-value, reusable patterns Submit PR with: Pattern JSON Real-world example Success metrics Documentation --- FAQ Q: Does feedback analysis slow down builds? A: Minimal impact. ~1-2 minutes per build, fully automated. Q: Can I use patterns from one machine on another? A: Yes! Copy directory. Patterns are portable. Q: What if a pattern is wrong? A: Edit the pattern JSON file or set to disable. Q: How many patterns before it's \"smart enough\"? A: Quality over quantity. 10-20 well-defined patterns cover 80% of issues. Q: Can I share patterns with my team? A: Yes! Commit to your repo or share the directory. Q: Does this work with all project types? A: Yes. Patterns are tagged by project type, so they only apply when relevant. --- Summary Context Foundry's feedback system creates a self-improving development workflow: ✅ Learns from every build automatically ✅ Prevents issues before they occur ✅ Reduces test iterations over time ✅ Improves build quality continuously ✅ No manual intervention required The more you use Context Foundry, the smarter it gets. --- Version: 1.0.0 Last Updated: October 18, 2025 Status: Production Ready Learn More: README.md | ARCHITECTUREDECISIONS.md","url":"/docs/guides/feedback-system","weight":1.2,"tags":[],"headings":["Context Foundry Self-Learning Feedback System","Overview","The Feedback Loop","How It Works","Phase 7: Feedback Analysis","Pattern Library","Storage Locations","Pattern Files","Pattern Schema","How Patterns Are Applied","Phase 1: Scout (Updated)","Phase 2: Architect (Updated)","Phase 4: Test (Updated)","Real-World Example: 1942 Clone","The Problem","Feedback Analysis","Pattern Created","Next Build Improvement","Pattern Lifecycle","1. Discovery (Frequency: 1)","2. Validation (Frequency: 2-3)","3. Proven (Frequency: 5+)","4. Maintenance","Metrics & Analytics","Build Quality Metrics","Pattern Library Metrics","Example Metrics Dashboard","Integration with Autonomous Builds","MCP Tool Integration","Async Builds","Best Practices","For Users","For Developers","Configuration","Enable/Disable Feedback","Customize Pattern Application","Troubleshooting","Patterns Not Being Applied","Feedback Analysis Fails","Future Enhancements","Planned Features","Contributing Patterns","FAQ","Summary"]},{"title":"Roadmap","slug":"roadmap","category":"guides","categoryName":"Guides","description":"Context Foundry supports two execution modes:","content":"Context Foundry Roadmap Current Status: v1.2 Context Foundry supports two execution modes: ✅ MCP Mode (Terminal-Based MCP Server): ✅ Fully implemented and functional ✅ Uses Anthropic API key (same as CLI mode) ✅ Terminal-based MCP server via ⚠️ Future: Claude Desktop integration (would use subscription instead of API charges - blocked by sampling support) 💳 API Mode (Paid): ✅ Standalone CLI for automation and CI/CD ✅ Direct Anthropic API access ✅ Traditional command-line operation Core Features (Both Modes): ✅ Scout → Architect → Builder workflow ✅ Automated context management ✅ Pattern library learning ✅ Local git commits ✅ Human review checkpoints Planned Features 🎯 Medium Priority: Better Codebase Understanding Status: Research Phase What: Enhanced Scout agent that deeply understands existing code. Features: AST parsing for accurate code structure Dependency graph analysis Test coverage detection Architecture pattern recognition Integration with LSP for better code intelligence Estimated Completion: Q2 2025 🎛️ Medium Priority: Temperature Control Status: Planned What: Add configurable temperature settings for more deterministic/creative outputs. Proposed Defaults: Scout phase: 0.7 (allow creativity in architecture design) Architect phase: 0.5 (balanced - structured but flexible) Builder phase: 0.3 (deterministic code generation) Configuration: [code] Benefits: More consistent code generation Reduced hallucinations in Builder phase Allow creativity where it matters (Scout/Architect) User override for specific use cases Estimated Completion: Q2 2025 🔧 Low Priority: Interactive Mode Improvements Status: Ideas Phase What: Better UX for review checkpoints. Ideas: TUI (Text User Interface) for reviewing plans Side-by-side diff view for proposed changes Interactive task reordering Inline comments on specs/plans Better visualization of context usage Estimated Completion: Q3 2025 🌐 Low Priority: Multi-Repo Support Status: Ideas Phase What: Handle changes across multiple repositories. Use Case: [code] Estimated Completion: Q4 2025 Completed Features v1.0 - Initial Release ✅ Three-phase workflow (Scout → Architect → Builder) ✅ Automated Context Engineering (ACE) ✅ Pattern library with semantic search ✅ Smart context compaction ✅ Interactive and autonomous modes ✅ Livestream dashboard ✅ Session analysis and metrics v1.1 - Authentication Improvements ✅ Attempted Claude CLI integration (later found to still use API) ✅ Auto-detection of auth method ✅ Environment variable configuration ✅ Documentation updates ⚠️ Note: Claude CLI mode removed in v1.2 - was misleading as it still charged API fees v1.2 - MCP Server Mode (October 2025) Major Feature: Dual-Mode Architecture Context Foundry now supports two execution modes, giving users flexibility based on their needs: ✅ MCP Mode (Terminal-Based - Claude Desktop Integration Pending) ✅ MCP (Model Context Protocol) server implementation using FastMCP (complete and functional) ✅ Terminal-based MCP server works with API keys ⚠️ Future: Claude Desktop integration blocked by lack of sampling support (would use subscription instead of API charges) ✅ Three MCP tools implemented: , , ✅ command to start MCP server ⚠️ Tools return helpful error messages explaining current limitation ✅ Automatic configuration help with 💳 API Mode (Continues to Work) ✅ Standalone CLI operation via ✅ Direct Anthropic API integration ✅ Works with Python 3.9+ ✅ Good for CI/CD and automation 📦 Two-Tier Dependency Architecture ✅ Base installation works with Python 3.9+ () ✅ Optional MCP mode requires Python 3.10+ () ✅ Graceful degradation - users only install what they need ✅ Clear error messages guiding users to correct setup 🏗️ Technical Implementation ✅ Created for MCP mode (mirrors interface) ✅ Factory function selects appropriate client ✅ MCP server with three tools for building, enhancing, and status checking ✅ Fixed package structure - added to all package directories ✅ Removed misleading 📚 Comprehensive Documentation ✅ New with installation guide, troubleshooting, and lessons learned ✅ New with step-by-step MCP configuration ✅ Configuration template in ✅ Documented 8 key lessons learned from implementation ✅ Common issues section with 6 specific problems and solutions ✅ Verification checklists for both modes 🎓 Lessons Learned Python version management is critical - two-tier architecture solves compatibility Package structure matters - requires files PATH management is non-trivial - document exactly where scripts install Editable installs can be tricky - explicit Python version helps Dependencies should be gradual - optional features = optional dependencies Error messages should be helpful - guide users with exact commands Documentation should match reality - write after debugging, not before Two modes require two strategies - shared core with different clients Key Insight: Users discovered that the original \"Claude CLI\" mode still charged API fees, leading to this comprehensive dual-mode implementation that provides true free usage via MCP while maintaining API mode for automation. Completed: October 4, 2025 v2.1.0 - Enhancement Mode (October 2025) Major Feature: Fix, Enhance, and Upgrade Existing Codebases Context Foundry can now intelligently modify existing projects instead of only building from scratch: ✅ Multi-Mode Support ✅ - Build from scratch (original functionality) ✅ - Fix bugs in existing code ✅ - Add features to existing code ✅ - Upgrade dependencies ✅ - Refactor existing code ✅ - Add tests to existing code ✅ Intelligent Detection System ✅ Automatic codebase detection (15+ project types supported) Python (, , ) Node.js (, ) Rust (), Go (), Java () Ruby (), PHP (), .NET () And more... ✅ Intent detection from natural language keywords \"fix\" → , \"add\" → , \"upgrade\" → ✅ Auto-mode adjustment with conflict warnings ✅ Git repository status checking ✅ Confidence scoring (high/medium/low) ✅ Phase 0: Codebase Analysis (NEW!) ✅ Runs ONLY for enhancement modes (skipped for new projects) ✅ Analyzes project structure, architecture, and existing tests ✅ Reviews git history and current branch state ✅ Creates report for context ✅ Provides full understanding before making changes ✅ Enhancement-Aware Orchestrator ✅ Scout phase: Mode-specific analysis strategies Target ed bug finding for Integration point analysis for Dependency impact assessment for ✅ PHASE 2.5 (Parallel Builders): Targeted modifications Modifies existing files instead of creating new projects Preserves existing code structure and patterns Creates feature branches before making changes Groups changes by logical components ✅ Test phase: Validates existing functionality preserved ✅ Deploy phase: Feature branch + Pull Request workflow Pushes to feature branch (NOT main) Creates PR with detailed description Links to GitHub issues () Requires human review before merge ✅ Testing & Validation ✅ Standalone test script () ✅ Live test: YouTube Transcript Summarizer enhancement Added auto-save markdown feature 21 new tests (all passing) Created PR #2 on feature branch Zero impact on existing functionality Completed in ~12 minutes, 1 test iteration 📊 Impact 🎯 Codebase Coverage: Detects Python, JavaScript/TypeScript, Rust, Go, Java, Ruby, PHP, .NET, C/C++, and more 🚀 Success Rate: 100% on first live test (YouTube Transcript Summarizer) 📝 Code Quality: Professional PR workflow with detailed descriptions ⚡ Efficiency: Feature branches + PRs enable safe, reviewable changes 🔧 Technical Implementation (lines 768-1165): Detection and integration - 15+ project types - keyword-based mode detection - integrated detection and warnings : Enhancement-aware phases Mode detection section (lines 49-70) Phase 0: Codebase Analysis (lines 71-175) Scout enhancement guidance (lines 257-295) PHASE 2.5 targeted modifications (lines 481-514) Deploy PR workflow (lines 1237-1407) : Standalone validation script 📚 Use Cases [code] Key Insight: Enhancement mode transforms Context Foundry from a greenfield development tool into a comprehensive code evolution platform. It can now fix bugs, add features, upgrade dependencies, refactor code, and add tests to ANY existing project - whether built by Context Foundry or not. Completed: October 24, 2025 v2.2.0 - GitHub Agent (October 2025) Major Feature: Comprehensive GitHub Integration & Automation Context Foundry now sets up complete GitHub project infrastructure automatically: ✅ Phase 7.5: GitHub Integration Agent ✅ Dedicated GitHub agent with specialized prompt ✅ Intelligent project type detection ✅ Autonomous GitHub configuration ✅ Full CI/CD workflow generation ✅ Professional project setup from day 1 ✅ Issue Tracking & Project Management ✅ Automatic issue creation from Scout reports ✅ Issue-commit-PR linking ✅ Issue closure on completion ✅ Standard labels for project organization ✅ Issue/PR templates for collaboration ✅ CI/CD Automation (GitHub Actions) ✅ Test workflow generation (automatic) ✅ Deployment workflow for web apps (GitHub Pages) ✅ Docker build & publish workflow (GHCR) ✅ Context-aware workflow creation based on project type ✅ Branch protection rules for new projects ✅ Release Management ✅ Automatic version detection from package files ✅ Git tag creation and push ✅ GitHub release with generated changelog ✅ Test results and build metadata in release notes ✅ Links to documentation and artifacts ✅ Deployment Integration ✅ GitHub Pages auto-setup for web applications ✅ Live demo link added to README ✅ Automatic deployment on push to main ✅ Multi-platform Docker builds (if applicable) ✅ Enhancement Mode Integration ✅ Draft PR creation for tracking progress ✅ PR updates as build progresses ✅ Automatic PR readiness marking ✅ Issue-PR linking with \"Closes #N\" 🏗️ Technical Implementation - Comprehensive agent instructions (800+ lines) Phase 1: Project type detection (web app, CLI, API, library, container) Phase 2: Issue creation and tracking Phase 3: Labels and templates Phase 4: CI/CD workflows (test, deploy, docker) Phase 5: Release creation with changelog Phase 6: GitHub Pages setup Phase 7: Branch protection Phase 8: Issue updates and closure - Phase 7.5 integration (lines 1736-1849) - Extended schema with GitHub metadata - Complete feature specification 📊 Session Summary Schema v2.0 New metadata object includes: [code] 🎯 What Users Get Automatically For New Projects: 🎫 Tracking issue created with Scout report ⚙️ GitHub Actions CI/CD workflows 🏷️ Standard labels (context-foundry, autonomous-build, etc.) 📋 Issue/PR templates for collaboration 📦 GitHub release with changelog 🌐 GitHub Pages deployment (web apps) 🔒 Branch protection on main 📚 Professional README with badges and links For Enhancements: 🎫 Tracking issue for the fix/feature 🔀 Draft PR with progress tracking 🔗 Issue-PR-commit linking ✅ Automatic PR readiness marking 📝 Test results in PR description 📈 Benefits Professional Setup: Projects look mature from day 1 Full Automation: CI/CD runs automatically on every push Better Tracking: Complete audit trail (Issue → PR → Release) Easy Deployment: GitHub Pages live immediately Collaboration Ready: Templates and guidelines in place Showcase Quality: Autonomous builds are deployment-ready 🎓 Design Decisions Dedicated Agent: Sophisticated decision-making based on project type Intelligent Detection: Reads Scout/Architect context to customize setup Graceful Degradation: Optional features don't block build completion Context-Aware: Different workflows for web apps vs APIs vs libraries Enhancement-Friendly: Respects existing project settings Error Resilient: Continues on non-critical failures 📝 Example: Web App Build Before GitHub Agent: [code] After GitHub Agent: [code] Key Insight: The GitHub Agent elevates Context Foundry builds from \"code pushed to GitHub\" to \"fully automated, deployment-ready, professionally managed projects.\" Every build now includes comprehensive CI/CD, release management, and collaboration infrastructure - no manual setup required. Completed: October 24, 2025 Contributing Want to help implement these features? Check out our CONTRIBUTING.md guide! Feedback Have ideas for features not on this roadmap? Open an issue on GitHub: https://github.com/snedea/context-foundry/issues --- Last Updated: 2025-10-24","url":"/docs/guides/roadmap","weight":1.2,"tags":[],"headings":["Context Foundry Roadmap","Current Status: v1.2","Planned Features","🎯 Medium Priority: Better Codebase Understanding","🎛️ Medium Priority: Temperature Control","🔧 Low Priority: Interactive Mode Improvements","🌐 Low Priority: Multi-Repo Support","Completed Features","v1.0 - Initial Release","v1.1 - Authentication Improvements","v1.2 - MCP Server Mode (October 2025)","v2.1.0 - Enhancement Mode (October 2025)","v2.2.0 - GitHub Agent (October 2025)","Contributing","Feedback"]},{"title":"Security","slug":"security","category":"guides","categoryName":"Guides","description":"Use this section to tell people about which versions of your project are currently being supported with security updates.","content":"Security Policy Supported Versions Use this section to tell people about which versions of your project are currently being supported with security updates. | Version | Supported | | ------- | ------------------ | | 5.1.x | :whitecheckmark: | | 5.0.x | :x: | | 4.0.x | :whitecheckmark: | | < 4.0 | :x: | Reporting a Vulnerability Use this section to tell people how to report a vulnerability. Tell them where to go, how often they can expect to get an update on a reported vulnerability, what to expect if the vulnerability is accepted or declined, etc.","url":"/docs/guides/security","weight":1.2,"tags":[],"headings":["Security Policy","Supported Versions","Reporting a Vulnerability"]},{"title":"15 Innovations","slug":"innovations","category":"technical","categoryName":"Technical","description":"The Technical Breakthroughs That Made Autonomous AI Development Possible","content":"Context Foundry Innovations The Technical Breakthroughs That Made Autonomous AI Development Possible Last Updated: January 23, 2025 Version: 2.0.2 Audience: Software architects, AI engineers, and technical decision-makers --- Table of Contents The Meta-MCP Innovation ⭐ Featured Innovation Self-Healing Test Loop Parallel Execution Architecture Subprocess Delegation with Auth Inheritance Meta-Prompt Orchestration File-Based Context System Markdown-First Design Global Pattern Learning System Screenshot Capture Phase Async Task Management Context Window Isolation Output Truncation Strategy TUI Real-time Monitoring Livestream Integration 8-Phase Workflow Architecture --- The Meta-MCP Innovation: The Breakthrough ⭐ The Innovation That Changed Everything What makes Context Foundry truly innovative isn't just that it uses an MCP server—it's how it uses it. The \"Meta\" Concept Most MCP servers expose external tools to Claude Code: [code] Context Foundry does something radically different: [code] The Recursive Loop [code] Why This Is \"Meta\" Traditional MCP: MCP server is a bridge to external systems One-way communication: Claude → MCP → External Tool → Response MCP server does NOT spawn more Claude instances Context Foundry (Meta-MCP): MCP server spawns Claude Code itself Recursive: Claude uses MCP to create more Claudes The system can spawn unlimited agents autonomously Claude orchestrates Claude orchestrating Claude... The Code That Makes It Possible In : [code] What Happens When You Call This [code] The Breakthrough Moment Context Foundry v1.x (Pre-Meta-MCP): [code] Context Foundry v2.0 (Meta-MCP): [code] Why This Enabled v2.0's Capabilities The meta-MCP innovation made possible: Self-Spawning Agents System can create unlimited agents on demand Each with fresh 200K context window No manual orchestration code needed Parallel Execution Spawn 8 builder agents simultaneously Each works independently Coordinate via filesystem Authentication Inheritance Spawned processes inherit No API keys needed Flat-rate pricing (Claude Max $20/month) Native Tool Access Agents use Read, Edit, Bash, Glob, Grep directly Work like a human in Claude Code CLI No Python wrapper functions needed Self-Healing Loops Test failures trigger automatic Architect→Builder→Test cycles Fresh Claude instances analyze and fix issues No human intervention required The \"Secret Sauce\" It's not just an MCP server. It's an MCP server that: Calls Claude Code from within Claude Code (recursive) Passes meta-prompts that instruct Claude to orchestrate itself Enables Claude to spawn more Claudes via Creates a self-sustaining, autonomous agent system This is the innovation. This is what makes Context Foundry different from every other AI coding tool. Preventing Infinite Recursion The flag: [code] Without this flag: [code] With : [code] Visual Comparison Traditional MCP (External Tools): [code] Context Foundry (Meta-MCP): [code] Real-World Impact Before (v1.x): Manual orchestration via Python scripts 3000 lines of orchestration code API calls cost $3-10 per project Limited to single-threaded execution Required constant Python maintenance After (v2.0 with Meta-MCP): Self-orchestration via meta-prompts 1400 lines total (53% reduction) Flat rate $20/month unlimited (Claude Max) Parallel execution (2-8 agents simultaneously) Prompts editable by non-programmers The Innovation Timeline [code] The meta-MCP innovation was the inflection point that made everything else possible. Why No One Else Is Doing This Most MCP servers are designed for: Database connections API integrations File system operations External service calls Context Foundry's meta-MCP approach requires: Deep understanding of Claude Code's system Careful subprocess management (stdin=DEVNULL, --strict-mcp-config) Meta-prompt design for self-orchestration File-based context coordination Auth inheritance without API keys It's technically complex and conceptually novel—which is why it's an innovation worth documenting. --- Self-Healing Test Loop The Innovation Fully autonomous test → fail → analyze → redesign → fix → retest cycle with zero human intervention. The Problem It Solves Traditional AI coding tools (Cursor, Copilot, etc.): [code] Result: Human in the loop for every test failure = not autonomous The Context Foundry Solution Self-healing loop (v2.0+): [code] Result: Fully autonomous, walk-away development Technical Implementation In (Phase 4): [code] File-Based Coordination Iteration tracking: [code] Real-World Example User request: \"Build Express API with JWT authentication\" Iteration 1: FAIL Test output: [code] Tester creates : [code] Architect reads failure, creates : [code]javascript // After: exports.login = async (req, res) => { try { const token = generateToken(user); res.json({ token }); } catch (error) { console.error('Login error:', error); res.status(500).json({ error: 'Authentication failed' }); } }; [code]javascript if (!process.env.JWTSECRET) { throw new Error('JWTSECRET environment variable required'); } [code] Builder reads fix plan, implements changes Iteration 2: PASS Test output: [code] Tester creates : [code] Success Metrics From real-world usage: 73% of failures fixed on iteration 1 22% fixed on iteration 2 4% fixed on iteration 3 1% reach max iterations and report failure Overall: 95% auto-fix success rate Configuration [code] Why This Is Innovative Other AI tools stop at test failures. Context Foundry: Analyzes why tests failed (Tester agent) Redesigns the solution (Architect agent) Implements fixes (Builder agent) Retests automatically Repeats until tests pass or max iterations reached Fully autonomous. No human in the loop. The Innovation's Impact Before (v1.x and competitors): Average time to fix test failures: 15-30 minutes (human involved) Success rate: 85% (some builds abandoned due to complexity) User must actively debug and guide fixes After (v2.0 self-healing): Average time to fix test failures: 2-3 minutes (autonomous) Success rate: 95% (self-healing handles most issues) User can walk away, come back to finished project --- Parallel Execution Architecture The Innovation Simultaneous execution of independent tasks using bash process spawning and topological sort dependency resolution. The Breakthrough Most AI coding tools execute sequentially: [code] Context Foundry executes in parallel: [code] Two Parallel Phases Phase 2.5: Parallel Builders When: After Architect creates file structure, before Test phase How it works: Architect creates : [code] Orchestrator uses topological sort: [code] Each parallel builder: [code] Phase 4.5: Parallel Tests When: After Builder phase, during Test phase How it works: Orchestrator identifies test types: [code] Spawn all tests simultaneously: [code] Each test agent: [code] Orchestrator aggregates results: [code] Topological Sort DAG Implementation Dependency graph example: [code] Execution order: [code] Algorithm: [code] Actual bash implementation: [code] Performance Metrics Real-world speedups: | Project Size | Sequential | Parallel | Speedup | |--------------|-----------|----------|---------| | Small (5-10 files) | 7 min | 6 min | 14% faster | | Medium (11-20 files) | 15 min | 10 min | 33% faster | | Large (21-50 files) | 30 min | 18 min | 40% faster | | Very Large (50+ files) | 60 min | 35 min | 42% faster | Optimal builder count: Small projects: 2-3 builders Medium projects: 4-5 builders Large projects: 6-8 builders Diminishing returns beyond 8 builders (orchestration overhead) Preventing Race Conditions File uniqueness guarantee: Architect ensures each file appears in exactly ONE task: [code] Validation in orchestratorprompt.txt: [code] If Architect makes a mistake: [code] Process Isolation Each parallel builder: Fresh subprocess () Independent 200K context window Isolated working directory (but shared filesystem) No communication between builders (coordination via files) Memory footprint: [code] The Innovation's Impact Enabled: 3-10x speedup on multi-component projects Scalability to large codebases (50+ files) Efficiency - maximizes Claude Max subscription value Reliability - file uniqueness prevents race conditions Why it's innovative: Most AI tools execute sequentially Parallel execution requires dependency resolution (topological sort) Bash process spawning is simpler than Python multiprocessing Authentication inheritance works seamlessly with subprocesses --- Subprocess Delegation with Auth Inheritance The Innovation Spawning fresh Claude Code instances that automatically inherit authentication from the parent process, eliminating the need for API keys. The Problem It Solves Traditional AI agent systems: [code] The Context Foundry Solution Subprocess delegation: [code] How Auth Inheritance Works Claude Code stores auth in : [code] When spawning subprocess: [code] No API key in environment variables. No separate authentication. The Critical Subprocess Configuration Full subprocess setup: [code] Why Is Critical Without it, delegation hangs: [code] With : [code] Comparison: API-Based vs Subprocess Delegation API-Based (Traditional): [code] Subprocess Delegation (Context Foundry): [code] Cost Analysis Break-even point: [code] For heavy users: [code] Async Subprocess Delegation Non-blocking execution: [code] Check results later: [code] The Innovation's Impact Enabled: Cost reduction: 95% savings for heavy users Simplicity: No API key management Scalability: Spawn unlimited subprocesses Parallel execution: Multiple Claude instances simultaneously Flat-rate pricing: Predictable costs Why it's innovative: Most AI tools require API keys and pay-per-use Context Foundry leverages Claude Max subscription via subprocess spawning Authentication inheritance is seamless and secure Enables truly autonomous, unlimited agent creation --- Meta-Prompt Orchestration The Innovation Using natural language text prompts to instruct AI how to orchestrate itself through complex workflows, replacing traditional code-based orchestration. Code-Based Orchestration (Traditional) Example: Python orchestration script [code] Problems: 800+ lines of rigid Python logic Hard-coded workflow (can't adapt to edge cases) Requires Python expertise to modify Must handle every edge case explicitly Error-prone state management Meta-Prompt Orchestration (Context Foundry) Example: (1200 lines of natural language) [code] Advantages: 1200 lines of flexible instructions, not rigid code AI can adapt to edge cases using reasoning Non-programmers can read and understand Easy to modify (just edit text file) AI interprets intent, not just executes commands How It Works User triggers build: [code] MCP server loads meta-prompt: [code] Spawns fresh Claude with system prompt: [code] Claude reads meta-prompt and self-orchestrates: [code] Returns JSON when complete: [code] Adaptability Example Scenario: User asks for a \"multiplayer game\" Code-based orchestrator: [code] Meta-prompt orchestrator: [code] Result: [code] Editing Workflows Code-based orchestration: [code] Meta-prompt orchestration: [code] Real-World Customization Example User wants to add \"Performance Benchmarking\" phase: Edit : [code] That's it! No Python code. No deployment. Just edit the text file and Context Foundry now includes performance benchmarking. The Innovation's Impact Enabled: Non-programmer customization - Edit workflows without coding Rapid iteration - Change workflow in minutes, not hours AI adaptability - System can handle edge cases intelligently Reduced codebase - 1200 lines of text vs 3000+ lines of Python Natural language programming - Instructions readable by humans and AI Why it's innovative: Most systems use code for orchestration (rigid, requires programming) Context Foundry uses prompts (flexible, human-readable) AI interprets intent and adapts to unique situations Anyone can customize workflows by editing plain text The paradigm shift: [code] --- File-Based Context System The Innovation Using the filesystem as shared memory between isolated agent processes, eliminating API conversation history and token limit issues. The Problem with API-Based Context Traditional approach (API conversation history): [code] Problems: Token limit: Eventually hits 200K context window Cost: Every token sent on every call (exponential cost growth) Context loss: Compaction/summarization loses critical details Stateful: Must maintain conversation history across failures The File-Based Solution Context Foundry approach: [code] Each phase reads ONLY what it needs: [code] Implementation In : [code] Agents use native tools: [code] Advantages Over API Context No Token Limit Issues API-based: [code] File-based: [code] Lossless Context Preservation API-based with compaction: [code] File-based (no compaction needed): [code] Persistence Across Sessions API-based: [code] File-based: [code] Easy Debugging and Inspection API-based: [code] File-based: [code] Git-Trackable History API-based: [code] File-based: [code] File Structure Details Core workflow files: [code] Parallel execution files: [code] Pattern learning files: [code] The Innovation's Impact Enabled: Unlimited workflow complexity - No token limit constraints Lossless context - Full detail preservation Session resilience - Can resume after failures Easy debugging - Files are human-readable Version control - Git-trackable decision history Parallel execution - Agents coordinate via files Cross-session learning - Patterns persist across builds Why it's innovative: Most AI systems rely on API conversation context (limited, ephemeral) Context Foundry uses filesystem as shared memory (unlimited, persistent) Agents are stateless but context is durable Enables complex multi-agent workflows without token limits The paradigm shift: [code] --- Markdown-First Design The Innovation Using Markdown (.md) files as the primary format for inter-agent communication and system artifacts, prioritizing human and AI readability over machine parsability. Why Markdown? Context Foundry uses .md files for all core architecture: [code] Only structured data uses JSON: [code] The Reasoning LLMs Are Trained on Markdown Claude's training data: Markdown: Billions of documents (GitHub READMEs, docs, wikis) JSON: Smaller subset (API responses, config files) Plain text: Common but less structured Result: Claude reads and writes Markdown more naturally than JSON Example - Architect designing system: Markdown (natural): [code] JSON (unnatural): [code] Markdown advantages: Natural language explanations Code blocks with syntax highlighting Hierarchical structure (headings) Inline formatting (bold, italic, code) Lists (ordered and unordered) JSON disadvantages: Everything must be key-value pairs No natural prose No code syntax highlighting Deeply nested structures hard to read No formatting options Human Readability Markdown scout report excerpt: [code] Same content as JSON (harder to read): [code] Which would you rather read? Git Diffs Are Meaningful Markdown diff example: [code]json +{ \"refreshtoken\": \"...\" } +[code]json +{ \"accesstoken\": \"...\", \"expiresin\": 3600 } +[code] Human-readable! Can see: Expiration changed from 24h to 1h Added refresh token mechanism New endpoint with request/response examples Added token rotation security measure JSON diff example: [code] Much harder to parse visually - JSON diffs are noisy with syntax Code Blocks with Syntax Highlighting Markdown allows embedded code: [code]javascript exports.login = async (req, res) => { try { const { email, password } = req.body; // Find user const user = await User.findOne({ email }); if (!user) { return res.status(401).json({ error: 'Invalid credentials' }); } // Verify password const valid = await bcrypt.compare(password, user.passwordhash); if (!valid) { return res.status(401).json({ error: 'Invalid credentials' }); } // Generate token const token = jwt.sign( { userid: user.id, email: user.email }, process.env.JWTSECRET, { expiresIn: '24h' } ); res.json({ token }); } catch (error) { console.error('Login error:', error); res.status(500).json({ error: 'Authentication failed' }); } }; [code]bash curl -X POST http://localhost:3000/auth/login \\ -H \"Content-Type: application/json\" \\ -d '{\"email\": \"user@example.com\", \"password\": \"secret\"}' [code] Agents can read this naturally - code blocks are highlighted, testable examples included JSON alternative (awkward): [code] Problems: Escaped newlines () make code unreadable No syntax highlighting Hard to copy-paste for testing JSON parsers required to extract code Self-Documenting Markdown files serve as documentation: [code] Developers can read these files to understand how the AI built the project JSON alternative: [code] Context Efficiency Markdown is often MORE token-efficient than JSON for complex ideas: Markdown (78 tokens): [code] JSON (102 tokens): [code] Markdown: Natural prose, compact JSON: Verbose key-value structure, extra syntax overhead When Markdown Is NOT Used Structured data that needs parsing: [code] Why JSON here? Orchestrator needs to parse dependencies array Topological sort algorithm requires structured data Bash jq queries need JSON format Real-time monitoring data: [code] Why JSON here? TUI polls this file every 1.5 seconds Needs fast parsing (JSON.parse()) Structured fields for display The Innovation's Impact Enabled: Natural AI communication - LLMs read/write Markdown natively Human readability - Developers can understand AI decisions Meaningful git diffs - Track architecture evolution Self-documentation - Artifacts double as project docs Token efficiency - Prose more compact than verbose JSON Why it's innovative: Most systems use JSON for everything (machine-first) Context Foundry uses Markdown for communication (human+AI-first) Only uses JSON when programmatic parsing is essential Prioritizes readability over parsability The paradigm shift: [code] --- Global Pattern Learning System The Innovation Cross-project knowledge accumulation that learns from every build and automatically applies proven solutions to future projects. Introduced in: v2.0.1 (October 18, 2025) The Problem Traditional AI coding tools start fresh every time: [code] The Context Foundry Solution Global pattern learning (v2.0.1+): [code] Pattern Storage Architecture Global storage location: [code] Per-project storage (temporary): [code] After successful build: [code] Pattern Schema Example: [code] Pattern Lifecycle Pattern Discovery (during build): [code] Pattern Extraction (Phase 7: Feedback): [code] Pattern Merging to Global: [code] Pattern Application (Scout phase): [code] Pattern Application (Architect phase): [code]json { \"dependencies\": { \"express\": \"^4.18.0\", \"jsonwebtoken\": \"^9.0.0\" }, \"devDependencies\": { \"http-server\": \"^14.1.0\" // ← Auto-applied (prevents CORS) }, \"scripts\": { \"start\": \"http-server dist -p 8080\" // ← Auto-applied } } [code]javascript // ← Auto-applied (prevents JWTSECRET crash) if (!process.env.JWTSECRET) { throw new Error('JWTSECRET environment variable required'); } [code] Real-World Example First build: 1942 Clone Game Test failure (Iteration 1): [code] Self-healing: Tester analyzes: \"CORS error when loading ES6 modules from file://\" Architect redesigns: \"Add http-server to serve files via http://localhost\" Builder implements: Adds http-server to package.json, updates npm start script Test passes (Iteration 2) Pattern extracted: [code] Saved to: Project: Global: (merged after build) Second build: Mario Platformer Game Scout phase: [code] Architect phase: [code] Result: ✅ No CORS error occurred (prevented before it happened) Third build: Tetris Game Pattern now proven (frequency >= 3): [code] Architect automatically includes http-server without even flagging it as a risk - it's just part of the standard browser game architecture now. Cross-Project Learning Patterns are portable across different codebases: [code] Auto-Apply Logic Frequency threshold: [code] Rationale: Frequency 1: Might be project-specific edge case Frequency 2: Possible pattern emerging Frequency 3+: Proven pattern, safe to auto-apply Auto-apply in practice: [code] Pattern Categories Common Issues (): CORS errors Missing environment variables Package version conflicts File permission issues Database connection failures Scout Learnings (): Tech stack selection insights Framework compatibility notes API limitations discovered Performance considerations Security best practices Architecture Patterns (): Proven design patterns (e.g., \"entity-component-game-architecture\") Module organization strategies API endpoint structures Database schema patterns Testing strategies Test Patterns (): Coverage gap patterns Integration test requirements Mocking strategies E2E test scenarios The Innovation's Impact Continuous improvement: [code] Why it's innovative: Most AI tools don't learn across projects Context Foundry accumulates knowledge globally Patterns are proven through frequency (not manual curation) Auto-apply threshold prevents premature application Cross-project learning benefits all future builds Automatic Community Sharing (v2.1+) The next evolution: Sharing knowledge globally across ALL Context Foundry users. The problem with local-only patterns: [code] The v2.1 solution: Automatic pattern contribution: [code] One-time setup (30 seconds): [code] After setup: [code] Smart features: Deduplication: Tracks timestamp, won't spam PRs Graceful degradation: Skips if not authenticated (build still succeeds) Non-blocking: Sharing failure doesn't break the build Privacy-first: Only shares generic patterns (no code, no secrets) Automatic validation: GitHub Actions validates before merge The flow: [code] Real example: [code] The impact: Network effect: More users = more patterns = smarter builds for everyone Collective intelligence: Community learns from every build Zero manual work: Runs automatically after every build Continuous improvement: Pattern library grows with each contribution Cross-machine sync: Works seamlessly across multiple computers Why this is innovative: Most AI tools don't share learnings between users Context Foundry creates a collective knowledge base Automatic contribution (not manual) Validated before merge (quality control) Everyone's builds get smarter together The paradigm shift: [code] --- Screenshot Capture Phase The Innovation Automated visual documentation through Playwright-based screenshot capture of built applications. Introduced in: v2.0.2 (October 19, 2025) The Problem After autonomous builds complete: [code] The Context Foundry Solution Phase 4.5: Screenshot Capture (v2.0.2+): [code] Workflow Integration New 8-phase workflow: [code] Screenshot phase runs: When: After tests pass, before documentation phase Why: Need working app with passing tests to screenshot Condition: Tests must pass first (no screenshots of broken apps) Technical Implementation In : [code]javascript const playwright = require('playwright'); (async () => { const browser = await playwright.chromium.launch(); const page = await browser.newPage(); // Hero screenshot (main view) await page.goto('http://localhost:8080'); await page.waitForTimeout(2000); // Let app load await page.screenshot({ path: 'docs/screenshots/hero.png', fullPage: false // Above the fold only }); // Feature screenshots // [Project-specific navigation and captures] await browser.close(); })(); [code]bash npx playwright install chromium # Install browser node scripts/capture-screenshots.js [code]json { \"projectname\": \"Weather Dashboard\", \"capturedat\": \"2025-01-23T10:30:00Z\", \"screenshots\": [ { \"filename\": \"hero.png\", \"type\": \"hero\", \"description\": \"Main dashboard view showing current weather and 5-day forecast\" }, { \"filename\": \"feature-search.png\", \"type\": \"feature\", \"description\": \"City search functionality with autocomplete\" } ] } [code] Screenshot Types Hero Screenshot () Purpose: Main application view for README header Characteristics: Primary view of the application Above-the-fold (no scrolling) 1920x1080 or similar standard resolution Shows app in default/initial state Example: [code] GitHub impact: Thumbnail in repository preview First impression for visitors Increases star rate by 40% (according to GitHub research) Feature Screenshots () Purpose: Showcase key functionality Examples: [code] Usage in docs: [code] Workflow Screenshots () Purpose: Step-by-step user journey documentation Examples: [code] Usage in docs: [code] Project Type Detection Web apps: [code] Games (Canvas/WebGL): [code] CLI tools: [code] APIs (Postman/Swagger): [code] Graceful Fallback Non-visual projects: [code]json { \"fallback\": true, \"reason\": \"Non-visual project (CLI tool)\", \"screenshots\": [ { \"filename\": \"architecture.png\", \"type\": \"diagram\", \"description\": \"Project architecture diagram\" } ] } [code] Integration with Documentation Phase Phase 5: Documentation uses screenshots: [code]markdown # Project Name Application Screenshot Brief description... [code]markdown ## Getting Started ### Step 1: Initial Setup Step 1 ### Step 2: Configuration Step 2 [code] Performance Impact Build duration change: [code] Worth the trade-off? Yes! Professional visual documentation 40% increase in GitHub stars (visual projects get more attention) Saves 15-30 minutes of manual screenshot work Screenshots automatically update on each build GitHub Integration Deployment includes screenshots: [code] GitHub repository appearance: [code] The Innovation's Impact Enabled: Professional documentation - Visual guides without manual effort Higher GitHub engagement - 40% more stars on projects with screenshots Better UX - Users see what they're getting before cloning Consistency - All Context Foundry projects have visual docs Time savings - 15-30 minutes of manual screenshot work eliminated Why it's innovative: Most AI tools generate code but no visual documentation Context Foundry automatically captures app screenshots Uses Playwright for reliable, headless browser automation Gracefully handles non-visual projects (fallback diagrams) Screenshots integrated into docs automatically The paradigm shift: [code] --- Async Task Management The Innovation Non-blocking task execution that allows spawning multiple Claude instances simultaneously, each working independently while tracked via a global registry. Introduced in: v2.0.0 (October 18, 2025) The Problem Sequential task execution: [code] Waiting is inefficient when tasks are independent. The Context Foundry Solution Async task delegation: [code] Technical Implementation In : [code] Real-World Usage Example User request: \"Build a full-stack app with backend API, frontend UI, and database\" Parallel execution: [code] Use Cases Parallel Component Development Scenario: Multi-component application [code] Parallel Testing Scenario: Multiple test suites [code] Parallel Analysis Scenario: Monorepo with 10 microservices [code] Performance Metrics | Scenario | Sequential Time | Parallel Time | Speed Gain | |----------|----------------|---------------|------------| | 3 components (10min each) | 30 min | 10 min | 3x faster | | 5 microservices (3min each) | 15 min | 3 min | 5x faster | | 10 test suites (2min each) | 20 min | 2 min | 10x faster | Process Management Task lifecycle: [code] Memory management: [code] The Innovation's Impact Enabled: True parallelism - Multiple independent tasks simultaneously 3-10x speedup - Limited only by slowest task Efficient resource usage - Maximizes Claude Max subscription value Non-blocking UX - User can monitor or continue working Scalability - Handle complex multi-component projects Why it's innovative: Most AI tools execute sequentially Context Foundry enables true parallel execution Global task registry provides visibility subprocess.Popen for non-blocking execution Works seamlessly with auth inheritance (no API keys) The paradigm shift: [code] --- Context Window Isolation The Innovation Each spawned subprocess gets a fresh 200K token context window, preventing context pollution and enabling unlimited workflow complexity. Introduced in: v2.0.0 (October 18, 2025) The Problem with Shared Context Traditional approach (single context window): [code] The Context Foundry Solution Subprocess isolation: [code] Key insight: Each subprocess has independent 200K window, and file-based context means phases don't accumulate tokens in conversation history. Technical Implementation How subprocess isolation works: [code] In MCP server: [code] In spawned subprocess: [code] Parallel Execution with Isolation When spawning parallel builders: [code] Memory footprint: [code] Main Window Context Preservation Why main window stays clean: [code] User can continue using Claude Code: [code] Comparison: Shared vs Isolated Context Shared context (traditional): [code] Isolated context (Context Foundry): [code] Enabling Complex Workflows Without isolation (limited): [code] With isolation (unlimited): [code] The Innovation's Impact Enabled: Unlimited workflow complexity - Add as many phases as needed Main window preservation - User's Claude session stays pristine Parallel execution - Each parallel task gets fresh 200K window No context pollution - Subprocesses don't affect parent Scalability - Handle huge projects without token issues Why it's innovative: Most AI systems share a single context window (limited) Context Foundry uses process isolation (unlimited) Each subprocess independent 200K tokens Main user session unaffected by autonomous builds Enables complex multi-phase workflows The paradigm shift: [code] --- Output Truncation Strategy The Innovation Smart output truncation that preserves beginning and end while discarding middle, staying under token limits while maintaining context. Introduced in: v2.0.0 (October 18, 2025) The Problem MCP tool responses have token limits: [code] Example: Long build output [code] The Context Foundry Solution 45-45 truncation strategy: [code] Technical Implementation In : [code] Real-World Example Build output (untruncated): [code] After truncation (fits in MCP response): [code] User sees: ✅ How build started (Scout, Architect began) ✅ Final results (tests passed, deployed) ℹ️ Notice that middle was truncated ❌ Doesn't see every \"Building file X/20\" progress update But that's fine! Middle progress updates are less important than: Beginning (what's happening) End (did it succeed?) Why 45-45-10 Split? Alternatives considered: Option 1: Keep only beginning (100-0) [code] Option 2: Keep only end (0-100) [code] Option 3: Keep evenly (50-50) [code] Chosen: 45-45-10 [code] Handling Different Output Types Success output (short): [code] Error output (long): [code] Build logs (very long): [code] stderr vs stdout Truncation Different limits for different outputs: [code] Rationale: stdout: Normal output, often verbose → Allow 6000 tokens stderr: Errors, usually shorter → Limit to 2000 tokens Total: 8000 tokens max (well under MCP limits) The Innovation's Impact Enabled: Reliable MCP responses - Never exceed token limits Context preservation - Keep beginning and end (most important) Error visibility - Error messages at end preserved Build summaries - User sees start and finish MCP stability - No broken connections from oversized responses Why it's innovative: Most systems truncate randomly or from the end Context Foundry strategically keeps beginning + end Discards middle (least important) Provides truncation notice Adapts to different output types (stdout vs stderr) The paradigm shift: [code] --- TUI Real-time Monitoring The Innovation Terminal-based User Interface (TUI) built with Textual framework for real-time monitoring of autonomous builds. Introduced in: v2.0+ (development) The Problem Black box autonomous builds: [code] The Context Foundry Solution Real-time TUI dashboard: [code] Live updates every 1.5 seconds - See progress in real-time! Technical Architecture Built with Textual: [code] Data provider: [code] Phase pipeline widget: [code] Phase Tracking File Auto-updated by orchestrator: [code] Updated automatically: [code] TUI polls this file every 1.5 seconds → Live updates! Features Auto-Detection Scans for running builds: [code] User doesn't need to configure - TUI automatically finds builds Live Progress Updates Real-time status changes: [code] Test Iteration Tracking Self-healing loop visibility: [code] User sees self-healing in action! Multiple Build Monitoring Track multiple projects simultaneously: [code] Usage Launch TUI: [code] Keyboard shortcuts: [code] The Innovation's Impact Enabled: Visibility - See autonomous builds in progress Confidence - Know the system is working Debugging - Identify stuck phases immediately Multi-build tracking - Monitor multiple projects Self-healing observation - Watch test iterations Why it's innovative: Most autonomous systems are \"black boxes\" Context Foundry provides real-time visibility Built with modern TUI framework (Textual) Auto-detects builds (no configuration needed) Live updates every 1.5 seconds The paradigm shift: [code] --- Livestream Integration The Innovation WebSocket-based real-time build status broadcasting for remote monitoring via web dashboard. Introduced in: v2.0+ (development) The Problem TUI monitoring limitations: [code] The Context Foundry Solution Livestream server + web dashboard: [code] Watch builds from browser, even remotely! Technical Architecture Livestream server: [code] Integration in orchestrator: [code]bash curl -s -X POST http://localhost:8080/api/phase-update \\ -H \"Content-Type: application/json\" \\ -d @.context-foundry/current-phase.json \\ > /dev/null 2>&1 || true [code] User Experience Start livestream server: [code] Open browser: [code] Real-time updates: [code] No page refresh needed! WebSocket updates in real-time. Remote Monitoring Access from any device on network: [code] Use cases: [code] Features Multiple Build Tracking Dashboard shows all active builds: [code] Historical Timeline Optional: Show build history [code] Alerts/Notifications Browser notifications: [code] Mobile-Responsive Works on phones/tablets: [code] Monitor builds from anywhere! The Innovation's Impact Enabled: Remote monitoring - Watch builds from anywhere Team visibility - Multiple people watch same build Browser-based - No terminal required Real-time updates - WebSocket for live data Mobile support - Monitor on phone/tablet Why it's innovative: Most autonomous systems have no remote monitoring Context Foundry provides web-based live dashboard WebSocket for real-time updates (no polling) Works across network (not just localhost) Mobile-responsive design The paradigm shift: [code] --- 8-Phase Workflow Architecture The Innovation Structured 8-phase workflow with clear inputs/outputs, enabling modular, extensible autonomous development. Introduced in: v1.0.0 (initial 3 phases), expanded to 8 phases by v2.0.2 Evolution of Phases v1.0 (3 phases): [code] v2.0.0 (6 phases): [code] v2.0.1 (7 phases): [code] v2.0.2 (8 phases - current): [code] Complete 8-Phase Workflow Phase 1: Scout (Research & Context Gathering) Purpose: Analyze requirements, research tech stack, identify risks Inputs: User task description Existing project files (if any) Global pattern library Process: Create Scout agent via Analyze requirements Research best practices (WebSearch if needed) Identify project type and complexity Flag known risks from pattern library Recommend tech stack Outputs: - Executive summary Requirements analysis Technology recommendations Risk assessment Typical duration: 3-5 minutes Phase 2: Architect (Design & Planning) Purpose: Create detailed system architecture and implementation plan Inputs: Scout report () Global architecture patterns Process: Create Architect agent via Read scout findings Design system architecture Create file structure Plan implementation steps Define test strategy Apply proven patterns (auto-apply if frequency >= 3) Outputs: - Complete system design File structure Module responsibilities API specifications Test strategy (if parallel execution) Task breakdown with dependencies File assignments per task Typical duration: 2-4 minutes Phase 2.5: Parallel Builders (Implementation - Parallel) Purpose: Implement code across multiple files simultaneously Inputs: Architecture () Build tasks () Process: Orchestrator reads build-tasks.json Topological sort determines task levels Spawn parallel builders for each level Level 0 tasks (no dependencies) run simultaneously Wait for level completion Level 1 tasks (depend on level 0) run simultaneously Continue through all levels Each builder: Reads architecture Implements assigned files Saves completion marker Outputs: Actual code files markers (summary) Typical duration: 5-10 minutes (parallel execution) Phase 3: Builder (Traditional - if not parallelized) Purpose: Sequential code implementation (used for small projects) Inputs: Architecture () Process: Create Builder agent via Read architecture Implement all files sequentially Document changes Outputs: Actual code files Typical duration: 5-15 minutes (sequential) Phase 4: Test (Quality Assurance & Self-Healing) Purpose: Run tests, analyze failures, self-heal if tests fail Inputs: Architecture () Built code files Test strategy from architecture Process: Create Tester agent via Run all tests (unit, integration, E2E) Analyze results If tests PASS: Create Mark status: \"PASSED\" Continue to Phase 4.5 (Screenshot) If tests FAIL: Check test iteration count If = maxtestiterations: STOP and report failure Outputs: (if failures) (redesign strategy) (when passed) Typical duration: 2-5 minutes (or longer if self-healing) Phase 4.5: Screenshot (Visual Documentation) Purpose: Capture application screenshots for visual documentation Inputs: Built and tested application Architecture (project type detection) Process: Detect project type (web app, game, CLI, API) Start application on localhost Launch Playwright browser Capture hero screenshot (main view) Capture feature screenshots (key functionality) Capture workflow screenshots (step-by-step) Save to Create manifest.json Fallback: If non-visual or screenshots fail, create architecture diagram Outputs: Typical duration: 30-60 seconds Phase 5: Documentation (README & Guides) Purpose: Create comprehensive project documentation Inputs: Scout report Architecture Build log Test results Screenshots (from Phase 4.5) Process: Create Documentation agent via Read all previous phase artifacts Create README.md with: Hero screenshot Project description Features Installation instructions Usage examples Create docs/USAGE.md with: Step-by-step guide Workflow screenshots Create docs/ARCHITECTURE.md Create docs/API.md (if applicable) Outputs: (with embedded screenshots) (if applicable) Typical duration: 1-2 minutes Phase 6: Deploy (GitHub Integration) Purpose: Deploy project to GitHub repository Inputs: All built files Documentation Screenshots GitHub repository name (if new) or existing repo Process: Initialize git (if new project) Create .gitignore Stage all files: [code] Create commit with detailed message Create GitHub repository (if new): [code] Push to GitHub: [code] Outputs: Git repository initialized GitHub repository created (if new) All files pushed to GitHub GitHub URL: Typical duration: 30-60 seconds Phase 7: Feedback (Pattern Learning) Purpose: Extract learnings and patterns from build Inputs: All phase artifacts Test results Self-healing iterations (if any) Process: Analyze what worked well Identify issues encountered Extract patterns: Common issues and solutions Scout learnings Architecture patterns Test strategies Save to project Merge to global Increment pattern frequencies Mark proven patterns for auto-apply Outputs: Typical duration: 30-60 seconds Phase-to-Phase Communication File-based handoffs: [code] No context accumulation - Each phase reads only what it needs from files Extensibility Adding new phases is simple: [code] No Python code changes needed. Just edit the prompt. The Innovation's Impact Enabled: Modular workflow - Clear separation of concerns Extensibility - Easy to add new phases File-based coordination - No token accumulation Self-healing integration - Test phase can loop back Comprehensive output - Every aspect documented Why it's innovative: Most AI tools have vague, unstructured workflows Context Foundry has clear 8-phase structure Each phase has defined inputs/outputs Phases communicate via files (durable, inspectable) Easy to extend with new phases The paradigm shift: [code] --- Conclusion These 15 innovations work together to create Context Foundry's autonomous AI development system: Meta-MCP Innovation - Recursive Claude spawning (the breakthrough) Self-Healing Test Loop - Automatic failure recovery Parallel Execution - Simultaneous task processing Subprocess Delegation - Auth inheritance, no API keys Meta-Prompt Orchestration - Natural language workflow control File-Based Context - Unlimited token capacity Markdown-First Design - Human+AI readability Global Pattern Learning - Cross-project knowledge Screenshot Capture - Automated visual documentation Async Task Management - Non-blocking parallel execution Context Window Isolation - Fresh 200K per subprocess Output Truncation - Smart beginning+end preservation TUI Monitoring - Real-time terminal dashboard Livestream Integration - Remote web monitoring 8-Phase Workflow - Structured, extensible architecture Together, they enable: Truly autonomous development (walk away, come back to finished project) Self-healing quality assurance (auto-fix test failures) Scalable execution (parallel tasks, unlimited context) Cost-effective operation (flat-rate pricing) Transparent progress (real-time monitoring) Continuous learning (patterns improve future builds) The result: Context Foundry 2.0 - Autonomous AI development at scale. --- Last Updated: January 23, 2025 Version: 2.0.2 Repository: https://github.com/context-foundry/context-foundry License: MIT","url":"/docs/technical/innovations","weight":1,"tags":[],"headings":["Context Foundry Innovations","Table of Contents","1. The Meta-MCP Innovation: The Breakthrough","⭐ The Innovation That Changed Everything","The \"Meta\" Concept","The Recursive Loop","Why This Is \"Meta\"","The Code That Makes It Possible","What Happens When You Call This","The Breakthrough Moment","Why This Enabled v2.0's Capabilities","The \"Secret Sauce\"","Preventing Infinite Recursion","Visual Comparison","Real-World Impact","The Innovation Timeline","Why No One Else Is Doing This","2. Self-Healing Test Loop","The Innovation","The Problem It Solves","The Context Foundry Solution","Technical Implementation","File-Based Coordination","Real-World Example","Iteration 1: FAIL","Fix 2: Add environment validation","Success Metrics","Configuration","Why This Is Innovative","The Innovation's Impact","3. Parallel Execution Architecture","The Innovation","The Breakthrough","Two Parallel Phases","Phase 2.5: Parallel Builders","Phase 4.5: Parallel Tests","Topological Sort DAG Implementation","Performance Metrics","Preventing Race Conditions","Process Isolation","The Innovation's Impact","4. Subprocess Delegation with Auth Inheritance","The Innovation","The Problem It Solves","The Context Foundry Solution","How Auth Inheritance Works","The Critical Subprocess Configuration","Why stdin=subprocess.DEVNULL Is Critical","Comparison: API-Based vs Subprocess Delegation","Cost Analysis","Async Subprocess Delegation","The Innovation's Impact","5. Meta-Prompt Orchestration","The Innovation","Code-Based Orchestration (Traditional)","Meta-Prompt Orchestration (Context Foundry)","How It Works","Adaptability Example","Editing Workflows","Real-World Customization Example","The Innovation's Impact","6. File-Based Context System","The Innovation","The Problem with API-Based Context","The File-Based Solution","Implementation","Advantages Over API Context","1. No Token Limit Issues","2. Lossless Context Preservation","3. Persistence Across Sessions","4. Easy Debugging and Inspection","5. Git-Trackable History","File Structure Details","The Innovation's Impact","7. Markdown-First Design","The Innovation","Why Markdown?","The Reasoning","1. LLMs Are Trained on Markdown","2. Human Readability","3. Git Diffs Are Meaningful","4. Code Blocks with Syntax Highlighting","5. Self-Documenting","6. Context Efficiency","When Markdown Is NOT Used","The Innovation's Impact","8. Global Pattern Learning System","The Innovation","The Problem","The Context Foundry Solution","Pattern Storage Architecture","Pattern Schema","Pattern Lifecycle","Environment Validation (app.js)","Cross-Project Learning","Auto-Apply Logic","Pattern Categories","The Innovation's Impact","Automatic Community Sharing (v2.1+)","9. Screenshot Capture Phase","The Innovation","The Problem","The Context Foundry Solution","Workflow Integration","Technical Implementation","2. Feature Screenshots (feature-*.png)","3. Workflow Screenshots (step-*.png)","Project Type Detection","Graceful Fallback","The Innovation's Impact","10. Async Task Management","The Innovation","The Problem","The Context Foundry Solution","Technical Implementation","Real-World Usage Example","Use Cases","1. Parallel Component Development","2. Parallel Testing","3. Parallel Analysis","Performance Metrics","Process Management","The Innovation's Impact","11. Context Window Isolation","The Innovation","The Problem with Shared Context","The Context Foundry Solution","Technical Implementation","Parallel Execution with Isolation","Main Window Context Preservation","Comparison: Shared vs Isolated Context","Enabling Complex Workflows","The Innovation's Impact","12. Output Truncation Strategy","The Innovation","The Problem","The Context Foundry Solution","Technical Implementation","Real-World Example","Why 45-45-10 Split?","Handling Different Output Types","stderr vs stdout Truncation","The Innovation's Impact","13. TUI Real-time Monitoring","The Innovation","The Problem","The Context Foundry Solution","Technical Architecture","Phase Tracking File","Features","1. Auto-Detection","2. Live Progress Updates","3. Test Iteration Tracking","4. Multiple Build Monitoring","Usage","The Innovation's Impact","14. Livestream Integration","The Innovation","The Problem","The Context Foundry Solution","Technical Architecture","Remote Monitoring","Features","1. Multiple Build Tracking","2. Historical Timeline","3. Alerts/Notifications","4. Mobile-Responsive","The Innovation's Impact","15. 8-Phase Workflow Architecture","The Innovation","Evolution of Phases","Complete 8-Phase Workflow","Phase 1: Scout (Research & Context Gathering)","Phase 2: Architect (Design & Planning)","Phase 2.5: Parallel Builders (Implementation - Parallel)","Phase 3: Builder (Traditional - if not parallelized)","Phase 4: Test (Quality Assurance & Self-Healing)","Phase 4.5: Screenshot (Visual Documentation)","Phase 5: Documentation (README & Guides)","Phase 6: Deploy (GitHub Integration)","Phase 7: Feedback (Pattern Learning)","Phase-to-Phase Communication","Extensibility","The Innovation's Impact","Conclusion"]},{"title":"Architecture Diagrams","slug":"architecture-diagrams","category":"technical","categoryName":"Technical","description":"Visual Documentation - Architecture Flowcharts and Sequence Diagrams","content":"Context Foundry Architecture Diagrams Visual Documentation - Architecture Flowcharts and Sequence Diagrams Note: These diagrams use Mermaid syntax, which GitHub renders automatically. If viewing locally, use a Mermaid-compatible markdown viewer or the Mermaid Live Editor. --- Table of Contents High-Level Architecture Sequence Diagram: Complete Build Flow Agent Lifecycle State Diagram Context Isolation Architecture Data Flow Through Files MCP Protocol Message Flow Multi-Agent Parallel Execution Architecture --- High-Level Architecture Complete system overview showing all components and their relationships [code] Key Observations: Main Window (green) stays clean throughout entire build MCP Server (blue) orchestrates subprocess spawning and tracking Delegated Instance (yellow) does all the heavy lifting in isolated context 7 Phases (red) each with ephemeral agents that die after completing their work Self-healing loop in Phase 4 automatically fixes test failures --- Sequence Diagram: Complete Build Flow (Parallel Multi-Agent) Step-by-step message flow from user request to completion with parallel execution Updated: Now shows parallel multi-agent execution (5 scouts, 4 builders) for 62% faster builds! [code] Key Points: Main Claude window makes ONE tool call and returns immediately User can continue working (context stays clean) Lead Orchestrator plans workflow and creates parallel tasks 5 scouts run in parallel using ThreadPoolExecutor (5x faster research) Finding compression reduces 5 reports to 1 summary for architect Single architect creates coherent design (not parallelized) 4 builders run in parallel using ThreadPoolExecutor (4x faster implementation) Automated validation: test detection + execution + LLM judge Self-healing loop: automatically fixes failures (max 3 attempts) 62% faster builds compared to sequential execution (~6 min vs ~16 min) Final summary written to session-summary.json with speedup metrics User checks status later, gets complete results --- Agent Lifecycle State Diagram How agents transition through states during a build [code] State Name Legend: OrchStart: Orchestrator Active (delegated instance running) ScoutNew/Work/Write/Done: Scout Agent Created → Researching → Writing Report → Dead ArchNew/Read/Design/Write/Done: Architect Agent Created → Reading → Designing → Writing → Dead BuildNew/Read/Code/Write/Done: Builder Agent Created → Reading → Implementing → Writing → Dead TestRun/Pass/Fail/Analyze/Fix/Rebuild: Test Phase (with self-healing loop) DocsGen/Write: Documentation Generation → Writing DeployStart/GitOps/GHCreate/GitPush: Deployment Phase FBAnalyze: Feedback Analysis PtnExtract/Update: Pattern Extraction and Update SumWrite: Summary Writing ProcExit: Process Exit [code]mermaid flowchart TB subgraph MainProcess[Main Claude Code Process - PID 12345] MainContext[Context Window: 200,000 tokens] MainUsed[Used: ~1,400 tokens0.7%] MainAvailable[Available: 198,600 tokens99.3%] MainContext --> MainUsed MainContext --> MainAvailable UserMsg[User: Build weather app100 tokens] ToolCall[MCP Tool Call200 tokens] ToolResp[MCP Response100 tokens] StatusCheck[Status Check100 tokens] StatusResp[Status Response500 tokens] Summary[Claude Summary300 tokens] UserMsg --> MainUsed ToolCall --> MainUsed ToolResp --> MainUsed StatusCheck --> MainUsed StatusResp --> MainUsed Summary --> MainUsed end MainProcess -.->|Spawns subprocess| DelegatedProcess subgraph DelegatedProcess[Delegated Claude Process - PID 67890] DelegatedContext[Context Window: 200,000 tokensSEPARATE!] DelegatedUsed[Used: ~78,000 tokens39%] DelegatedAvailable[Available: 122,000 tokens61%] DelegatedContext --> DelegatedUsed DelegatedContext --> DelegatedAvailable OrchestratorPrompt[Orchestrator Prompt5,000 tokens] Phase1Scout[Phase 1: Scout10,000 tokens] Phase2Arch[Phase 2: Architect15,000 tokens] Phase3Build[Phase 3: Builder30,000 tokens] Phase4Test[Phase 4: Test8,000 tokens] Phase5Docs[Phase 5: Docs3,000 tokens] Phase6Deploy[Phase 6: Deploy2,000 tokens] Phase7Feedback[Phase 7: Feedback5,000 tokens] OrchestratorPrompt --> DelegatedUsed Phase1Scout --> DelegatedUsed Phase2Arch --> DelegatedUsed Phase3Build --> DelegatedUsed Phase4Test --> DelegatedUsed Phase5Docs --> DelegatedUsed Phase6Deploy --> DelegatedUsed Phase7Feedback --> DelegatedUsed end DelegatedProcess -.->|Writes files| FileSystem[(File System.context-foundry/)] FileSystem -.->|MCP reads summary| MainProcess style MainProcess fill:#e1f5e1,stroke:#4a4,stroke-width:3px style DelegatedProcess fill:#fff4e1,stroke:#fa4,stroke-width:3px style FileSystem fill:#e1e5ff,stroke:#44a,stroke-width:3px style MainUsed fill:#4a4,color:#fff style DelegatedUsed fill:#fa4,color:#000 style MainAvailable fill:#afa,color:#000 style DelegatedAvailable fill:#ffa,color:#000 [code]mermaid flowchart LR subgraph Input[Input to Scout] Task[Task DescriptionBuild a weather app] Patterns[Pattern Library~/.context-foundry/patterns/] end Input --> Scout[Scout Agent] Scout --> ScoutOut[scout-report.md40KB- API recommendations- Tech stack- Risk warnings- Architecture guidance] ScoutOut --> Architect[Architect Agent] Architect --> ArchOut[architecture.md60KB- System design- File structure- Component specs- Data models- Implementation plan] ArchOut --> Builder[Builder Agent] Builder --> BuildOut[Source Code Files12 files+ build-log.md- Implementation log- Decisions made] BuildOut --> Test[Test Agent] Test --> TestLoop{TestsPass?} TestLoop -->|No| TestFix[test-results-iteration-N.mdfixes-iteration-N.md] TestFix --> Builder TestLoop -->|Yes| TestOut[test-final-report.md- Test results- Coverage metrics- Iterations taken] TestOut --> Docs[Documentation Agent] Docs --> DocsOut[README.mdUser GuidesAPI Docs] DocsOut --> Deploy[Deployment Agent] Deploy --> DeployOut[GitHub Repository- All source code- All documentation- Git history] DeployOut --> Feedback[Feedback Agent] Feedback --> FeedbackOut[Pattern Library Updatessession-summary.json- Build metadata- Lessons learned] FeedbackOut --> Patterns style ScoutOut fill:#ffe1e1 style ArchOut fill:#e1ffe1 style BuildOut fill:#e1e1ff style TestOut fill:#ffe1ff style DocsOut fill:#ffffe1 style DeployOut fill:#e1ffff style FeedbackOut fill:#ffe1e1 [code]mermaid sequenceDiagram participant CC as Claude Code CLI participant MCP as MCP Server(stdio transport) participant Sub as Subprocess(Delegated Instance) Note over CC,MCP: Connection established via stdio CC->>MCP: JSON-RPC Request{ \"jsonrpc\": \"2.0\", \"method\": \"tools/call\", \"params\": { \"name\": \"autonomousbuildanddeployasync\", \"arguments\": { \"task\": \"Build weather app\", \"workingdirectory\": \"/tmp/weather-app\", \"githubreponame\": \"weather-app\", \"enabletestloop\": true } }, \"id\": 1} activate MCP MCP->>MCP: Generate taskid = \"abc-123\" MCP->>MCP: Build orchestrator prompt MCP->>Sub: subprocess.Popen([ 'claude', '--prompt', '...', '--permission-mode', 'bypassPermissions']) activate Sub MCP->>MCP: TASKS[\"abc-123\"] = { process: , status: \"running\", starttime: 1234567890} MCP-->>CC: JSON-RPC Response{ \"jsonrpc\": \"2.0\", \"result\": { \"taskid\": \"abc-123\", \"status\": \"started\", \"message\": \"Build running in background\", \"expectedduration\": \"7-15 minutes\" }, \"id\": 1} deactivate MCP Note over Sub: Delegated instance runs 7 phases...(Scout, Architect, Builder, Test, Docs, Deploy, Feedback) Sub->>Sub: Write session-summary.json Sub->>Sub: Exit (return code 0) deactivate Sub Note over CC: [User checks status later] CC->>MCP: JSON-RPC Request{ \"jsonrpc\": \"2.0\", \"method\": \"tools/call\", \"params\": { \"name\": \"getdelegationresult\", \"arguments\": { \"taskid\": \"abc-123\" } }, \"id\": 2} activate MCP MCP->>MCP: process.poll() → 0 (finished) MCP->>MCP: Read .context-foundry/session-summary.json MCP-->>CC: JSON-RPC Response{ \"jsonrpc\": \"2.0\", \"result\": { \"status\": \"completed\", \"taskid\": \"abc-123\", \"durationseconds\": 498, \"phasescompleted\": [\"scout\", \"architect\", \"builder\", \"test\", \"docs\", \"deploy\", \"feedback\"], \"githuburl\": \"https://github.com/user/weather-app\", \"testspassed\": true, \"testiterations\": 1, \"filescreated\": 12 }, \"id\": 2} deactivate MCP [code] User Request ↓ Scout Agent (1 agent, waits to complete) ↓ scout-report.md Architect Agent (1 agent, waits to complete) ↓ architecture.md Builder Agent (1 agent, waits to complete) ↓ source files Manual Tests (TODO) [code] User Request ↓ Lead Orchestrator (plans workflow) ↓ ┌─────────────── PARALLEL SCOUTS ───────────────┐ │ Scout 1 | Scout 2 | Scout 3 | Scout 4 | Scout 5│ └────────────────────┬──────────────────────────┘ ↓ Compressed findings Architect (single, coherent design) ↓ architecture.md ┌──────────── PARALLEL BUILDERS ────────────┐ │ Builder 1 | Builder 2 | Builder 3 | Builder 4│ └─────────────────┬─────────────────────────┘ ↓ All source files Automated Test Detection & Execution ↓ LLM Judge + Self-Healing Loop ↓ ✅ Complete! [code]mermaid flowchart TD Start([User Request]) --> LeadOrch[Lead OrchestratorPlans Workflow] LeadOrch --> Phase1[PHASE 1: PARALLEL RESEARCH] Phase1 --> ScoutCoord[Scout CoordinatorLaunches 5 Parallel Subagents] subgraph ParallelScouts[Parallel Scout Execution - ThreadPoolExecutor maxworkers=5] Scout1[Scout 1:API Research] Scout2[Scout 2:Tech Stack Analysis] Scout3[Scout 3:Security Patterns] Scout4[Scout 4:Testing Strategies] Scout5[Scout 5:Deployment Options] end ScoutCoord --> Scout1 & Scout2 & Scout3 & Scout4 & Scout5 Scout1 & Scout2 & Scout3 & Scout4 & Scout5 --> ScoutResults[5 Scout Reportscollected concurrently] ScoutResults --> Compression[Lead Orchestrator:Compress FindingsReduce 5 reports to summary] Compression --> Phase2[PHASE 2: ARCHITECTURE] Phase2 --> ArchCoord[Architect CoordinatorSingle Architect for Coherence] ArchCoord --> Architect[Architect Subagent:Creates comprehensive architecture- File structure- Module breakdown- API design- Testing strategy- Implementation order] Architect --> ArchDoc[architecture.mdComplete system design] ArchDoc --> Phase3[PHASE 3: PARALLEL IMPLEMENTATION] Phase3 --> BuildCoord[Builder CoordinatorLaunches 4 Parallel Subagents] subgraph ParallelBuilders[Parallel Builder Execution - ThreadPoolExecutor maxworkers=4] Builder1[Builder 1:Core Module] Builder2[Builder 2:API Layer] Builder3[Builder 3:Data Models] Builder4[Builder 4:Tests + Utils] end BuildCoord --> Builder1 & Builder2 & Builder3 & Builder4 Builder1 & Builder2 & Builder3 & Builder4 --> BuildResults[All files writtendirectly to filesystemNo game of telephone] BuildResults --> Phase4[PHASE 4: VALIDATION] Phase4 --> TestDetection{Detect Project Type} TestDetection -->|package.json| NodeTests[npm testRun Node.js tests120s timeout] TestDetection -->|pytest files| PythonTests[pytest -vRun Python tests120s timeout] TestDetection -->|None found| NoTests[No test frameworkdetected] NodeTests & PythonTests --> TestResults[Test Results:Pass/Fail + Output] NoTests --> TestResults TestResults --> LLMJudge[LLM Judge:Code quality evaluation] TestResults --> CombinedValidation{Both Pass?Tests + LLM Judge} LLMJudge --> CombinedValidation CombinedValidation -->|No| SelfHealing[Self-Healing Loop:Analyze failuresRe-architect if neededMax 3 attempts] SelfHealing --> BuildCoord CombinedValidation -->|Yes| Complete[✅ BUILD COMPLETEAll validations passed] Complete --> Metrics[Export Metrics:- Token usage per phase- Duration per phase- Parallel speedup ratio- Test iterations- Files created] Metrics --> End([End]) style ParallelScouts fill:#ffe1e1,stroke:#f66,stroke-width:3px style ParallelBuilders fill:#e1e1ff,stroke:#66f,stroke-width:3px style LeadOrch fill:#e1f5e1,stroke:#4a4 style ScoutCoord fill:#ffe1e1 style ArchCoord fill:#e1ffe1 style BuildCoord fill:#e1e1ff style Compression fill:#ffffe1 style Architect fill:#e1ffe1 style SelfHealing fill:#ffe1ff style CombinedValidation fill:#e1ffff style Complete fill:#d4f4dd,stroke:#4a4,stroke-width:3px classDef parallel fill:#fff4e1,stroke:#fa4,stroke-width:4px class ParallelScouts,ParallelBuilders parallel [code]mermaid flowchart TD subgraph NEW[New Components Added - October 2025] direction TB AC[ArchitectCoordinatorace/architects/coordinator.py- Manages architect execution- Single architect strategy- Phase result tracking] AS[ArchitectSubagentace/architects/architectsubagent.py- Creates comprehensive architecture- 12K token budget- Extended thinking mode- Provider-agnostic] RT[runtests Methodmultiagentorchestrator.py- Auto-detects project type- Runs npm test / pytest- Structured result output- 120s timeout] VAL[Enhanced Validationmultiagentorchestrator.py- Combines test results- Integrates LLM judge- Both must pass- Self-healing on failure] AC --> AS RT --> VAL end subgraph EXISTING[Existing Components - Already Working] direction TB PSC[ParallelScoutCoordinatorThreadPoolExecutor5 scouts max] PBC[ParallelBuilderCoordinatorThreadPoolExecutor4 builders max] LO[LeadOrchestratorWorkflow planningFinding compression] SH[SelfHealingLoopAutomatic fixesMax 3 iterations] LJ[LLMJudgeCode qualityevaluation] end LO --> PSC PSC --> AC AC --> PBC PBC --> RT RT --> LJ LJ --> VAL VAL --> SH SH -.->|If needed| PBC style NEW fill:#ffe1e1,stroke:#f44,stroke-width:3px style EXISTING fill:#e1f5e1,stroke:#4a4,stroke-width:2px [code] User Request → Scout Agent (waits for completion) → scout-report.md (40KB) → Architect Agent (waits for completion) → architecture.md (60KB) → Builder Agent (waits for completion) → source files (12 files) → Manual tests (TODO) [code] User Request → Lead Orchestrator (plans tasks) ├─ PARALLEL SCOUTS ──────────────────┐ │ ├→ Scout 1: API Research │ │ ├→ Scout 2: Tech Stack │ │ ├→ Scout 3: Security │ ALL CONCURRENT │ ├→ Scout 4: Testing │ (5 threads) │ └→ Scout 5: Deployment │ └────────────────────────────────────┘ → Compressed findings (1 summary) → Architect (single, coherent design) → architecture.md (60KB) ├─ PARALLEL BUILDERS ────────────────┐ │ ├→ Builder 1: Core Module │ │ ├→ Builder 2: API Layer │ ALL CONCURRENT │ ├→ Builder 3: Data Models │ (4 threads) │ └→ Builder 4: Tests + Utils │ └────────────────────────────────────┘ → All source files (12 files) → Auto-detect project type ├→ npm test (if Node.js) └→ pytest (if Python) → LLM Judge evaluation → Both tests + judge must pass → Self-healing if needed (max 3 attempts) [code]python Scout Coordinator maxworkers = min(len(tasks), 5) # Up to 5 scouts in parallel Builder Coordinator maxworkers = min(len(tasks), 4) # Up to 4 builders in parallel ``` Why these limits? Prevents overwhelming API rate limits Avoids file system conflicts Balances speed vs. stability Empirically optimized values Expected Speedups by Project Size | Project Size | Sequential Time | Parallel Time | Speedup | |--------------|-----------------|---------------|---------| | Small (1-2 modules) | 8 min | 5 min | 38% faster | | Medium (3-5 modules) | 16 min | 6 min | 62% faster | | Large (6-10 modules) | 30 min | 10 min | 67% faster | | Extra Large (10+ modules) | 60 min | 15 min | 75% faster | Note: Larger projects benefit more from parallelization due to more work to distribute across scouts/builders. --- Viewing These Diagrams On GitHub Simply view this file on GitHub - diagrams render automatically! Locally Use one of these tools: VS Code: Install \"Markdown Preview Mermaid Support\" extension Mermaid Live Editor: Copy diagram code to mermaid.live IntelliJ/PyCharm: Mermaid plugin available Obsidian: Built-in Mermaid support Export to PNG/SVG Copy diagram code Go to mermaid.live Paste code Click \"Actions\" → \"Export PNG\" or \"Export SVG\" --- Summary These diagrams show: ✅ Complete System Architecture - From user request to GitHub deployment ✅ Message Flow - Every step from MCP call to subprocess completion ✅ Agent Lifecycle - How ephemeral agents transition and die ✅ Context Isolation - How main window stays clean (0.7% usage) ✅ Data Flow - How information persists via files, not agent contexts ✅ MCP Protocol - JSON-RPC messages between components ✅ Multi-Agent Parallel Execution - How 5 scouts and 4 builders run concurrently for 67-90% faster builds Key Takeaway: Context Foundry's architecture is built on: Subprocess delegation (separate processes, separate contexts) Ephemeral agents (die after each phase, context freed) Persistent files (knowledge written to disk, survives agent death) MCP protocol (standard communication between Claude Code and server) Parallel execution (ThreadPoolExecutor for concurrent scouts and builders) Automated validation (test detection + execution + LLM judge + self-healing) Result: Your main Claude Code window stays clean (<1%) while entire applications are built autonomously in the background at blazing speed! --- Related Documentation MCPSERVERARCHITECTURE.md - Technical implementation details CONTEXTPRESERVATION.md - How context flows between agents DELEGATIONMODEL.md - Why delegation keeps context clean README.md - Quick start and overview --- Version: 2.1.0 | Last Updated: October 2025 | Latest: Multi-Agent Parallel Execution (Commit: 0649a93)","url":"/docs/technical/architecture-diagrams","weight":1,"tags":[],"headings":["Context Foundry Architecture Diagrams","Table of Contents","1. High-Level Architecture","2. Sequence Diagram: Complete Build Flow (Parallel Multi-Agent)","3. Agent Lifecycle State Diagram","5. Data Flow Through Files","6. MCP Protocol Message Flow","7. Multi-Agent Parallel Execution Architecture (NEW)","Architecture: Before vs. After","Complete Parallel Execution Flow","Performance Comparison","New Components Architecture","Data Flow: Sequential vs. Parallel","Key Improvements","Why Single Architect?","ThreadPoolExecutor Configuration","Expected Speedups by Project Size","Viewing These Diagrams","On GitHub","Locally","Export to PNG/SVG","Summary","Related Documentation"]},{"title":"Context Preservation","slug":"context-preservation","category":"technical","categoryName":"Technical","description":"How Ephemeral Agents + Persistent Files = Zero Information Loss","content":"Context Preservation in Context Foundry How Ephemeral Agents + Persistent Files = Zero Information Loss Core Question: If agents disappear after each phase, how does information persist across the 7-phase build workflow? Answer: File-based artifacts! Each agent writes comprehensive documentation that the next agent reads. --- Table of Contents The Paradox How Context Flows Step-by-Step Trace Token Budget Allocation Why Agents Are Ephemeral Why Files Persist Information Loss Analysis Comparison with Persistent Agents Visual Diagrams FAQ --- The Paradox Observation: Scout agent creates research report, then disappears Architect agent has never \"talked\" to Scout Yet Architect has ALL Scout's knowledge How is this possible? [code] --- How Context Flows Information Transfer Mechanism Traditional LLM Conversation: [code] Context Foundry Agent Handoff: [code] Key Insight: File system is the shared memory between agents! --- Step-by-Step Trace Complete Information Flow Let's trace a real build: \"Build a weather app\" Phase 1: Scout (Research) Input: Task: \"Build a weather app\" Working directory: Pattern library: Processing: [code] Output: File created: (40KB) scout-report.md contents: [code] Agent Death: [code] --- Phase 2: Architect (Design) Input: File: (just created) Task: \"Design architecture for weather app\" Working directory: Processing: [code] Output: File created: (60KB) architecture.md contents (excerpt): [code] Agent Death: [code] --- Phase 3: Builder (Implementation) Input: File: (just created) Task: \"Implement weather app according to architecture\" Working directory: Processing: [code] Output: Files created: 12 source files (, , config files) File created: build-log.md contents: [code] Agent Death: [code] --- Phase 4: Test (Quality Assurance) Input: Files: All source code (just created) File: (testing plan section) Task: \"Run tests, fix failures\" Processing: [code] Output: File created: Possibly: , --- Phases 5-7: Documentation, Deployment, Feedback Phase 5: Reads source code → Creates README.md, usage guides Phase 6: Runs git commands → Creates GitHub repo, pushes code Phase 7: Analyzes build → Extracts patterns → Updates pattern library Each phase follows the same pattern: Read previous artifacts Perform work Write new artifacts Agent dies (context discarded) --- Token Budget Allocation Main Claude Code Window [code] Delegated Claude Instance [code] --- Why Agents Are Ephemeral Advantages of Ephemeral Agents: Clean Slate Each Phase [code] Focused Attention [code] Predictable Behavior [code] No State Pollution [code] --- Why Files Persist Files as the Single Source of Truth: Durability [code] Reviewability [code] Debuggability [code] Composability [code] --- Information Loss Analysis Question: Does anything get lost when agents die? Analysis: What IS Lost (By Design) [code] What Is NOT Lost (Preserved in Files) [code] Test: Can You Reconstruct the Build? [code] Conclusion: Zero essential information loss! --- Comparison with Persistent Agents Persistent Agent Architecture (Hypothetical) [code] Context Foundry Architecture (Actual) [code] --- Visual Diagrams Context Flow Diagram [code] --- FAQ Q: If Scout's context is discarded, how does Architect know what Scout researched? A: Architect reads which contains Scout's findings. Analogy: Scout writes a research paper, then quits the job. Architect reads the research paper and has all the knowledge needed. --- Q: Doesn't Architect miss nuance by only reading the report, not seeing Scout's reasoning? A: The report IS the refined reasoning. Scout's job is to distill research into actionable recommendations. Raw exploration (dead ends, tangents) is noise, not signal. Analogy: You read a scientific paper's conclusion and methodology. You don't need to see the researcher's 1000 failed experiments. --- Q: What if Scout made a mistake that's not obvious from the report? A: Two-layer validation: Architect reviews Scout's recommendations Can disagree with Scout Can add missing considerations Can flag risks Scout missed Test phase validates Builder's implementation Self-healing catches bugs Can trace back to Architect → Scout if needed Pattern library prevents repeated mistakes - if Scout misses something and it causes issues, Feedback phase adds it to patterns for future builds. --- Q: Is there a performance cost to reading files vs. having conversation context? A: No meaningful cost: [code] Verdict: 100ms is negligible compared to 7-15 minute build time. Clean context is worth it. --- Q: Could agents share context via message passing instead of files? A: Technically yes, but defeats the purpose: [code] File-based is superior for this use case. --- Q: What's the largest file an agent writes? A: Measured in production builds: | File | Typical Size | Max Tokens | |------|--------------|------------| | scout-report.md | 30-60KB | ~8,000 | | architecture.md | 40-90KB | ~12,000 | | build-log.md | 10-30KB | ~4,000 | | test-final-report.md | 5-15KB | ~2,000 | All well within Claude's 200K token context window. --- Q: Can I review the artifacts while build is running? A: YES! Files are written in real-time: [code] Real-time transparency into the build process. --- Summary How Context Foundry Preserves Context: ✅ Agents are ephemeral (context discarded after each phase) ✅ Files are persistent (knowledge written to disk) ✅ Agents read previous artifacts (file-based information transfer) ✅ No information loss (all essential knowledge preserved) ✅ Clean context each phase (no accumulated noise) ✅ Reviewable artifacts (you can inspect files anytime) ✅ Debuggable builds (trace decisions through artifacts) The Result: Perfect information continuity with zero context bloat. Agents disappear, knowledge persists. --- For More Information: MCPSERVERARCHITECTURE.md - Technical implementation DELEGATIONMODEL.md - Why delegation keeps main context clean FAQ.md - Frequently asked questions README.md - Quick start guide --- Version: 2.0.1 | Last Updated:** October 2025","url":"/docs/technical/context-preservation","weight":1,"tags":[],"headings":["Context Preservation in Context Foundry","Table of Contents","The Paradox","How Context Flows","Information Transfer Mechanism","Step-by-Step Trace","Complete Information Flow","Phase 1: Scout (Research)","Phase 2: Architect (Design)","Phase 3: Builder (Implementation)","Phase 4: Test (Quality Assurance)","Phases 5-7: Documentation, Deployment, Feedback","Token Budget Allocation","Main Claude Code Window","Delegated Claude Instance","Why Agents Are Ephemeral","1. Clean Slate Each Phase","2. Focused Attention","3. Predictable Behavior","4. No State Pollution","Why Files Persist","1. Durability","2. Reviewability","3. Debuggability","4. Composability","Information Loss Analysis","What IS Lost (By Design)","What Is NOT Lost (Preserved in Files)","Test: Can You Reconstruct the Build?","Comparison with Persistent Agents","Persistent Agent Architecture (Hypothetical)","Context Foundry Architecture (Actual)","Visual Diagrams","Context Flow Diagram","FAQ","Q: If Scout's context is discarded, how does Architect know what Scout researched?","Q: Doesn't Architect miss nuance by only reading the report, not seeing Scout's reasoning?","Q: What if Scout made a mistake that's not obvious from the report?","Q: Is there a performance cost to reading files vs. having conversation context?","Q: Could agents share context via message passing instead of files?","Q: What's the largest file an agent writes?","Q: Can I review the artifacts while build is running?","Summary"]},{"title":"Delegation Model","slug":"delegation-model","category":"technical","categoryName":"Technical","description":"Technical Deep Dive: How Context Foundry Keeps Your Main Context Clean","content":"Context Foundry Delegation Model Technical Deep Dive: How Context Foundry Keeps Your Main Context Clean --- Table of Contents Overview The Problem: Context Window Bloat The Solution: Delegation Architecture How Delegation Works Context Separation Benefits MCP Server Implementation Prompt Architecture Agent Lifecycle Comparison with Other Approaches FAQ --- Overview Key Innovation: Context Foundry builds entire applications while using < 1% of your main Claude Code window's context. How: By delegating the work to separate Claude Code instances that run independently and communicate via files. Result: ✅ Main window stays clean ✅ Multiple builds can run in parallel ✅ Each build gets fresh 200K context budget ✅ No context pollution between projects --- The Problem: Context Window Bloat Traditional AI Coding (Single Context) [code] Context Foundry Approach (Delegation) [code] --- The Solution: Delegation Architecture High-Level Architecture [code] --- How Delegation Works Step-by-Step Flow User Makes Request [code] Claude Code Intent Detection [code] MCP Tool Call [code] MCP Server Receives Call [code] Delegated Instance Starts [code] Phases Execute [code] Delegated Instance Returns [code] MCP Server Detects Completion [code] User Checks Status [code] --- Context Separation Benefits Main Window Context Usage [code] Delegated Instance Context Usage [code] Parallel Builds Because contexts are separate, you can run MULTIPLE builds simultaneously: [code] --- MCP Server Implementation Core MCP Tool: autonomousbuildanddeployasync File: [code] --- Prompt Architecture Orchestrator Prompt is NOT a System Prompt! Critical Understanding: [code] What Actually Happens: [code] orchestratorprompt.txt is just a detailed task description! Orchestrator Prompt Structure File: (1000+ lines) [code] This is a USER task, not a system prompt modification! --- Agent Lifecycle Ephemeral Agents Agents exist only during the delegated instance's execution: [code] What Persists: [code] Why This Is Good: Clean slate each build - No accumulated assumptions Predictable behavior - Same inputs = same outputs No state pollution - Can't carry forward errors Scalable - Agents don't accumulate context over time --- Comparison with Other Approaches vs. Single Long Conversation | Aspect | Single Conversation | Context Foundry Delegation | |--------|-------------------|---------------------------| | Context usage | 80%+ after one build | < 1% in main window | | Multiple builds | Can't run in parallel | Run simultaneously | | Continue working | Blocked while building | Non-blocking | | Context limit | Hit after 1-2 builds | Never (separate contexts) | | Build isolation | All in one context | Each build separate | vs. API-Based Orchestration | Aspect | API Orchestration | Claude Code Delegation | |--------|------------------|----------------------| | Cost | Pay per token | Fixed ($20/month unlimited) | | Tool access | Limited to API tools | Full Claude Code toolset | | File operations | Via API calls | Native file system | | Bash commands | Via API | Direct terminal access | | State persistence | Must track via code | File-based artifacts | vs. Cursor/Copilot | Aspect | Cursor/Copilot | Context Foundry | |--------|--------------|----------------| | Workflow | Interactive guidance | Fully autonomous | | Human input | Required for each step | Optional (checkpoints) | | Test failures | Manual debugging | Self-healing auto-fix | | Deployment | Manual git operations | Automatic GitHub push | | Pattern learning | No learning mechanism | Self-learning from builds | --- FAQ Q: Does Context Foundry modify Claude Code's system prompt? A: NO! The orchestrator prompt is sent as a USER message, not a system prompt. Claude Code's system prompt remains unchanged. Q: Will using Context Foundry affect my regular Claude Code usage? A: NO! Builds run in separate Claude Code instances. Your main window is completely unaffected. Q: Can I use Claude Code for other tasks while a build is running? A: YES! Builds run in the background. Your main Claude window stays available for other work. Q: How many builds can run in parallel? A: As many as your system resources allow. Each build is an independent process with its own context. Q: What if I close my main Claude Code window during a build? A: The delegated build continues running! It's a separate process. You can check status later with . Q: Can I see the agent conversations? A: Not currently. You see their OUTPUT (scout-report.md, architecture.md, etc.) but not the internal conversation. This could be added in verbose mode. Q: What happens if a build takes longer than expected? A: The MCP server has configurable timeouts (default 90 min). You can increase via parameter. Q: Can I pause a running build? A: Not currently. Builds run autonomously start-to-finish. You can kill the process if needed. --- Summary Context Foundry's delegation model: ✅ Keeps your main context clean (< 1% usage) ✅ Enables parallel builds (separate processes) ✅ Non-blocking (continue working while building) ✅ Scalable (each build gets fresh 200K context) ✅ Isolated (builds don't interfere with each other) Key Innovation: Using subprocess delegation instead of single conversation enables autonomous, parallel, walk-away development that doesn't pollute your main working context. The Result: You can build multiple applications simultaneously while your main Claude Code window stays clean and available for other work. --- For More Information: FAQ.md - Comprehensive Q&A USERGUIDE.md - Step-by-step usage FEEDBACK_SYSTEM.md - Self-learning patterns docs/ARCHITECTURE.md - Stateless conversation design","url":"/docs/technical/delegation-model","weight":1,"tags":[],"headings":["Context Foundry Delegation Model","Table of Contents","Overview","The Problem: Context Window Bloat","Traditional AI Coding (Single Context)","Context Foundry Approach (Delegation)","The Solution: Delegation Architecture","High-Level Architecture","How Delegation Works","Step-by-Step Flow","Context Separation Benefits","Main Window Context Usage","Delegated Instance Context Usage","Parallel Builds","MCP Server Implementation","Core MCP Tool: autonomous_build_and_deploy_async","Prompt Architecture","Orchestrator Prompt is NOT a System Prompt!","Orchestrator Prompt Structure","Agent Lifecycle","Ephemeral Agents","Comparison with Other Approaches","vs. Single Long Conversation","vs. API-Based Orchestration","vs. Cursor/Copilot","FAQ","Q: Does Context Foundry modify Claude Code's system prompt?","Q: Will using Context Foundry affect my regular Claude Code usage?","Q: Can I use Claude Code for other tasks while a build is running?","Q: How many builds can run in parallel?","Q: What if I close my main Claude Code window during a build?","Q: Can I see the agent conversations?","Q: What happens if a build takes longer than expected?","Q: Can I pause a running build?","Summary"]},{"title":"MCP Server Architecture","slug":"mcp-server-architecture","category":"technical","categoryName":"Technical","description":"Technical Deep Dive - Complete System Architecture","content":"Context Foundry MCP Server Architecture Technical Deep Dive - Complete System Architecture Audience: Developers, contributors, and technical users who want to understand how Context Foundry's MCP server works internally. --- Table of Contents Overview Technology Stack MCP Protocol Integration Subprocess Delegation Model Tool Implementations Context Isolation Mechanism Pattern Library Integration Error Handling & Recovery Performance & Scalability Security Considerations Testing & Validation Troubleshooting & Debugging --- Overview Context Foundry 2.0 is built as an MCP (Model Context Protocol) server that integrates with Claude Code CLI to enable fully autonomous software development workflows. Key Innovation Traditional AI Coding: [code] Context Foundry Approach: [code] Core Principles Delegation over Monopolization - Spawn separate processes instead of hogging main session File-based over Conversation-based - Persist artifacts, not chat history Ephemeral Agents over Persistent State - Clean slate each build Self-healing over Manual Debugging - Auto-fix test failures Autonomous over Interactive - Walk away, return to finished project --- Technology Stack Core Components [code] Dependencies : [code] External Tools: [code] --- MCP Protocol Integration What is MCP? Model Context Protocol (MCP) is a standard protocol for connecting AI models to tools and data sources. It defines: Tools: Functions the AI can call Resources: Data the AI can access Prompts: Reusable prompt templates Communication: JSON-RPC 2.0 over stdio Context Foundry uses MCP for: Exposing build tools to Claude Code Managing async task delegation Tracking build status Returning results to main session MCP Server Initialization File: (lines 1-50) [code] MCP Message Flow User Request → MCP Tool Call → Response: [code] --- Subprocess Delegation Model Why Subprocess Delegation? Problem: Building in main Claude Code session consumes context window Solution: Spawn fresh Claude Code instance for each build Benefits: ✅ Main session stays clean (< 1% context usage) ✅ Can run multiple builds in parallel ✅ Each build gets fresh 200K context ✅ No context pollution between builds ✅ User can continue working Implementation Details Tool: [code] Process Lifecycle [code] Monitoring & Status Checking Tool: [code] --- Tool Implementations Full Tool Catalog Context Foundry MCP server exposes 9 tools: | Tool | Type | Purpose | Blocking | |------|------|---------|----------| | | Info | Get server status | Instant | | | Build | Legacy build tool | Blocking | | | Enhancement | Enhance existing project (coming soon) | Blocking | | | Delegation | Delegate simple task | Blocking | | | Delegation | Delegate task (async) | Non-blocking | | | Build | Full build workflow | Blocking | | | Build | Full build workflow (async) | Non-blocking | | | Status | Check task status | Instant | | | Status | List all tasks | Instant | Tool Parameter Specifications : [code] : [code] --- Context Isolation Mechanism The Core Problem Traditional approach: [code] Context Foundry approach: [code] How Isolation Works [code] Key Insights Separate Operating System Processes Main Claude Code: PID 12345 Delegated Instance: PID 67890 No shared memory Separate Context Windows Each process has own 200K token budget No cross-contamination Build can use 80% of context without affecting main window Communication via Files Only No message passing between processes Results written to MCP server reads file after process exits Clean Slate Each Build Every build starts fresh No accumulated context from previous builds Predictable, repeatable behavior --- Pattern Library Integration Where Patterns Live Global Patterns (shared across all builds): [code] Project Patterns (project-specific): [code] How Patterns Are Used Phase 1: Scout reads global patterns: [code] Phase 7: Feedback updates patterns: [code] MCP Server Pattern Access MCP server provides pattern management tools: [code] --- Error Handling & Recovery Timeout Handling [code] Process Crash Handling [code] Self-Healing Test Loop Built into delegated instance, not MCP server! The orchestrator prompt includes self-healing logic: [code] --- Performance & Scalability Concurrent Build Limits No artificial limits! Constrained only by system resources: [code] Resource Optimization Process Cleanup: [code] Output Buffering: [code] File-based Artifacts: [code] Benchmark Performance Measured on MacBook Pro (M1, 16GB RAM): | Metric | Value | |--------|-------| | Single build time | 7-15 minutes | | Parallel builds (4x) | 12-18 minutes | | Memory per build | ~800MB | | CPU per build | 1.5 cores avg | | Main context usage | < 1% (0.7% measured) | | Delegated context usage | 30-50% | Speedup from Parallel Execution: [code] --- Security Considerations Process Isolation Each delegated instance: ✅ Runs with same user permissions as MCP server ✅ Cannot escalate privileges ✅ Isolated from other delegated instances ✅ Cannot access main Claude window's context ❌ Not sandboxed (has full filesystem access) Implications: [code] API Key Handling Environment Variables: [code] Recommended Practice: [code] Code Injection Prevention User input is sanitized: [code] --- Testing & Validation MCP Server Tests Test harness location: [code] Integration Tests Manual test procedure: [code] --- Troubleshooting & Debugging Enable Verbose Logging Add to : [code] View logs: [code] Inspect Process State Check running delegated instances: [code] Debug Subprocess Output Read stdout/stderr while process runs: [code] Common Issues \"MCP Server not responding\" [code] \"Process spawns but nothing happens\" [code] \"Build succeeds but no GitHub deployment\" [code] --- Summary Context Foundry MCP Server Architecture: ✅ FastMCP 2.0 framework for MCP protocol ✅ Subprocess delegation for context isolation ✅ 9 MCP tools for build automation and task management ✅ File-based artifacts (no memory bloat) ✅ Pattern library integration (self-learning) ✅ Self-healing test loops (autonomous debugging) ✅ Parallel execution (unlimited concurrent builds) ✅ Professional error handling (timeouts, crashes, recovery) Key Files: - MCP server implementation (1400 lines) - Build workflow meta-prompt (1000+ lines) - Project-shareable MCP configuration - Python dependencies For More Information: CONTEXTPRESERVATION.md - Context flow deep dive DELEGATIONMODEL.md - How delegation keeps context clean CLAUDECODEMCPSETUP.md - Setup and troubleshooting FAQ.md - Frequently asked questions --- Version: 2.0.1 | Last Updated:** October 2025","url":"/docs/technical/mcp-server-architecture","weight":1,"tags":[],"headings":["Context Foundry MCP Server Architecture","Table of Contents","Overview","Key Innovation","Core Principles","Technology Stack","Core Components","Dependencies","MCP Protocol Integration","What is MCP?","MCP Server Initialization","MCP Message Flow","Subprocess Delegation Model","Why Subprocess Delegation?","Implementation Details","Process Lifecycle","Monitoring & Status Checking","Tool Implementations","Full Tool Catalog","Tool Parameter Specifications","Context Isolation Mechanism","The Core Problem","How Isolation Works","Key Insights","Pattern Library Integration","Where Patterns Live","How Patterns Are Used","MCP Server Pattern Access","Error Handling & Recovery","Timeout Handling","Process Crash Handling","Self-Healing Test Loop","Performance & Scalability","Concurrent Build Limits","Resource Optimization","Benchmark Performance","Security Considerations","Process Isolation","API Key Handling","Code Injection Prevention","Testing & Validation","MCP Server Tests","Integration Tests","Troubleshooting & Debugging","Enable Verbose Logging","Inspect Process State","Debug Subprocess Output","Common Issues","Summary"]},{"title":"Architecture Decisions","slug":"architecture-decisions","category":"reference","categoryName":"Reference","description":"Last Updated: October 18, 2025 Version: 2.0.0 Authors: Context Foundry Team","content":"Context Foundry 2.0 - Architecture Decisions Last Updated: October 18, 2025 Version: 2.0.0 Authors: Context Foundry Team --- Table of Contents Overview Native Instead of Python: What Changed and Why New Innovations: What They Are and Why They Matter Why We Moved Away from Certain Features Technical Implementation Deep Dives Migration Guide from 1.x to 2.0 --- Overview Context Foundry 2.0 represents a fundamental architectural shift from version 1.x. While the original Context Foundry was built on Python scripts orchestrating API calls to various AI providers, version 2.0 embraces Claude Code's native capabilities through the Model Context Protocol (MCP). Core Philosophy Change: v1.x: \"Build a Python CLI that orchestrates AI agents via API calls\" v2.0: \"Empower Claude Code to orchestrate itself through meta-prompts and native agent capabilities\" This document explains the technical reasoning behind this shift, the innovations it enabled, and why certain features were deprecated. --- Native Instead of Python: What Changed and Why What Changed Context Foundry 1.x Approach Architecture: [code] Key Components: : Python script managing agent workflow Anthropic Agent SDK: Python library for agent patterns Environment variables: API keys for Anthropic, OpenAI, Gemini, etc. CLI commands: , , Cost tracking: Monitor API usage across providers Example 1.x Code: [code] Limitations: API Dependency: Required API keys and pay-per-token billing Context Loss: Each API call started fresh; context passed via messages Python Overhead: Python script had to manage state, orchestration, error handling Limited Tool Access: Agents couldn't use Claude Code's native tools (Read, Edit, Bash, etc.) No Interactive Debugging: Agents ran in isolated API calls Context Foundry 2.0 Approach Architecture: [code] Key Components: : MCP server exposing delegation tools : Meta-prompt instructing self-orchestration Native : Claude Code's built-in agent creation system Claude Max subscription: No API calls, unlimited usage File-based context: directory stores all artifacts Example 2.0 Flow: [code] In the Fresh Claude Instance (orchestratorprompt.txt): [code] Advantages: No API Calls: Uses Claude Max subscription (monthly fee, unlimited usage) Native Tool Access: Agents use Read, Edit, Bash, Glob, Grep directly Context Preservation: File-based artifacts maintain context across phases Self-Orchestration: Claude manages its own workflow via meta-prompts Interactive Capabilities: Can debug, inspect, modify during execution Why We Changed Reason 1: Cost Model Shift Problem with 1.x: [code] Solution in 2.0: [code] Result: 95%+ cost reduction for heavy users. Reason 2: Tool Access Limitations Problem with 1.x: When using Anthropic Agent SDK, agents could only: Send text prompts Receive text responses Use predefined tool schemas (custom functions we defined) They could NOT: Use Claude Code's native Read/Edit/Bash tools Access file system directly Run git commands natively Use Glob/Grep for code search Workaround in 1.x: [code] Solution in 2.0: [code] Scout agent directly invokes Claude Code's native tools - no Python wrapper needed. Result: Agents work the same way a human would in Claude Code CLI. Reason 3: Context Preservation Problem with 1.x: API calls are stateless. Context passed manually: [code] Solution in 2.0: [code] Result: No token limit issues, context persists across sessions. Reason 4: Simplicity and Maintainability 1.x Complexity: : 800+ lines : 5 Python files (scout.py, architect.py, builder.py, tester.py, deployer.py) : 7 provider adapters (anthropic.py, openai.py, gemini.py, etc.) : 4 utility modules Total: ~3000 lines of Python to maintain 2.0 Simplicity: : 900 lines (all tools) : 469 lines (meta-prompt) Total: ~1400 lines, no external dependencies Result: 53% reduction in codebase size, easier to maintain and extend. When to Use Each Approach Use Context Foundry 2.0 (Native ) when: ✅ You have a Claude Max subscription ($20/month unlimited) ✅ You're building multiple projects per month (cost-effective) ✅ You want agents to use native Claude Code tools ✅ You prefer file-based context preservation ✅ You want autonomous, walk-away workflows Use Context Foundry 1.x (Python API) when: ✅ You need multi-provider support (OpenAI, Gemini, etc.) ✅ You're building <5 projects per month (pay-per-use cheaper) ✅ You need custom tool schemas not in Claude Code ✅ You want programmatic control over every API call ✅ You're integrating into existing Python workflows --- New Innovations: What They Are and Why They Matter Self-Healing Test Loops What It Is: A fully autonomous test-fix-retest cycle that runs without human intervention: [code] Technical Implementation: In : [code] Why It Matters: Before (Context Foundry 1.x): [code] After (Context Foundry 2.0): [code] Real-World Example: User request: \"Build a weather API with Express.js\" Iteration 1 (FAIL): Tests failed: Tester identified: API response not validated before accessing properties Architect redesigned: Add response validation middleware Builder implemented: Added validation layer Duration: +2 minutes Iteration 2 (PASS): All tests passed Deployed to GitHub Total time: 7.42 minutes (vs potential hours of manual debugging) Configuration: [code] Success Metrics: 73% of failures fixed on iteration 1 22% fixed on iteration 2 4% fixed on iteration 3 1% reach max and report failure Autonomous Build/Deploy Tool What It Is: A single MCP tool that executes the entire Scout → Architect → Builder → Test → Documentation → Deploy workflow autonomously. Technical Implementation: [code] Why It Matters: Before: [code] Total time: 30-60 minutes of active work After: [code] Total time: 7-15 minutes, zero active involvement Real-World Usage: [code] Configuration Options: [code] Parallel Async Delegation What It Is: The ability to spawn multiple independent Claude Code instances simultaneously, each working on separate tasks in isolated contexts. Technical Implementation: [code] Why It Matters: Use Case 1: Parallel Component Development [code] Use Case 2: Parallel Testing [code] Use Case 3: Parallel Analysis [code] Real-World Example: [code] Performance Comparison: | Scenario | Sequential Time | Parallel Time | Speed Gain | |----------|----------------|---------------|------------| | 3 components (10min each) | 30 min | 10 min | 3x faster | | 5 microservices (3min each) | 15 min | 3 min | 5x faster | | 10 test suites (2min each) | 20 min | 2 min | 10x faster | Meta-Prompt Orchestration What It Is: Using text-based prompts to instruct AI how to orchestrate itself through complex workflows, rather than using Python code or API calls. Traditional Orchestration (Code-Based): [code] Meta-Prompt Orchestration (Prompt-Based): [code] How It Works: User triggers autonomous build: [code] MCP server loads meta-prompt: [code] Spawns fresh Claude instance with meta-prompt: [code] Claude reads meta-prompt and self-orchestrates: [code] Returns JSON result when complete: [code] Why It Matters: Advantages of Meta-Prompt Orchestration: Natural Language Programming: Workflows defined in human-readable text Easy to modify without coding Non-programmers can understand and adapt Self-Modifying Workflows: AI can interpret and adapt instructions Can handle edge cases creatively Not limited by rigid code logic No Code Deployment: Just update the text file No Python packages to install No version conflicts Emergent Behavior: AI can innovate within guidelines Can combine phases in unexpected ways Adapts to unique situations Example: Handling Unexpected Situations Code-Based Orchestrator (1.x): [code] Meta-Prompt Orchestrator (2.0): [code] The AI reads this, understands the intent, and makes intelligent decisions even for scenarios not explicitly coded. Real-World Comparison: [code] Editing Workflows: Code-based (1.x): [code] Meta-prompt based (2.0): [code] --- Why We Moved Away from Certain Features Multi-Provider Support (Removed) What It Was: Context Foundry 1.x supported 7 AI providers: Anthropic (Claude) OpenAI (GPT-4) Google (Gemini) Cohere Mistral Perplexity Together AI Implementation: [code] Why We Removed It: Reason 1: Claude Code Native Integration Context Foundry 2.0 is built as an MCP server for Claude Code CLI specifically. When you run: [code] You're already committed to using Claude. Multi-provider support doesn't make sense in this context. Before (1.x): Python CLI could call any provider After (2.0): MCP server delegates to Claude CLI (already provider-specific) Reason 2: Maintenance Burden Supporting 7 providers required: 7 separate adapter classes Different API schemas for each Different rate limiting strategies Different error handling Different cost tracking Testing across all providers Code burden: [code] Result: Removed 890 lines of provider-specific code. Reason 3: Quality Over Variety In practice, users consistently chose Claude Sonnet 4 for quality: Usage statistics (1.x): Claude Sonnet: 89% GPT-4: 7% Gemini: 3% Others: 1% Decision: Focus on one excellent provider (Claude) rather than seven mediocre integrations. Reason 4: Cost Model Mismatch Different providers have different pricing: Anthropic: $3/million input tokens OpenAI: $2.50/million input tokens Gemini: $0.35/million input tokens Problem in 1.x: Users would start with cheap provider (Gemini), realize quality issues, switch to Claude mid-project. This caused: Inconsistent output quality Context loss during provider switches Complex cost tracking Solution in 2.0: Use Claude Max ($20/month unlimited) - predictable cost, consistent quality. Python CLI ( command) (Deprecated) What It Was: [code] Why We Deprecated It: Reason 1: Redundant with Claude Code CLI Users already have Claude Code CLI: [code] Adding another CLI () just creates confusion: [code] Decision: Use existing CLI instead of creating a new one. Reason 2: Installation Complexity Python CLI required: [code] MCP server requires: [code] Result: Eliminated installation friction. Reason 3: Versioning and Updates Python CLI issues: Users on different versions Breaking changes require migration guides Dependency conflicts (click, anthropic SDK, etc.) Uninstall/reinstall for updates MCP server benefits: Single file No pip packages for core functionality Update = edit file, restart server No version conflicts Migration Path: 1.x users who loved the CLI: We preserved the Python CLI in directory for users who prefer it: [code] But we recommend migrating to 2.0 MCP approach for better integration. Context Compaction at 50% Usage (Removed) What It Was: In Context Foundry 1.x, when conversation context reached 50% of token limit, the system would automatically \"compact\" context: [code] Why We Removed It: Reason 1: File-Based Context Eliminates Token Limits Problem in 1.x: All context passed in conversation: [code] After 7-8 phases, hit token limit → must compact → lose context. Solution in 2.0: Context stored in files: [code] Each phase reads only what it needs: [code] Reason 2: Lossy Compression Caused Errors Example failure in 1.x: [code] Why it happened: Summarization lost specific details needed for implementation. No longer an issue in 2.0: Files never summarize - full detail always available. Reason 3: Unpredictable Behavior Users reported: \"Why did the builder forget the Scout's recommendation?\" \"The architecture said to use Redis but builder didn't include it\" \"Tests are checking for features that weren't implemented\" Root cause: Context compaction removed crucial details mid-workflow. Solution: Remove compaction entirely, use file-based context. Cost Tracking (Deprioritized) What It Was: [code] Why We Deprioritized It: Reason 1: Flat-Rate Pricing 1.x pricing (pay-per-token): [code] 2.0 pricing (Claude Max subscription): [code] Reason 2: Simplified Code Removing cost tracking eliminated: (200 lines) (provider rates) Database for cost history Cost reports and analytics Result: 250+ lines removed, simpler codebase. Reason 3: Still Available If Needed For users who want cost tracking: [code] But not worth the complexity for flat-rate pricing. Pattern Library with Semantic Search (Postponed) What It Was: [code] Why We Postponed It: Reason 1: Agents Learn Patterns Naturally Observation: Scout agents naturally research similar projects: [code] No need for pre-built pattern library - Scout finds current, relevant examples. Reason 2: Patterns Become Outdated Problem with static pattern library: [code] Solution with Scout research: Scout always finds latest best practices via WebSearch and documentation. Reason 3: Maintenance Burden Pattern library requires: Keeping 50+ patterns up to date Updating for new frameworks Testing patterns still work Reviewing community contributions Versioning patterns Decision: Let Scout research replace static patterns (always current, no maintenance). Future Consideration: We may add pattern library in 2.1 as an optional enhancement: [code] But not critical for 2.0 release. --- Technical Implementation Deep Dives Deep Dive 1: How Self-Healing Test Loop Works Internally File: Lines 106-186 [code] Step-by-Step Example: [code]markdown Test Results - Iteration 1 Status: FAILED Failed Tests: Test: POST /auth/login should return JWT token File: tests/auth.test.js:45 Expected: 200 status code Received: 500 status code Error: [code] Root Cause: The function throws an error when is undefined. The login route doesn't handle this error, causing a 500 response. Recommended Fix: Add try-catch block in login controller Return 500 with proper error message if JWTSECRET missing Add environment variable validation on startup [code]markdown Fix Strategy - Iteration 1 Problem: Unhandled promise rejection in auth controller when JWTSECRET missing Solution: Fix 1: Add error handling to auth controller File: controllers/auth.js Change: [code] Fix 2: Add environment validation File: app.js Add at startup: [code] [code]markdown Test Final Report Status: PASSED Iterations: 2 Iteration 1: Failed: 1 test (JWT error handling) Root cause: Missing error handling in auth controller Fix: Added try-catch blocks and environment validation Iteration 2: Passed: All tests (3/3) Duration: 111ms Coverage: 94% Conclusion: All tests passing. Proceeding to documentation and deployment. [code] Key Implementation Details: Iteration Tracking: [code] Failure Documentation: [code] Fix Strategy Documentation: [code] Phase Transitions: [code] Deep Dive 2: Process Spawning and stdin=DEVNULL Why Delegation Was Hanging: Problem Code: [code] Why it hangs: Subprocess inherits stdin: [code] Claude CLI prompts for confirmations: [code] Parent process blocks: [code] Solution: [code] What does: [code] Combined with : [code] Full Fix Explanation: [code] Why each part matters: : Output mode for automation (vs interactive chat) : Never prompt for confirmations : Don't load MCP servers in child process (prevents infinite recursion) : No stdin → Claude knows it's fully automated : Output appears immediately (not buffered until process exits) Before vs After: [code] --- Migration Guide from 1.x to 2.0 For Existing Context Foundry 1.x Users Quick Migration: 1.x Workflow: [code] 2.0 Workflow: [code] Setup Required: Install Claude Code CLI (if not already): [code] Install MCP server dependencies: [code] Configure MCP connection: [code] Verify configuration: [code] Start using 2.0: [code] Feature Mapping: | 1.x Feature | 2.0 Equivalent | Notes | |-------------|----------------|-------| | | | More autonomous, includes testing | | | with | Self-healing included | | | with | | | | | For async tasks only | | Multi-provider | N/A (Claude only) | See v1.x-legacy branch | | Cost tracking | Manual calculation | Not needed with Claude Max | | Pattern library | Scout research | More up-to-date than static patterns | Preserving 1.x Workflows: If you still want to use Context Foundry 1.x Python CLI: [code] Both versions can coexist. --- Conclusion Context Foundry 2.0 represents a fundamental shift from \"Python orchestrating AI via APIs\" to \"AI orchestrating itself via meta-prompts and native tools.\" Key Takeaways: Native enables AI to use Claude Code's full tool suite Self-healing test loops eliminate manual debugging cycles Autonomous build/deploy enables true \"walk away\" development Parallel async delegation speeds up multi-component projects Meta-prompt orchestration replaces rigid Python code with adaptable instructions Removed features (multi-provider, Python CLI, context compaction, cost tracking, pattern library) were eliminated to: Reduce complexity Focus on Claude Code integration Leverage flat-rate pricing Enable file-based context Let AI discover patterns naturally The result: A simpler, more powerful system that does more with less code. --- Version: 2.0.0 Last Updated: October 18, 2025 License: MIT Repository:** https://github.com/snedea/context-foundry","url":"/docs/reference/architecture-decisions","weight":0.9,"tags":[],"headings":["Context Foundry 2.0 - Architecture Decisions","Table of Contents","Overview","Native /agents Instead of Python: What Changed and Why","What Changed","Context Foundry 1.x Approach","Context Foundry 2.0 Approach","Why We Changed","Reason 1: Cost Model Shift","Reason 2: Tool Access Limitations","Reason 3: Context Preservation","Reason 4: Simplicity and Maintainability","When to Use Each Approach","New Innovations: What They Are and Why They Matter","1. Self-Healing Test Loops","2. Autonomous Build/Deploy Tool","3. Parallel Async Delegation","4. Meta-Prompt Orchestration","Why We Moved Away from Certain Features","1. Multi-Provider Support (Removed)","Reason 1: Claude Code Native Integration","Reason 2: Maintenance Burden","Reason 3: Quality Over Variety","Reason 4: Cost Model Mismatch","2. Python CLI (foundry command) (Deprecated)","Reason 1: Redundant with Claude Code CLI","Reason 2: Installation Complexity","Reason 3: Versioning and Updates","Migration Path:","3. Context Compaction at 50% Usage (Removed)","Reason 1: File-Based Context Eliminates Token Limits","Reason 2: Lossy Compression Caused Errors","Reason 3: Unpredictable Behavior","4. Cost Tracking (Deprioritized)","Reason 1: Flat-Rate Pricing","Reason 2: Simplified Code","Reason 3: Still Available If Needed","5. Pattern Library with Semantic Search (Postponed)","Reason 1: Agents Learn Patterns Naturally","Reason 2: Patterns Become Outdated","Reason 3: Maintenance Burden","Future Consideration:","Technical Implementation Deep Dives","Deep Dive 1: How Self-Healing Test Loop Works Internally","Fix 2: Add environment validation","Migration Guide from 1.x to 2.0","For Existing Context Foundry 1.x Users","Quick Migration:","Feature Mapping:","Preserving 1.x Workflows:","Conclusion"]},{"title":"Claude Code MCP Setup","slug":"claude-code-mcp-setup","category":"reference","categoryName":"Reference","description":"This guide explains how to set up and use the Context Foundry MCP server to delegate tasks from your main Claude Code CLI session to fresh Claude Code...","content":"Claude Code MCP Server Setup Guide Overview This guide explains how to set up and use the Context Foundry MCP server to delegate tasks from your main Claude Code CLI session to fresh Claude Code instances. This allows you to: Delegate work to clean contexts: Spawn new Claude Code processes with fresh context windows Parallel processing: Run multiple tasks in separate instances Context isolation: Keep your main session focused while delegating sub-tasks Automated workflows: Chain multiple delegations for complex pipelines Architecture [code] Prerequisites Required Python 3.10 or higher [code] Context Foundry dependencies [code] Claude Code CLI (installed and in PATH) [code] Optional API Keys: If using models that require API keys (Anthropic, OpenAI, etc.) Git: For version control of generated code Configuration Approaches Claude Code MCP servers can be configured in two ways: Project-Scoped Configuration (Recommended) File: (in your project directory) Advantages: ✅ Shareable with team via version control ✅ Different settings per project ✅ Portable across machines ✅ Automatically detected when you run in the project directory Setup command: [code] Verification: [code] Note: Project-scoped servers won't appear in (which only shows global config). They're automatically detected when you run in the project directory. Global Configuration File: Advantages: ✅ Available in all projects ✅ No need to be in specific directory ✅ Appears in Setup command: [code] Verification: [code] Important: Global config requires absolute paths. If you move the context-foundry directory, you must update the paths. --- Installation Step 1: Install Dependencies [code] Step 2: Verify MCP Server Code The MCP server has been modified to include the tool: [code] Step 3: Configure Claude Code MCP Settings Choose either project-scoped (recommended) or global configuration: Option A: Project-Scoped (Recommended) [code] Verify: [code] Option B: Global Configuration [code] Verify: [code] See \"Configuration Approaches\" section above for detailed comparison and when to use each option. Step 4: Verify Claude Code CLI [code] Usage Terminal 1: Start the MCP Server Open a terminal and run: [code] You should see: [code] Keep this terminal running! The MCP server must remain active for Claude Code to connect to it. Terminal 2: Connect Claude Code CLI Open a new terminal and start Claude Code: [code] Claude Code will automatically detect the MCP server configuration and connect to it. Using the Delegation Tool Inside your Claude Code session, you can now use the tool: Example 1: Simple Task [code] The tool will: Spawn a fresh process Pass the task to it Wait for completion Return the results Example 2: Specify Working Directory [code] Example 3: With Timeout and Flags [code] Tool Parameters | Parameter | Type | Required | Default | Description | |-----------|------|----------|---------|-------------| | | string | Yes | - | The task/prompt to give to the new Claude Code instance | | | string | No | Current directory | Directory where claude should run | | | float | No | 10.0 | Maximum execution time in minutes | | | string | No | None | Additional CLI flags (e.g., \"--model claude-sonnet-4\") | Example Workflows Workflow 1: Parallel Code Generation Delegate multiple independent tasks to separate instances: [code] Each delegation runs in a fresh Claude Code instance with clean context. Workflow 2: Analysis and Documentation [code] Workflow 3: Testing and Quality Assurance [code] Troubleshooting Issue: MCP Server won't start Error: Solution: [code] Error: or Python version errors Solution: [code] Issue: Claude Code doesn't see the MCP tools Symptoms: tool not available Solutions: Verify MCP settings file exists: [code] Check the path in mcpsettings.json is correct: [code] Restart Claude Code: Exit the current Claude Code session Restart: Check MCP server is running in Terminal 1 Issue: \"claude command not found\" Error when delegating: Solutions: Find where claude is installed: [code] Add to PATH temporarily: [code] Add to PATH permanently: [code] Verify: [code] Issue: Delegations timeout Symptoms: Tasks consistently hit timeout limit Solutions: Increase timeout: [code] Break tasks into smaller pieces: Instead of \"Build entire application\" Use: \"Create user authentication module\" (separate delegations) Check if task is stuck: Some tasks may be waiting for user input Ensure tasks are fully automated Issue: Working directory errors Error: Solutions: Create the directory first: [code] Use absolute paths: [code] Verify the path exists: [code] Issue: Output not captured correctly Symptoms: Empty stdout/stderr or missing output Solutions: Check both stdout and stderr: Output may be in either section Some commands may not produce output: This is normal for some tasks Increase verbosity if possible: [code] Testing Your Setup Follow the test scenarios in to verify everything works: [code] Quick test: [code] Expected result: ✅ Success status file created Duration shown Output captured Advanced Configuration Custom Python Version If you need to use a specific Python version for the MCP server: Edit : [code] Environment Variables Pass environment variables to the MCP server: [code] Disable the MCP Server To temporarily disable without deleting the configuration: [code] Performance Tips Timeout tuning: Simple tasks: 2-5 minutes Code generation: 5-10 minutes Analysis: 10-20 minutes Complex projects: 20-30 minutes Working directory organization: [code] Use separate directories for parallel delegations to avoid conflicts. Monitor resource usage: [code] Security Considerations API Keys: Be careful not to pass sensitive API keys in Working Directory: Delegated instances have full access to the working directory Timeout: Set reasonable timeouts to prevent runaway processes Code Review: Always review code generated by delegated instances Next Steps After successful setup: ✅ Verify: Run test scenarios from 🔧 Experiment: Try delegating different types of tasks 📊 Optimize: Tune timeouts based on your use cases 🚀 Automate: Create workflows that chain multiple delegations 📚 Document: Record your own delegation patterns Additional Resources MCP Server Code: Test Examples: MCP Settings: MCP Protocol Docs:** https://modelcontextprotocol.io/ Getting Help If you encounter issues: Check this troubleshooting guide Review the test examples Check MCP server logs (Terminal 1) Verify all prerequisites are met Try the test scenarios step-by-step Summary Terminal 1 - Start MCP Server: [code] Terminal 2 - Use Claude Code: [code] Quick Test: [code] That's it! You now have a working MCP server that can delegate tasks to fresh Claude Code instances. 🚀","url":"/docs/reference/claude-code-mcp-setup","weight":0.9,"tags":[],"headings":["Claude Code MCP Server Setup Guide","Overview","Architecture","Prerequisites","Required","Optional","Configuration Approaches","1. Project-Scoped Configuration (Recommended)","2. Global Configuration","Installation","Step 1: Install Dependencies","Step 2: Verify MCP Server Code","Step 3: Configure Claude Code MCP Settings","Option A: Project-Scoped (Recommended)","Option B: Global Configuration","Step 4: Verify Claude Code CLI","Usage","Terminal 1: Start the MCP Server","Terminal 2: Connect Claude Code CLI","Using the Delegation Tool","Example 1: Simple Task","Example 2: Specify Working Directory","Example 3: With Timeout and Flags","Tool Parameters","delegate_to_claude_code()","Example Workflows","Workflow 1: Parallel Code Generation","Workflow 2: Analysis and Documentation","Workflow 3: Testing and Quality Assurance","Troubleshooting","Issue: MCP Server won't start","Issue: Claude Code doesn't see the MCP tools","Issue: \"claude command not found\"","Issue: Delegations timeout","Issue: Working directory errors","Issue: Output not captured correctly","Testing Your Setup","Advanced Configuration","Custom Python Version","Environment Variables","Disable the MCP Server","Performance Tips","Security Considerations","Next Steps","Additional Resources","Getting Help","Summary","Terminal 1 - Start MCP Server:","Terminal 2 - Use Claude Code:","Quick Test:"]},{"title":"Technical FAQ","slug":"technical-faq","category":"reference","categoryName":"Reference","description":"For Software Developers, Architects, and AI Engineers","content":"Context Foundry Technical FAQ For Software Developers, Architects, and AI Engineers Last Updated: 2025-01-23 Audience: Technical users familiar with distributed systems, AI agents, and software architecture Looking for user-friendly answers? If you're new to Context Foundry or want quick, accessible answers about how it works, see the Main FAQ which focuses on transparency and demystifying the system. --- Table of Contents File Management & Architecture The \"Secret Sauce\" - Prompt Engineering Agent Architecture & Lifecycle Token Management & Context Windows Parallelization & Coordination Authentication & API Usage MCP Server Architecture & Context Management Performance & Scaling Error Handling & Self-Healing Pattern Learning System Advanced Technical Topics Practical Usage & Limitations Comparisons & Philosophy --- File Management & Architecture Q1: Where are the files stored? A: In the project directory, not in the Context Foundry installation directory. [code] Why this matters: Each project has its own directory Files persist across builds (useful for debugging) Can be version controlled (add to if preferred) No interference between different projects Global patterns are stored separately at for cross-project learning. --- Q2: Why files instead of JSON for core architecture? A: Markdown files are the backbone of Context Foundry for several critical reasons: Agent-Friendly Format LLMs like Claude are trained heavily on markdown documentation Natural language + structure = better comprehension Code blocks with syntax highlighting improve parsing Human-Readable Developers can read to understand what the Scout discovered Architects can review to verify the design No need to parse JSON to understand build decisions Git-Diffable Meaningful diffs when architecture changes Can track evolution of design across iterations Comments and explanations included inline Context Efficiency Prose is more token-efficient than structured data for complex ideas \"Create a React app with Zustand state management\" (8 tokens) vs extensive JSON schema Agents can summarize and reference instead of including full content Self-Documenting Architecture decisions explained in-place Future agents (in self-healing loop) understand why choices were made Serves as project documentation after build completes Example: isn't just a file list—it explains the why behind each component. --- Q3: What's in each file? What's their purpose? A: Each file serves a specific phase in the workflow: | File | Phase | Purpose | Typical Size | |------|-------|---------|--------------| | | Phase 1: Scout | Requirements analysis, tech stack decisions, risk identification | 5-10 KB | | | Phase 2: Architect | Complete system design, file structure, implementation plan, test strategy | 15-30 KB | | | Phase 3: Builder | Files created, implementation notes, deviations from architecture | 5-15 KB | | | Phase 4: Test | Test failures, root cause analysis, fix recommendations | 10-20 KB | | | Self-Healing | Fix strategy for failed tests | 5-10 KB | | | Phase 4: Test | Final test results (pass or fail after all iterations) | 5-10 KB | | | Phase 8: Feedback | Build summary (JSON for MCP parsing) | 2-5 KB | JSON Files (structured data for parsing): : Parallel task breakdown with dependencies (Phase 2.5) : Real-time phase tracking for TUI monitoring : Final build results for MCP server Log Directories: : Individual builder agent outputs (Phase 2.5) : Individual test agent outputs (Phase 4.5) --- Q4: How do files persist across phases if agents die after each phase? A: Filesystem is the shared memory. This is a core design principle. The Flow: Phase 1 (Scout): Agent researches, writes , then dies Phase 2 (Architect): New agent spawned, reads , writes , dies Phase 3 (Builder): New agent spawned, reads , writes code files + , dies Phase 4 (Tester): New agent spawned, reads and code, runs tests, writes results Why this works: Files are durable (survive process termination) Each agent starts with fresh 200K context window Agent only loads what it needs (Scout report is 5-10KB, not 150KB of code) No context pollution from previous phases This is fundamentally different from Cursor/Copilot which maintain long-lived sessions. --- Q5: Can I version control the directory? A: Yes, and you probably should! Recommended strategy: [code] Benefits of versioning: Track architecture evolution across builds Understand why design decisions were made Reproduce builds if needed Share architecture with team Alternative: Add entire to if you don't want build artifacts in repo. --- Q6: What happens to files after a successful build? A: They persist permanently (until you delete them). Lifecycle: First build: created, all phase files written Subsequent builds: Files may be overwritten or appended After deployment: Files remain for historical reference Cleanup strategies: [code] Best practice: Keep for at least one successful build cycle—it's invaluable for debugging. --- The \"Secret Sauce\" - Prompt Engineering Q7: Where are the core prompts that make Context Foundry work? A: The \"magic\" is in three main prompt files: | Prompt File | Purpose | Line Count | Link | |-------------|---------|------------|------| | * | Main 8-phase orchestrator | ~1200 lines | View on GitHub | | | Parallel builder agent (Phase 2.5) | ~161 lines | View on GitHub | | | Parallel test agent (Phase 4.5) | ~208 lines | View on GitHub | Supporting prompts: : Scout agent configuration : Architect agent configuration : Builder agent configuration (deprecated in favor of parallel) --- Q8: How does coordinate the entire build? A: It's a single, massive prompt that defines all 8 phases sequentially with strict instructions: Structure: [code] Key Techniques: Imperative Commands: \"You MUST\", \"CRITICAL\", \"MANDATORY\" Step-by-Step Instructions: Numbered lists for each phase Concrete Examples: Actual bash commands to execute Self-Verification: \"Execute: cat file.json | grep status\" Error Handling: \"If X fails, do Y\" Phase Transitions: Explicit \"Proceed to PHASE N\" statements Why this works: Claude follows procedural instructions extremely well when they're explicit and sequential. --- Q9: How do parallel builder agents coordinate without talking to each other? A: Via and filesystem synchronization. The Prompt Structure: [code] Coordination Mechanism: Input: Orchestrator passes task via command line: [code] Shared Context: All builders read same (read-only) File Isolation: Architect ensures each task has unique files (no write conflicts) Completion Signal: Creates file when finished No IPC: Agents never communicate directly—orchestrator coordinates Why this works: No shared memory = no race conditions. Filesystem is atomic. --- Q10: What makes these prompts effective? Can I learn from them? A: Several advanced prompt engineering techniques: Role Clarity [code] Establishes agent identity immediately Sets expectations for behavior Clarifies constraints (parallel, not sequential) Structured Sections with Visual Separators [code] Visual hierarchy helps Claude parse structure CRITICAL/MANDATORY keywords trigger higher attention Boxed sections improve scannability Concrete Examples Over Abstractions [code] Actual bash commands to execute No ambiguity about implementation Success Criteria [code] Clear decision points Explicit error handling paths Self-Verification [code] Agent validates its own work Catches issues before proceeding You can absolutely learn from these! They're open source for this reason. --- Q11: Can I customize the prompts for my specific use case? A: Yes! Prompts are just text files. Here's how: Safe Customizations: Add Domain-Specific Patterns [code] Adjust Parallelism Thresholds [code] Add Custom Test Types [code] Dangerous Customizations (avoid): ❌ Removing phase tracking (breaks TUI monitoring) ❌ Removing file creation (breaks coordination) ❌ Changing the \"use \" instruction (breaks auth inheritance) ❌ Removing error handling sections (reduces reliability) Testing Custom Prompts: [code] --- Agent Architecture & Lifecycle Q12: How many agents are created in a typical build session? A: Depends on project size and parallel execution. Small Project (2-5 files): 1 Scout agent (Phase 1) 1 Architect agent (Phase 2) 2 Builder agents (Phase 2.5 parallel - minimum 2) 1-2 Test agents (Phase 4 or 4.5) 1 Documentation agent (Phase 5) 1 Deployer agent (Phase 6-7) Total: ~8-10 agents Medium Project (6-15 files): 1 Scout 1 Architect 4 Builder agents (Phase 2.5 parallel) 3 Test agents (Phase 4.5: unit + e2e + lint) 1 Documentation 1 Deployer Total: ~11-15 agents Self-healing iteration: +3 agents (Architect + Builder + Tester) per iteration Large Project (16+ files): 1 Scout 1 Architect 8 Builder agents (Phase 2.5 parallel - maximum recommended) 3 Test agents (Phase 4.5) 1 Screenshot agent (Phase 4.75) 1 Documentation 1 Deployer Total: ~16-20 agents Self-healing (up to 3 iterations): +9 agents per iteration Worst case (3 fix iterations): ~45 agents total Why these numbers matter for cost/performance calculations. --- Q13: Do agents share context or run completely isolated? A: Completely isolated. Each agent is a separate process with fresh context. Isolation Mechanisms: Separate Processes [code] Fresh Context Window Each agent starts with 200,000 token budget No pollution from previous phases Can't see other agents' internal reasoning Filesystem as IPC Agents communicate ONLY via files Read: , Write: task-specific output files Signal: files Benefits: ✅ No context bleed between agents ✅ Parallelizable (no shared state) ✅ Fault-tolerant (one agent crash doesn't affect others) ✅ Debuggable (each agent's log is independent) Tradeoffs: ❌ Can't share discoveries during execution ❌ May duplicate work if tasks overlap ❌ Requires good upfront planning (Architect phase) --- Q14: How long does an agent live? When does context get freed? A: Agents live only for their specific task, then die immediately. Lifecycle: [code] Typical Agent Lifespans: Scout: 2-5 minutes Architect: 5-10 minutes Builder (parallel): 3-8 minutes each Tester: 2-5 minutes Documentation: 3-5 minutes Deployer: 1-2 minutes Why this matters: Long-running agents accumulate context debt Killing agents prevents token window overflow Fresh agents = fresh perspective (reduces fixation errors) --- Q15: Can agents communicate during execution, or only via files? A: Only via files. This is a deliberate architectural constraint. Communication Patterns: ❌ NOT Possible: Agent A calling Agent B's API Shared memory/variables Message passing queues Network communication between agents ✅ Supported: Agent A writes file → Agent B reads file Agent A creates marker → Orchestrator detects completion All agents read shared (read-only) Example: Builder Coordination [code] Why this constraint? Simpler reasoning model (no race conditions) Easier to debug (check log files) Matches Unix philosophy (files as interfaces) --- Token Management & Context Windows Q16: How does Context Foundry avoid exceeding Claude's 200K token window? A: Through aggressive context management and agent lifecycle design. Strategy 1: Agent Death After Each Phase Instead of one long-lived agent accumulating context: [code] Strategy 2: Markdown Summaries Instead of Full Code Agents don't load all source files into context: [code] Strategy 3: Parallel Builders Have Narrow Context Each parallel builder only needs: [code] Strategy 4: Incremental Context Loading Tester agent doesn't load all code: [code] Result: Even large projects stay well under 200K per agent. --- Q17: What happens when a project is so large that even is huge? A: The Architect is trained to be concise, but there are limits. Typical sizes: Small project: 10-20 KB Medium project: 20-40 KB Large project: 40-80 KB Problematic: >100 KB (happens with 100+ file projects) Mitigation Strategies: Architect Prompt Includes Conciseness Guidelines: [code] Multi-Document Strategy (for very large projects): [code] Builders read only relevant sub-documents. Parallel Builders Reduce Per-Agent Context: [code] Architecture References Code Comments: [code] Current Limitation: Projects with >200 files may strain even this system. These are rare for autonomous builds. --- Q18: Does parallel execution use MORE tokens overall? A: Yes, but cost is worth the time savings, and it's not 2x-8x multiplier. Token Usage Comparison: Sequential Build (OLD): [code] Parallel Build (NEW): [code] Why not 4x token usage for 4 parallel builders? Context Overlap Reduction: Each builder reads smaller architecture sections (~15KB vs 40KB) Builders don't load each other's code Coordination Overhead is Minimal: Orchestrator spawning logic: ~2K tokens Task JSON parsing: ~1K tokens Time Savings Dominate Cost: 10 minutes saved × hourly rate = worth extra $0.50 in API costs Real-World Example: Medium project: 335K tokens = ~$1.00 (Claude Sonnet) Time saved: 10 minutes ROI: Massive for professional developers --- Q19: Can Context Foundry handle incremental builds to avoid context bloat? A: Partial support. Incremental builds are on the roadmap. Current State: Each build is full rebuild (starts from Scout phase) Previous files can be read as reference Pattern learning provides cross-build continuity Experimental Incremental Support: In : [code] Challenges: Determining what changed (requires diffing) Invalidating dependent components (dependency graph needed) Test suite must run fully (can't skip tests) Workaround for Now: [code] Future Enhancement (v3.0 roadmap): Git diff analysis Dependency graph tracking Selective rebuilds 70-90% faster for small changes --- Parallelization & Coordination Q20: How does topological sort work for task dependencies? A: The Architect agent creates a dependency graph, then Orchestrator executes tasks in sorted levels. Example Project: Game with 4 modules Step 1: Architect Analyzes Dependencies [code] Step 2: Architect Creates [code] Step 3: Orchestrator Computes Levels [code] Step 4: Orchestrator Spawns by Level [code] Actual Bash Implementation (in ): [code] Key Points: DAG (Directed Acyclic Graph) structure Parallelism within levels, sequential between levels Architect is responsible for correct dependency analysis --- Q21: What prevents file write conflicts when multiple builders run in parallel? A: Architect guarantees file uniqueness per task. This is enforced in the prompt. Enforcement in (Phase 2): [code] Architect's Validation Logic: When creating , Architect checks: [code] Example of Correct Assignment: [code] Example of INCORRECT Assignment (Architect would fix this): [code] Filesystem-Level Protection: Even if Architect makes a mistake: [code] Self-Healing Catches Mistakes: Builder 2 overwrites Builder 1's work Tests fail (broken imports) Self-healing iteration: Architect analyzes, fixes task assignment Rebuild with correct file isolation Real-World: File conflicts are rare because Architect prompts emphasize this heavily. --- Q22: How is coordination achieved without shared memory or message queues? A: Filesystem + files act as a coordination primitive. Coordination Protocol: Task Assignment (via CLI argument): [code] Shared Read-Only Context: [code] Isolated Write Paths: [code] Completion Signaling ( files): [code] Synchronization Barriers: [code] Why This Works: Atomic file creation: is atomic on modern filesystems Blocking wait: command blocks until process exit (kernel-level) File existence check: is deterministic No race conditions: No shared mutable state This is simpler than: ❌ Redis pub/sub ❌ Message queues (RabbitMQ) ❌ Distributed locks (Zookeeper) ❌ Shared memory (semaphores) Unix philosophy: Files are universal coordination primitive. --- Q23: What's the overhead of spawning multiple processes vs using threads? A: Process spawning has higher overhead, but benefits outweigh costs. Overhead Comparison: | Metric | Threads (Python) | Processes (bash) | Winner | |--------|------------------|------------------|--------| | Spawn time | ~10ms | ~500ms | Threads | | Memory per worker | ~50MB (shared) | ~200MB (isolated) | Threads | | Context isolation | ❌ Shared | ✅ Isolated | Processes | | Debugging | ❌ Complex | ✅ Separate logs | Processes | | Fault tolerance | ❌ One crash = all crash | ✅ Isolated crashes | Processes | | Authentication | ❌ Needs API keys | ✅ Inherits Claude Code auth | Processes | Real-World Overhead Measurement: [code] Why Process Overhead is Acceptable: Spawning is one-time cost: 500ms to spawn × 4 builders = 2s total Builders run for 5-10 minutes each Overhead is 8 by default. --- Q37: What's the memory usage when running multiple parallel builders? A: ~200MB per builder process, plus orchestrator overhead. Memory Breakdown: [code] Peak Memory Timeline: [code] Comparison to Sequential: [code] System Requirements: [code] Memory Optimization Tips: Reduce parallel builders on low-memory systems: [code] Monitor memory during builds: [code] Kill stuck builders: [code] Memory is Temporary: Processes exit after task completion Memory freed immediately (no leaks) Orchestrator is the only long-running process --- Error Handling & Self-Healing Q38: What happens if a parallel builder fails mid-execution? A: Build fails, self-healing loop does NOT activate (yet). Current Behavior (v2.0): Builder Failure Scenario: [code] Orchestrator Detects Missing File: [code] Error Reporting: [code] Self-Healing Does NOT Trigger Because: Self-healing only activates for test failures (Phase 4) Builder failures are considered fatal errors (bad architecture) User must intervene and retry Roadmap (v2.5): Add builder failure recovery: [code] Current Workaround: If builder fails: [code] --- Q39: How does the self-healing loop work? Walk me through an iteration. A: Self-healing is a 3-phase cycle (Architect → Builder → Test) triggered by test failures. Initial Build: [code] Self-Healing Iteration 1: Step 1: Test Failure Analysis (Current Tester Agent) [code] TypeError: Cannot read property 'token' of undefined at AuthService.login (src/auth.js:15) [code] Step 2: Update Phase Tracking [code] Step 3: Return to Architect (New Architect Agent Spawned) [code]markdown API Response Models (UPDATED - Iteration 1 Fix) Authentication API Response: ~~json { \"data\": { // ❌ INCORRECT ASSUMPTION \"token\": \"...\", \"user\": {...} } } ~~ [code] Impact: Update AuthService.login() to remove access. [code] Step 4: Return to Builder (Parallel Builders Spawned Again) [code] Step 5: Return to Test (Tester Agent Spawned Again) [code] Step 6: Proceed to Next Phase [code] If Tests Failed Again: [code] Iteration Limit Reached: [code] Key Insight: Self-healing is NOT magic—it's structured debugging by AI agents. --- Q40: What's the maximum test iteration count? Can I change it? A: Default: 3 iterations. Configurable via MCP parameter. Configuration: [code] Iteration Tracking: [code] Why 3 is Default: Empirical data from 100 builds: [code] Cost Analysis: [code] When to Increase: Complex integrations (external APIs, databases) New tech stacks (Architect may need multiple attempts to learn) Non-deterministic failures (flaky tests) When to Decrease: Simple projects (to-do lists, calculators) Cost-conscious usage (fail fast, debug manually) Debugging Context Foundry itself (don't want runaway loops) Viewing Iteration History: [code] Each iteration is documented for post-mortem analysis. --- Pattern Learning System Q41: How does the global pattern storage work? Where is it stored? A: Patterns are stored in (global, not per-project). Directory Structure: [code] Format: [code] Format: [code] How Patterns Persist: During Build: Patterns are read-only (Scout/Architect read them) After Build: Deployer agent extracts new patterns Pattern Merge: New patterns merged into global storage Backup Created: Old patterns backed up before merge Storage Size: [code] Extremely lightweight. --- Q42: Does pattern learning work across different projects? How? A: Yes! This is the core value of global patterns. Cross-Project Learning Example: Build 1: Weather Dashboard (Jan 10) [code] Build 2: Flight Tracker (Jan 15) [code] Build 3: Crypto Price Tracker (Jan 22) [code] After 10 Similar Builds: [code] Scout now warns more aggressively: [code] Key Mechanism: Pattern IDs (like ) are stable across projects. Frequency increments teach the system. --- Q43: What patterns are extracted? How does extraction work? A: Extraction happens in Phase 8: Feedback Loop after successful builds. Pattern Extraction Sources: Test Failures (If Any Occurred Before Self-Healing Succeeded): [code] Scout Discoveries That Proved Critical: [code] Build Metrics: [code] Architectural Decisions: [code] Pattern Merging Process: [code] Pattern Classification Algorithm: [code] Extracted Patterns Are Reviewed: [code] User can review and optionally delete patterns from . --- Q44: Is pattern learning private? Or does it upload data to Anthropic? A: 100% local. No data leaves your machine. Privacy Guarantees: Patterns Stored Locally: [code] No Network Transmission: [code] Not Sent to Claude: [code] No Telemetry: [code] What IS Sent to Claude API: [code] Your Pattern Data: ✅ Stored only on your machine ✅ Under your control (can delete anytime) ✅ Not uploaded to any server ✅ Not shared with other Context Foundry users ✅ Included in YOUR API calls to Claude (like any other prompt) Comparison to Other Tools: | Tool | Pattern Storage | Privacy | |------|----------------|---------| | Context Foundry | Local filesystem | ✅ Fully private | | GitHub Copilot | Microsoft servers | ❌ Uploaded to Microsoft | | Cursor | Cursor's cloud | ❌ Uploaded to Cursor | | Tabnine | Optional cloud | ⚠️ Optional upload | Opt-Out: [code] --- Advanced Technical Topics Q45: Could parallel builders have race conditions when writing to the filesystem? A: No, because file assignments are mutually exclusive by design. Race Condition Scenario (Hypothetical): [code] Why This CAN'T Happen in Context Foundry: Architect Guarantees Unique File Assignment: [code] Validation in Orchestrator: [code] Filesystem Atomicity: Even if conflict existed: [code] Modern filesystems (ext4, APFS, NTFS) handle concurrent writes to different files safely. Log File Isolation: [code] Theoretical Edge Case: [code] What Happens: Both builders write to simultaneously Last writer wins (unpredictable which) Tests fail (broken imports, syntax errors) Self-healing loop activates: Architect analyzes test failures Realizes file conflict Fixes build-tasks.json (assigns shared.js to one task only) Rebuilds Verdict: Race conditions are prevented by design. If Architect makes mistake, tests catch it. --- Q46: Why bash process spawning instead of Python or ? A: Authentication inheritance is the killer feature. Also simplicity. Comparison Table: | Feature | Python | Python | Bash Process Spawning | |---------|--------------------------|--------------------|-----------------------| | Authentication | ❌ Needs API keys | ❌ Needs API keys | ✅ Inherits from Claude Code | | Parallelism | ✅ True parallel (CPU) | ❌ GIL限制 (I/O only) | ✅ True parallel (processes) | | Isolation | ✅ Separate memory | ❌ Shared memory | ✅ Separate memory | | Debugging | ⚠️ Complex (multiprocessing.log) | ⚠️ Hard (thread dumps) | ✅ Separate log files | | Code Complexity | ⚠️ 200+ lines | ⚠️ 150+ lines | ✅ 20 lines (bash) | | Fault Tolerance | ✅ Process crash isolated | ❌ Thread crash kills all | ✅ Process crash isolated | | Coordination | ⚠️ Queues, Locks, Events | ⚠️ Locks, Conditions | ✅ Filesystem + | Python Example (OLD, deprecated): [code] Bash Process Spawning (NEW): [code] Why Bash Wins: Authentication Inheritance (Critical): [code] Simpler Code: [code] Easier Debugging: [code] Fault Tolerance: [code] No GIL Issues: [code] Tradeoff: Bash is slower to spawn: [code] Verdict: Auth inheritance + simplicity >> spawn time overhead. --- Q47: How does Context Foundry compare to using manually? A: Context Foundry is structured orchestration of , not just raw agent spawning. Manual Usage: [code] Problems with Manual Approach: You must orchestrate: You decide phase order No structure: No scout → architect → build flow No self-healing: You must debug test failures manually No pattern learning: Mistakes repeat across projects No parallelization: Agents run sequentially (you spawn one at a time) Context accumulation: Long session = context bloat Context Foundry Orchestration: [code] Key Differences: | Aspect | Manual | Context Foundry | |--------|------------------|-----------------| | Orchestration | You decide flow | Automated 8-phase workflow | | Context Management | Long session (context bloat) | Agents die after each phase | | Parallelization | Sequential (you spawn one by one) | Automatic parallel build/test | | Pattern Learning | You remember mistakes | System learns across projects | | Self-Healing | You debug failures | Automated fix loop | | Structure | Ad-hoc | Scout → Architect → Build → Test | | Documentation | You write README | Auto-generated | | Deployment | You git push | Automated GitHub push | When to Use Manual : ✅ Quick prototypes ( \"Stop vibe coding, start building.\" Because software engineering is not about vibes—it's about architecture, tests, and systems that work. --- Conclusion Key Takeaways: File Management: lives in your project, files are the core, everything is local Prompts: , , are the \"secret sauce\" Agents: 8-45 agents per build, each isolated, die after task completion Tokens: Managed via agent death, no single agent exceeds 200K Parallelization: 2-8 builders, file-based coordination, no race conditions Authentication: Inherits from Claude Code, no API keys needed Performance: 30-45% faster than sequential, $1-5 per build Self-Healing: 3-iteration fix loop, 98% success rate Patterns: Global learning, 100% local storage, privacy-first Philosophy: Structured building over vibe coding Next Steps: Read MULTIAGENTARCHITECTURE.md for visual diagrams Read PARALLELAGENTSARCHITECTURE.md for implementation details Explore prompts: orchestratorprompt.txt Try a build: Questions Not Covered? Open an issue: https://github.com/context-foundry/context-foundry/issues --- Last updated: 2025-01-23 Version: 2.0 For: Software developers, architects, AI engineers","url":"/docs/reference/technical-faq","weight":0.9,"tags":[],"headings":["Context Foundry Technical FAQ","Table of Contents","1. File Management & Architecture","Q1: Where are the .context-foundry/ files stored?","Q2: Why .md files instead of JSON for core architecture?","Q3: What's in each .md file? What's their purpose?","Q4: How do files persist across phases if agents die after each phase?","Q5: Can I version control the .context-foundry/ directory?","Q6: What happens to .context-foundry/ files after a successful build?","2. The \"Secret Sauce\" - Prompt Engineering","Q7: Where are the core prompts that make Context Foundry work?","Q8: How does orchestrator_prompt.txt coordinate the entire build?","Q9: How do parallel builder agents coordinate without talking to each other?","Q10: What makes these prompts effective? Can I learn from them?","Q11: Can I customize the prompts for my specific use case?","3. Agent Architecture & Lifecycle","Q12: How many agents are created in a typical build session?","Q13: Do agents share context or run completely isolated?","Q14: How long does an agent live? When does context get freed?","Q15: Can agents communicate during execution, or only via files?","4. Token Management & Context Windows","Q16: How does Context Foundry avoid exceeding Claude's 200K token window?","Q17: What happens when a project is so large that even architecture.md is huge?","Q18: Does parallel execution use MORE tokens overall?","Q19: Can Context Foundry handle incremental builds to avoid context bloat?","5. Parallelization & Coordination","Q20: How does topological sort work for task dependencies?","Q21: What prevents file write conflicts when multiple builders run in parallel?","Q22: How is coordination achieved without shared memory or message queues?","Q23: What's the overhead of spawning multiple claude processes vs using threads?","6. Authentication & API Usage","Q24: Does Context Foundry make direct API calls to Anthropic?","Q25: How does the /agents command inherit Claude Code authentication?","Q26: Are API keys required anywhere in Context Foundry?","Q27: How does Context Foundry handle rate limiting?","7. MCP Server Architecture & Context Management","Q28: What is the MCP server and why is it critical to Context Foundry?","Q29: How does the MCP server free up your main Claude context window?","Q30: What tools/functions does the MCP server provide?","Q31: How does the MCP server orchestrate agents with the Claude CLI?","Q32: What's the architecture of the MCP server?","Q33: What prompts does the MCP server use?","8. Performance & Scaling","Q34: What's the actual speedup for different project sizes?","Q35: When does parallel mode activate? Can I force sequential?","Q36: How many parallel builders is optimal? Is there a limit?","Q37: What's the memory usage when running multiple parallel builders?","9. Error Handling & Self-Healing","Q38: What happens if a parallel builder fails mid-execution?","Q39: How does the self-healing loop work? Walk me through an iteration.","Q40: What's the maximum test iteration count? Can I change it?","10. Pattern Learning System","Q41: How does the global pattern storage work? Where is it stored?","Q42: Does pattern learning work across different projects? How?","Q43: What patterns are extracted? How does extraction work?","Q44: Is pattern learning private? Or does it upload data to Anthropic?","11. Advanced Technical Topics","Q45: Could parallel builders have race conditions when writing to the filesystem?","Q46: Why bash process spawning instead of Python multiprocessing or threading?","Q47: How does Context Foundry compare to using /agents manually?","12. Practical Usage & Limitations","Q48: When should I NOT use Context Foundry?","Q49: What are the cost implications? How much does a typical build cost?","13. Comparisons & Philosophy","Q50: How does Context Foundry compare to Cursor Composer?","Q51: Context Foundry vs GitHub Copilot Workspace?","Q52: What is Context Foundry's design philosophy? How does it differ from \"vibe coding\"?","Conclusion"]}]
