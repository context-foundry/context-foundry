#!/usr/bin/env python3
"""
LLM-as-Judge Validator

Based on Anthropic's approach:
- Evaluate outputs against rubric
- Check: accuracy, completeness, quality, efficiency
- Return scores 0.0-1.0 with pass/fail
"""

import json
import os
from typing import Dict, Any, Optional
from anthropic import Anthropic


class LLMJudge:
    """
    LLM-as-judge for evaluating agent outputs.

    Evaluates against criteria:
    - Functionality (does it work?)
    - Completeness (all requirements met?)
    - Code quality (maintainable?)
    - Test coverage (adequate tests?)
    - Documentation (clear?)
    """

    JUDGE_PROMPT = """You are evaluating code generated by Builder subagents.

PROJECT REQUIREMENTS:
{requirements}

GENERATED CODE SUMMARY:
{code_summary}

FILES GENERATED:
{files_list}

EVALUATION CRITERIA:

1. FUNCTIONALITY (0.0-1.0)
   - Does the code appear complete and functional?
   - Are there obvious bugs or errors?
   - Does it match the requirements?

2. COMPLETENESS (0.0-1.0)
   - All required features implemented?
   - All expected files present?
   - Dependencies properly defined?

3. CODE QUALITY (0.0-1.0)
   - Well-structured and readable?
   - Proper error handling?
   - Follows best practices?

4. TEST COVERAGE (0.0-1.0)
   - Tests exist for main functionality?
   - Tests are meaningful (not trivial)?
   - Edge cases considered?

5. DOCUMENTATION (0.0-1.0)
   - Clear README or docs?
   - Code is well-commented?
   - Setup instructions clear?

For each criterion, provide:
- Score (0.0-1.0 as a number)
- Reasoning (1-2 sentences)
- Critical issues (list, if score < 0.7)

Then provide overall PASS/FAIL (pass requires all scores >= 0.7)

Output as JSON (valid JSON only, no markdown):
{{
  "functionality": {{"score": 0.0-1.0, "reasoning": "...", "issues": []}},
  "completeness": {{"score": 0.0-1.0, "reasoning": "...", "issues": []}},
  "code_quality": {{"score": 0.0-1.0, "reasoning": "...", "issues": []}},
  "test_coverage": {{"score": 0.0-1.0, "reasoning": "...", "issues": []}},
  "documentation": {{"score": 0.0-1.0, "reasoning": "...", "issues": []}},
  "overall": {{"pass": true/false, "critical_issues": []}}
}}"""

    def __init__(self, client: Optional[Anthropic] = None):
        """Initialize LLM Judge.

        Args:
            client: Optional Anthropic client. If None, creates new one.
        """
        if client is None:
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY not set")
            client = Anthropic(api_key=api_key)

        self.client = client
        self.model = os.getenv("VALIDATOR_MODEL", "claude-sonnet-4-20250514")

    def evaluate(self, requirements: str, code_summary: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate generated code against requirements.

        Args:
            requirements: Project requirements
            code_summary: Summary of generated code

        Returns:
            Evaluation dict with scores and pass/fail
        """

        print("\n⚖️  LLM Judge evaluating code...")

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=4000,
                messages=[{
                    "role": "user",
                    "content": self.JUDGE_PROMPT.format(
                        requirements=requirements,
                        code_summary=self._format_code_summary(code_summary),
                        files_list=self._format_files_list(code_summary.get('files', []))
                    )
                }]
            )

            # Extract JSON from response
            response_text = response.content[0].text

            # Try to extract JSON
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()

            evaluation = json.loads(response_text)

            # Print summary
            overall_pass = evaluation.get('overall', {}).get('pass', False)
            print(f"   Result: {'✅ PASS' if overall_pass else '❌ FAIL'}")

            for criterion in ['functionality', 'completeness', 'code_quality', 'test_coverage', 'documentation']:
                if criterion in evaluation:
                    score = evaluation[criterion].get('score', 0.0)
                    print(f"   {criterion.capitalize()}: {score:.2f}")

            return evaluation

        except Exception as e:
            print(f"   ⚠️  Evaluation failed: {e}")
            # Return failing evaluation
            return {
                'functionality': {'score': 0.0, 'reasoning': f'Evaluation error: {e}', 'issues': [str(e)]},
                'completeness': {'score': 0.0, 'reasoning': 'Could not evaluate', 'issues': []},
                'code_quality': {'score': 0.0, 'reasoning': 'Could not evaluate', 'issues': []},
                'test_coverage': {'score': 0.0, 'reasoning': 'Could not evaluate', 'issues': []},
                'documentation': {'score': 0.0, 'reasoning': 'Could not evaluate', 'issues': []},
                'overall': {'pass': False, 'critical_issues': [f'Evaluation error: {e}']}
            }

    def _format_code_summary(self, code_summary: Dict) -> str:
        """Format code summary for evaluation."""

        summary_parts = []

        if 'description' in code_summary:
            summary_parts.append(f"Description: {code_summary['description']}")

        if 'file_count' in code_summary:
            summary_parts.append(f"Total files: {code_summary['file_count']}")

        if 'total_lines' in code_summary:
            summary_parts.append(f"Total lines of code: {code_summary['total_lines']}")

        return '\n'.join(summary_parts) if summary_parts else "No summary available"

    def _format_files_list(self, files: list) -> str:
        """Format files list for evaluation."""

        if not files:
            return "No files generated"

        return '\n'.join(f"- {file}" for file in files)
